<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="理论部分CNN解决的问题在CNN出现之前，图像对于人工智能来说是一个难题，有2个原因：  图像需要处理的数据量太大，导致成本很高，效率很低。 图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高。  另一个角度（用全连接神经网络处理大尺寸图像的缺点）：  其次参数过多效率低下，训练困难，同时大量的参数也很快会导致网络过拟合 图像展开为向量会丢失空间信息；  卷积层 这个过程我们可以理解">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络实战_图片分类">
<meta property="og:url" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/index.html">
<meta property="og:site_name" content="Writing Lite">
<meta property="og:description" content="理论部分CNN解决的问题在CNN出现之前，图像对于人工智能来说是一个难题，有2个原因：  图像需要处理的数据量太大，导致成本很高，效率很低。 图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高。  另一个角度（用全连接神经网络处理大尺寸图像的缺点）：  其次参数过多效率低下，训练困难，同时大量的参数也很快会导致网络过拟合 图像展开为向量会丢失空间信息；  卷积层 这个过程我们可以理解">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/juanji.gif">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/150926.jpg">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/chihua.gif">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/LeNet.png">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/vgg.jpeg">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/3x3-5.jpeg">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/inception.png">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/residual.png">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/lenet-inception_restnet.png">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/output_23_0.png">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/output_29_0.png">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/output_33_1.png">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/output_35_1.png">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/output_37_1.png">
<meta property="og:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/output_39_1.png">
<meta property="article:published_time" content="2020-10-13T20:43:00.000Z">
<meta property="article:modified_time" content="2020-12-02T06:52:04.933Z">
<meta property="article:author" content="hwyoung">
<meta property="article:tag" content="深度学习 Tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/_image/juanji.gif">

<link rel="canonical" href="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>卷积神经网络实战_图片分类 | Writing Lite</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Writing Lite</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/10/13/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(4)-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.png">
      <meta itemprop="name" content="hwyoung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Writing Lite">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          卷积神经网络实战_图片分类
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-13 20:43:00" itemprop="dateCreated datePublished" datetime="2020-10-13T20:43:00+00:00">2020-10-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-12-02 06:52:04" itemprop="dateModified" datetime="2020-12-02T06:52:04+00:00">2020-12-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h2><h3 id="CNN解决的问题"><a href="#CNN解决的问题" class="headerlink" title="CNN解决的问题"></a>CNN解决的问题</h3><p>在CNN出现之前，图像对于人工智能来说是一个难题，有2个原因：</p>
<ol>
<li>图像需要处理的数据量太大，导致成本很高，效率很低。</li>
<li>图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高。</li>
</ol>
<p>另一个角度（用全连接神经网络处理大尺寸图像的缺点）：</p>
<ol>
<li>其次参数过多效率低下，训练困难，同时大量的参数也很快会导致网络过拟合</li>
<li>图像展开为向量会丢失空间信息；</li>
</ol>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p><img src="_image/juanji.gif"></p>
<p>这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。</p>
<p>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：</p>
<p><img src="_image/150926.jpg"></p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>池化层简单说就是下采样，他可以大大降低数据的维度，也可以缓解卷积层对位置的过度敏感性。其过程如下：</p>
<p><img src="_image/chihua.gif"></p>
<p>上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。</p>
<p>之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。</p>
<p>总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</p>
<h2 id="实战部分"><a href="#实战部分" class="headerlink" title="实战部分"></a>实战部分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> PIL</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卷积层</span></span><br><span class="line">x = tf.constant(<span class="built_in">range</span>(<span class="number">9</span>), dtype=tf.float32)</span><br><span class="line">x = tf.reshape(x, shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">print(<span class="string">&#x27;x :&#x27;</span>, tf.reshape(x, shape=(<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line">conv_layer = tf.keras.layers.Conv2D(filters=<span class="number">1</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), kernel_initializer=tf.keras.initializers.Ones())</span><br><span class="line">conv_ret = conv_layer(x)</span><br><span class="line">print(<span class="string">&#x27;shape :&#x27;</span>, conv_ret.shape)</span><br><span class="line">print(<span class="string">&#x27;conv ret :&#x27;</span>, tf.reshape(conv_ret, shape=(conv_ret.shape[<span class="number">1</span>], conv_ret.shape[<span class="number">2</span>])))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;kernel size :&#x27;</span>, conv_layer.weights[<span class="number">0</span>].shape)</span><br></pre></td></tr></table></figure>

<pre><code>x : tf.Tensor(
[[0. 1. 2.]
 [3. 4. 5.]
 [6. 7. 8.]], shape=(3, 3), dtype=float32)
shape : (1, 2, 2, 1)
conv ret : tf.Tensor(
[[ 8. 12.]
 [20. 24.]], shape=(2, 2), dtype=float32)
kernel size : (2, 2, 1, 1)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多输入层-卷积层</span></span><br><span class="line">x = tf.constant(<span class="built_in">range</span>(<span class="number">18</span>), dtype=tf.float32)</span><br><span class="line">x = tf.reshape(x, shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">print(<span class="string">&#x27;x(channel-1) :&#x27;</span>, tf.reshape(x[:, :, :, <span class="number">0</span>], shape=(<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line">print(<span class="string">&#x27;x(channel-2) :&#x27;</span>, tf.reshape(x[:, :, :, <span class="number">1</span>], shape=(<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line">conv_layer = tf.keras.layers.Conv2D(<span class="number">1</span>, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), kernel_initializer=tf.keras.initializers.Ones())</span><br><span class="line">conv_ret = conv_layer(x)</span><br><span class="line">print(<span class="string">&#x27;shape :&#x27;</span>, conv_ret.shape)</span><br><span class="line">print(<span class="string">&#x27;conv ret :&#x27;</span>, tf.reshape(conv_ret, shape=(conv_ret.shape[<span class="number">1</span>], conv_ret.shape[<span class="number">2</span>])))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;kernel size :&#x27;</span>, conv_layer.weights[<span class="number">0</span>].shape)</span><br></pre></td></tr></table></figure>

<pre><code>x(channel-1) : tf.Tensor(
[[ 0.  2.  4.]
 [ 6.  8. 10.]
 [12. 14. 16.]], shape=(3, 3), dtype=float32)
x(channel-2) : tf.Tensor(
[[ 1.  3.  5.]
 [ 7.  9. 11.]
 [13. 15. 17.]], shape=(3, 3), dtype=float32)
shape : (1, 2, 2, 1)
conv ret : tf.Tensor(
[[ 36.  52.]
 [ 84. 100.]], shape=(2, 2), dtype=float32)
kernel size : (2, 2, 2, 1)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 池化层</span></span><br><span class="line">x = tf.constant(<span class="built_in">range</span>(<span class="number">9</span>), dtype=tf.float32)</span><br><span class="line">x = tf.reshape(x, shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">print(<span class="string">&#x27;x :&#x27;</span>,tf.reshape(x, shape=(<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line">pool_ret = tf.keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">1</span>, <span class="number">1</span>))(x)</span><br><span class="line">print(<span class="string">&#x27;pooling ret: &#x27;</span>, tf.reshape(pool_ret, shape=(conv_ret.shape[<span class="number">1</span>], conv_ret.shape[<span class="number">2</span>])))</span><br></pre></td></tr></table></figure>

<pre><code>x : tf.Tensor(
[[0. 1. 2.]
 [3. 4. 5.]
 [6. 7. 8.]], shape=(3, 3), dtype=float32)
pooling ret:  tf.Tensor(
[[4. 5.]
 [7. 8.]], shape=(2, 2), dtype=float32)</code></pre>
<h3 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h3><p>LeNet 诞生于 1994 年，是最早的卷积神经网络之一，LeNet分为卷积层块和全连接层块两个部分。</p>
<p>卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用 5×5 的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为 2×2 ，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p>
<p>卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p>
<p><img src="_image/LeNet.png" alt="leNet"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">leNet_model = tf.keras.Sequential()</span><br><span class="line">leNet_model.add(tf.keras.layers.InputLayer(input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">leNet_model.add(tf.keras.layers.Conv2D(filters=<span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">leNet_model.add(tf.keras.layers.AveragePooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))) <span class="comment"># strides default is None, If None, it will default to `pool_size`.  </span></span><br><span class="line"></span><br><span class="line">leNet_model.add(tf.keras.layers.Conv2D(filters=<span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">leNet_model.add(tf.keras.layers.AveragePooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">leNet_model.add(tf.keras.layers.Flatten())</span><br><span class="line">leNet_model.add(tf.keras.layers.Dense(units=<span class="number">120</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">leNet_model.add(tf.keras.layers.Dense(units=<span class="number">84</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line">leNet_model.add(tf.keras.layers.Dense(units=<span class="number">10</span>, activation = <span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">leNet_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_5 (Conv2D)            (None, 28, 28, 6)         156       
_________________________________________________________________
average_pooling2d (AveragePo (None, 14, 14, 6)         0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 10, 10, 16)        2416      
_________________________________________________________________
average_pooling2d_1 (Average (None, 5, 5, 16)          0         
_________________________________________________________________
flatten (Flatten)            (None, 400)               0         
_________________________________________________________________
dense (Dense)                (None, 120)               48120     
_________________________________________________________________
dense_1 (Dense)              (None, 84)                10164     
_________________________________________________________________
dense_2 (Dense)              (None, 10)                850       
=================================================================
Total params: 61,706
Trainable params: 61,706
Non-trainable params: 0
_________________________________________________________________</code></pre>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>2012年，AlexNet横空出世。这个模型的名字来源于论文第一作者的姓名Alex Krizhevsky [1]。AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。</p>
<p>AlexNet与LeNet的设计理念非常相似，但也有显著的区别。</p>
<ol>
<li><p><strong>更深更宽</strong>，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数十倍。</p>
</li>
<li><p><strong>AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数</strong>。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。</p>
</li>
<li><p><strong>训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合</strong>。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout</p>
</li>
<li><p>AlexNet引入了大量的<strong>图像增广</strong>，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">alexNet_model = tf.keras.Sequential()</span><br><span class="line">alexNet_model.add(tf.keras.layers.InputLayer(input_shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用较大的11 x 11窗口来捕获物体。同时使用步幅4来较大幅度减小输出高和宽。这里使用的输出通道数比LeNet中的也要大很多</span></span><br><span class="line">alexNet_model.add(tf.keras.layers.Conv2D(filters=<span class="number">96</span>, kernel_size=(<span class="number">11</span>, <span class="number">11</span>), strides=(<span class="number">4</span>, <span class="number">4</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">alexNet_model.add(tf.keras.layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">alexNet_model.add(tf.keras.layers.ZeroPadding2D(padding=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">alexNet_model.add(tf.keras.layers.Conv2D(filters=<span class="number">256</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">alexNet_model.add(tf.keras.layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line"><span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">alexNet_model.add(tf.keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">alexNet_model.add(tf.keras.layers.Conv2D(filters=<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">alexNet_model.add(tf.keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">alexNet_model.add(tf.keras.layers.Conv2D(filters=<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">alexNet_model.add(tf.keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">alexNet_model.add(tf.keras.layers.Conv2D(filters=<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">alexNet_model.add(tf.keras.layers.Dense(<span class="number">4096</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">alexNet_model.add(tf.keras.layers.Dropout(rate=<span class="number">0.5</span>))</span><br><span class="line">alexNet_model.add(tf.keras.layers.Dense(<span class="number">4096</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">alexNet_model.add(tf.keras.layers.Dropout(rate=<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">alexNet_model.add(tf.keras.layers.Dense(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">alexNet_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_7 (Conv2D)            (None, 54, 54, 96)        11712     
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 26, 26, 96)        0         
_________________________________________________________________
zero_padding2d (ZeroPadding2 (None, 30, 30, 96)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 26, 26, 256)       614656    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 12, 12, 256)       0         
_________________________________________________________________
zero_padding2d_1 (ZeroPaddin (None, 14, 14, 256)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 12, 12, 384)       885120    
_________________________________________________________________
zero_padding2d_2 (ZeroPaddin (None, 14, 14, 384)       0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 12, 12, 384)       1327488   
_________________________________________________________________
zero_padding2d_3 (ZeroPaddin (None, 14, 14, 384)       0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 12, 12, 256)       884992    
_________________________________________________________________
dense_3 (Dense)              (None, 12, 12, 4096)      1052672   
_________________________________________________________________
dropout (Dropout)            (None, 12, 12, 4096)      0         
_________________________________________________________________
dense_4 (Dense)              (None, 12, 12, 4096)      16781312  
_________________________________________________________________
dropout_1 (Dropout)          (None, 12, 12, 4096)      0         
_________________________________________________________________
dense_5 (Dense)              (None, 12, 12, 10)        40970     
=================================================================
Total params: 21,598,922
Trainable params: 21,598,922
Non-trainable params: 0
_________________________________________________________________</code></pre>
<h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p>AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。我们将在本章的后续几节里介绍几种不同的深度网络设计思路。</p>
<p>本节介绍VGG，它的名字来源于论文作者所在的实验室Visual Geometry Group [1]。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。</p>
<p>VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为 3×3 的卷积层后接上一个步幅为2、窗口形状为 2×2 的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用vgg_block函数来实现这个基础的VGG块，它可以指定卷积层的数量num_convs和输出通道数num_channels。</p>
<p>不同结构的vgg：<br><img src="_image/vgg.jpeg"></p>
<p><strong>有用设计</strong>：两个3´3的卷积层串联相当于1个5´5的卷积层，即一个像素会跟周围5´5的像素产生关联，可以说感受野大小为5´5。而3个3´3的卷积层串联的效果则相当于1个7´7的卷积层。除此之外，3个串联的3´3的卷积层，拥有比1个7´7的卷积层更少的参数量，只有后者的。最重要的是，3个3´3的卷积层拥有比1个7´7的卷积层更多的非线性变换（前者可以使用三次ReLU激活函数，而后者只有一次），使得CNN对特征的学习能力更强。</p>
<p><img src="_image/3x3-5.jpeg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGGBlock</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_convs, num_channels</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(VGGBlock, self).__init__()</span><br><span class="line">    self.convs = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">      self.convs.append(tf.keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">      self.convs.append(tf.keras.layers.Conv2D(filters=num_channels, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    self.maxpool = tf.keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> self.convs:</span><br><span class="line">      inputs = layer(inputs)</span><br><span class="line">    outputs = self.maxpool(inputs)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">vgg_block = VGGBlock(<span class="number">3</span>, <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">test_inputs = tf.random.uniform(shape=(<span class="number">2</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), dtype=tf.float32)</span><br><span class="line">vgg_block.call(test_inputs).shape</span><br></pre></td></tr></table></figure>




<pre><code>TensorShape([2, 16, 16, 64])</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_vggNet</span>(<span class="params">conv_arch</span>):</span></span><br><span class="line">    vggNet_model = tf.keras.Sequential()</span><br><span class="line">    vggNet_model.add(tf.keras.layers.InputLayer(input_shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, num_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        vggNet_model.add(VGGBlock(num_convs, num_channels))</span><br><span class="line">    vggNet_model.add(tf.keras.layers.Flatten())</span><br><span class="line">    <span class="comment"># 全连接层部分</span></span><br><span class="line">    vggNet_model.add(tf.keras.layers.Dense(<span class="number">4096</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    vggNet_model.add(tf.keras.layers.Dropout(rate=<span class="number">0.5</span>))</span><br><span class="line">    vggNet_model.add(tf.keras.layers.Dense(<span class="number">4096</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    vggNet_model.add(tf.keras.layers.Dropout(rate=<span class="number">0.5</span>))</span><br><span class="line">    vggNet_model.add(tf.keras.layers.Dense(units=<span class="number">10</span>))</span><br><span class="line">    <span class="keyword">return</span> vggNet_model</span><br><span class="line"></span><br><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line">vggNet = generate_vggNet(conv_arch)</span><br><span class="line">vggNet.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_5&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
vgg_block_16 (VGGBlock)      (None, 112, 112, 64)      640       
_________________________________________________________________
vgg_block_17 (VGGBlock)      (None, 56, 56, 128)       73856     
_________________________________________________________________
vgg_block_18 (VGGBlock)      (None, 28, 28, 256)       885248    
_________________________________________________________________
vgg_block_19 (VGGBlock)      (None, 14, 14, 512)       3539968   
_________________________________________________________________
vgg_block_20 (VGGBlock)      (None, 7, 7, 512)         4719616   
_________________________________________________________________
flatten_1 (Flatten)          (None, 25088)             0         
_________________________________________________________________
dense_12 (Dense)             (None, 4096)              102764544 
_________________________________________________________________
dropout_6 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_13 (Dense)             (None, 4096)              16781312  
_________________________________________________________________
dropout_7 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_14 (Dense)             (None, 10)                40970     
=================================================================
Total params: 128,806,154
Trainable params: 128,806,154
Non-trainable params: 0
_________________________________________________________________</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vgg16_app = tf.keras.applications.VGG16(weights=<span class="literal">None</span>)</span><br><span class="line">vgg16_app.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;vgg16&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_9 (InputLayer)         [(None, 224, 224, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544 
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
predictions (Dense)          (None, 1000)              4097000   
=================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
_________________________________________________________________</code></pre>
<h3 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h3><p>Google Inception Net首次出现在ILSVRC 2014的比赛中（和VGGNet同年），就以较大优势取得了第一名。那届比赛中的Inception Net通常被称为Inception V1，它最大的特点是控制了计算量和参数量的同时，获得了非常好的分类性能——top-5错误率6.67%，只有AlexNet的一半不到。</p>
<p>Inception Module的基本结构，其中有4个分支：</p>
<ol>
<li>第一个分支对输入进行1´1的卷积，这其实也是NIN中提出的一个重要结构。1´1的卷积是一个非常优秀的结构，它可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道升维和降维。可以看到Inception Module的4个分支都用到了1´1卷积，来进行低成本（计算量比3´3小很多）的跨通道的特征变换。</li>
<li>第二个分支先使用了1´1卷积，然后连接3´3卷积，相当于进行了两次特征变换。</li>
<li>第三个分支类似，先是1´1的卷积，然后连接5´5卷积。</li>
<li>最后一个分支则是3´3最大池化后直接使用1´1卷积。<br><img src="_image/inception.png" alt="inception"></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(Inception, self).__init__()</span><br><span class="line"></span><br><span class="line">    self.b1_conv_1 = tf.keras.layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    self.b2_conv_1 = tf.keras.layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    self.b2_padding = tf.keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    self.b2_conv_3 = tf.keras.layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># padding(1) + conv(3*3) = conv(3*3, padding=&#x27;same&#x27;)</span></span><br><span class="line"></span><br><span class="line">    self.b3_conv_1 = tf.keras.layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    self.b3_padding = tf.keras.layers.ZeroPadding2D(padding=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    self.b3_conv_5 = tf.keras.layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    self.b4_padding = tf.keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    self.b4_pool_3 = tf.keras.layers.MaxPool2D(pool_size=<span class="number">3</span>, strides=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    self.b4_conv_1 = tf.keras.layers.Conv2D(filters=<span class="number">64</span>, kernel_size=<span class="number">1</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    self.concatenate = tf.keras.layers.Concatenate()</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">    <span class="comment"># 线路1，单1 x 1卷积层</span></span><br><span class="line">    branch_1 = self.b1_conv_1(inputs)</span><br><span class="line">    <span class="comment"># 线路2，1 x 1卷积层后接3 x 3卷积层</span></span><br><span class="line">    branch_2 = self.b2_conv_3(self.b2_padding(self.b2_conv_1(inputs)))</span><br><span class="line">    <span class="comment"># 线路3，1 x 1卷积层后接5 x 5卷积层</span></span><br><span class="line">    branch_3 = self.b3_conv_5(self.b3_padding(self.b3_conv_1(inputs)))</span><br><span class="line">    <span class="comment"># 线路4，3 x 3最大池化层后接1 x 1卷积层</span></span><br><span class="line">    branch_4 = self.b4_conv_1(self.b4_padding(self.b4_pool_3(inputs)))</span><br><span class="line">    <span class="comment"># channel_axis = -1</span></span><br><span class="line">    outputs = tf.keras.layers.concatenate([branch_1, branch_2, branch_3, branch_4], axis=-<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inception = Inception()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_inputs = tf.random.uniform(shape=(<span class="number">2</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line">inception(test_inputs).shape</span><br></pre></td></tr></table></figure>




<pre><code>TensorShape([2, 10, 10, 256])</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tmp = tf.keras.applications.InceptionV3(weights=<span class="literal">None</span>)</span><br><span class="line">tmp.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;inception_v3&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           [(None, 299, 299, 3) 0                                            
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 149, 149, 32) 864         input_11[0][0]                   
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 149, 149, 32) 96          conv2d_53[0][0]                  
__________________________________________________________________________________________________
activation (Activation)         (None, 149, 149, 32) 0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 147, 147, 32) 9216        activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 147, 147, 32) 96          conv2d_54[0][0]                  
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 147, 147, 32) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 147, 147, 64) 18432       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 64) 192         conv2d_55[0][0]                  
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 64) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_24 (MaxPooling2D) (None, 73, 73, 64)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 73, 73, 80)   5120        max_pooling2d_24[0][0]           
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 73, 73, 80)   240         conv2d_56[0][0]                  
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 73, 73, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 71, 71, 192)  138240      activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 71, 71, 192)  576         conv2d_57[0][0]                  
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 71, 71, 192)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
max_pooling2d_25 (MaxPooling2D) (None, 35, 35, 192)  0           activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 35, 35, 64)   12288       max_pooling2d_25[0][0]           
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_61[0][0]                  
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 35, 35, 48)   9216        max_pooling2d_25[0][0]           
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 35, 35, 96)   55296       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 48)   144         conv2d_59[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 96)   288         conv2d_62[0][0]                  
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 48)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 96)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_25[0][0]           
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 35, 35, 64)   12288       max_pooling2d_25[0][0]           
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 35, 35, 64)   76800       activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 35, 35, 96)   82944       activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 35, 35, 64)   192         conv2d_58[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 64)   192         conv2d_60[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_63[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 32)   96          conv2d_64[0][0]                  
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 35, 35, 64)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 64)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 32)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_5[0][0]               
                                                                 activation_7[0][0]               
                                                                 activation_10[0][0]              
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_68[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 35, 35, 96)   55296       activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 48)   144         conv2d_66[0][0]                  
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 96)   288         conv2d_69[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 48)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 96)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 35, 35, 64)   76800       activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 35, 35, 96)   82944       activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 64)   192         conv2d_65[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 64)   192         conv2d_67[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_70[0][0]                  
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 64)   192         conv2d_71[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 64)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 64)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 64)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_12[0][0]              
                                                                 activation_14[0][0]              
                                                                 activation_17[0][0]              
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_75[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 35, 35, 96)   55296       activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 48)   144         conv2d_73[0][0]                  
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 96)   288         conv2d_76[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 48)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 96)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 35, 35, 64)   76800       activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 35, 35, 96)   82944       activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_72[0][0]                  
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 64)   192         conv2d_74[0][0]                  
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_77[0][0]                  
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 64)   192         conv2d_78[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 64)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 64)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_19[0][0]              
                                                                 activation_21[0][0]              
                                                                 activation_24[0][0]              
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 35, 35, 64)   192         conv2d_80[0][0]                  
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 35, 35, 64)   0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 35, 35, 96)   55296       activation_27[0][0]              
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 96)   288         conv2d_81[0][0]                  
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 96)   0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 17, 17, 96)   82944       activation_28[0][0]              
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 17, 17, 384)  1152        conv2d_79[0][0]                  
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 17, 17, 96)   288         conv2d_82[0][0]                  
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 17, 17, 384)  0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 17, 17, 96)   0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
max_pooling2d_26 (MaxPooling2D) (None, 17, 17, 288)  0           mixed2[0][0]                     
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_26[0][0]              
                                                                 activation_29[0][0]              
                                                                 max_pooling2d_26[0][0]           
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 128)  384         conv2d_87[0][0]                  
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 128)  0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 17, 17, 128)  114688      activation_34[0][0]              
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_88[0][0]                  
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 128)  384         conv2d_84[0][0]                  
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_89[0][0]                  
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 128)  0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 17, 17, 128)  114688      activation_31[0][0]              
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_85[0][0]                  
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_90[0][0]                  
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 17, 17, 192)  172032      activation_32[0][0]              
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 17, 17, 192)  172032      activation_37[0][0]              
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 192)  576         conv2d_83[0][0]                  
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 192)  576         conv2d_86[0][0]                  
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 192)  576         conv2d_91[0][0]                  
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_92[0][0]                  
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 192)  0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 192)  0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 192)  0           batch_normalization_38[0][0]     
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_30[0][0]              
                                                                 activation_33[0][0]              
                                                                 activation_38[0][0]              
                                                                 activation_39[0][0]              
__________________________________________________________________________________________________
conv2d_97 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 160)  480         conv2d_97[0][0]                  
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 160)  0           batch_normalization_44[0][0]     
__________________________________________________________________________________________________
conv2d_98 (Conv2D)              (None, 17, 17, 160)  179200      activation_44[0][0]              
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_98[0][0]                  
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_99 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 160)  480         conv2d_94[0][0]                  
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_99[0][0]                  
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 160)  0           batch_normalization_41[0][0]     
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     
__________________________________________________________________________________________________
conv2d_95 (Conv2D)              (None, 17, 17, 160)  179200      activation_41[0][0]              
__________________________________________________________________________________________________
conv2d_100 (Conv2D)             (None, 17, 17, 160)  179200      activation_46[0][0]              
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_95[0][0]                  
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_100[0][0]                 
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_96 (Conv2D)              (None, 17, 17, 192)  215040      activation_42[0][0]              
__________________________________________________________________________________________________
conv2d_101 (Conv2D)             (None, 17, 17, 192)  215040      activation_47[0][0]              
__________________________________________________________________________________________________
conv2d_102 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_93[0][0]                  
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 192)  576         conv2d_96[0][0]                  
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 192)  576         conv2d_101[0][0]                 
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_102[0][0]                 
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 192)  0           batch_normalization_43[0][0]     
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 192)  0           batch_normalization_48[0][0]     
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_40[0][0]              
                                                                 activation_43[0][0]              
                                                                 activation_48[0][0]              
                                                                 activation_49[0][0]              
__________________________________________________________________________________________________
conv2d_107 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 160)  480         conv2d_107[0][0]                 
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 160)  0           batch_normalization_54[0][0]     
__________________________________________________________________________________________________
conv2d_108 (Conv2D)             (None, 17, 17, 160)  179200      activation_54[0][0]              
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_108[0][0]                 
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     
__________________________________________________________________________________________________
conv2d_104 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_109 (Conv2D)             (None, 17, 17, 160)  179200      activation_55[0][0]              
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 160)  480         conv2d_104[0][0]                 
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_109[0][0]                 
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 160)  0           batch_normalization_51[0][0]     
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     
__________________________________________________________________________________________________
conv2d_105 (Conv2D)             (None, 17, 17, 160)  179200      activation_51[0][0]              
__________________________________________________________________________________________________
conv2d_110 (Conv2D)             (None, 17, 17, 160)  179200      activation_56[0][0]              
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_105[0][0]                 
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_110[0][0]                 
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_103 (Conv2D)             (None, 17, 17, 192)  147456      mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_106 (Conv2D)             (None, 17, 17, 192)  215040      activation_52[0][0]              
__________________________________________________________________________________________________
conv2d_111 (Conv2D)             (None, 17, 17, 192)  215040      activation_57[0][0]              
__________________________________________________________________________________________________
conv2d_112 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_103[0][0]                 
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 192)  576         conv2d_106[0][0]                 
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 192)  576         conv2d_111[0][0]                 
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_112[0][0]                 
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 192)  0           batch_normalization_53[0][0]     
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 192)  0           batch_normalization_58[0][0]     
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_50[0][0]              
                                                                 activation_53[0][0]              
                                                                 activation_58[0][0]              
                                                                 activation_59[0][0]              
__________________________________________________________________________________________________
conv2d_117 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_117[0][0]                 
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     
__________________________________________________________________________________________________
conv2d_118 (Conv2D)             (None, 17, 17, 192)  258048      activation_64[0][0]              
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_118[0][0]                 
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     
__________________________________________________________________________________________________
conv2d_114 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_119 (Conv2D)             (None, 17, 17, 192)  258048      activation_65[0][0]              
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_114[0][0]                 
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_119[0][0]                 
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     
__________________________________________________________________________________________________
conv2d_115 (Conv2D)             (None, 17, 17, 192)  258048      activation_61[0][0]              
__________________________________________________________________________________________________
conv2d_120 (Conv2D)             (None, 17, 17, 192)  258048      activation_66[0][0]              
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_115[0][0]                 
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_120[0][0]                 
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_113 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_116 (Conv2D)             (None, 17, 17, 192)  258048      activation_62[0][0]              
__________________________________________________________________________________________________
conv2d_121 (Conv2D)             (None, 17, 17, 192)  258048      activation_67[0][0]              
__________________________________________________________________________________________________
conv2d_122 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_113[0][0]                 
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_116[0][0]                 
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_121[0][0]                 
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_122[0][0]                 
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_60[0][0]              
                                                                 activation_63[0][0]              
                                                                 activation_68[0][0]              
                                                                 activation_69[0][0]              
__________________________________________________________________________________________________
conv2d_125 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 17, 17, 192)  576         conv2d_125[0][0]                 
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 17, 17, 192)  0           batch_normalization_72[0][0]     
__________________________________________________________________________________________________
conv2d_126 (Conv2D)             (None, 17, 17, 192)  258048      activation_72[0][0]              
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_126[0][0]                 
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     
__________________________________________________________________________________________________
conv2d_123 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     
__________________________________________________________________________________________________
conv2d_127 (Conv2D)             (None, 17, 17, 192)  258048      activation_73[0][0]              
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_123[0][0]                 
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_127[0][0]                 
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     
__________________________________________________________________________________________________
conv2d_124 (Conv2D)             (None, 8, 8, 320)    552960      activation_70[0][0]              
__________________________________________________________________________________________________
conv2d_128 (Conv2D)             (None, 8, 8, 192)    331776      activation_74[0][0]              
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 8, 8, 320)    960         conv2d_124[0][0]                 
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 8, 8, 192)    576         conv2d_128[0][0]                 
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 8, 8, 320)    0           batch_normalization_71[0][0]     
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 8, 8, 192)    0           batch_normalization_75[0][0]     
__________________________________________________________________________________________________
max_pooling2d_27 (MaxPooling2D) (None, 8, 8, 768)    0           mixed7[0][0]                     
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_71[0][0]              
                                                                 activation_75[0][0]              
                                                                 max_pooling2d_27[0][0]           
__________________________________________________________________________________________________
conv2d_133 (Conv2D)             (None, 8, 8, 448)    573440      mixed8[0][0]                     
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 448)    1344        conv2d_133[0][0]                 
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 448)    0           batch_normalization_80[0][0]     
__________________________________________________________________________________________________
conv2d_130 (Conv2D)             (None, 8, 8, 384)    491520      mixed8[0][0]                     
__________________________________________________________________________________________________
conv2d_134 (Conv2D)             (None, 8, 8, 384)    1548288     activation_80[0][0]              
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 384)    1152        conv2d_130[0][0]                 
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 384)    1152        conv2d_134[0][0]                 
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 384)    0           batch_normalization_77[0][0]     
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 384)    0           batch_normalization_81[0][0]     
__________________________________________________________________________________________________
conv2d_131 (Conv2D)             (None, 8, 8, 384)    442368      activation_77[0][0]              
__________________________________________________________________________________________________
conv2d_132 (Conv2D)             (None, 8, 8, 384)    442368      activation_77[0][0]              
__________________________________________________________________________________________________
conv2d_135 (Conv2D)             (None, 8, 8, 384)    442368      activation_81[0][0]              
__________________________________________________________________________________________________
conv2d_136 (Conv2D)             (None, 8, 8, 384)    442368      activation_81[0][0]              
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     
__________________________________________________________________________________________________
conv2d_129 (Conv2D)             (None, 8, 8, 320)    409600      mixed8[0][0]                     
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_131[0][0]                 
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_132[0][0]                 
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_135[0][0]                 
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_136[0][0]                 
__________________________________________________________________________________________________
conv2d_137 (Conv2D)             (None, 8, 8, 192)    245760      average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 320)    960         conv2d_129[0][0]                 
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 192)    576         conv2d_137[0][0]                 
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 320)    0           batch_normalization_76[0][0]     
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_78[0][0]              
                                                                 activation_79[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_82[0][0]              
                                                                 activation_83[0][0]              
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 192)    0           batch_normalization_84[0][0]     
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_76[0][0]              
                                                                 mixed9_0[0][0]                   
                                                                 concatenate_2[0][0]              
                                                                 activation_84[0][0]              
__________________________________________________________________________________________________
conv2d_142 (Conv2D)             (None, 8, 8, 448)    917504      mixed9[0][0]                     
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 448)    1344        conv2d_142[0][0]                 
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 448)    0           batch_normalization_89[0][0]     
__________________________________________________________________________________________________
conv2d_139 (Conv2D)             (None, 8, 8, 384)    786432      mixed9[0][0]                     
__________________________________________________________________________________________________
conv2d_143 (Conv2D)             (None, 8, 8, 384)    1548288     activation_89[0][0]              
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 384)    1152        conv2d_139[0][0]                 
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 384)    1152        conv2d_143[0][0]                 
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 384)    0           batch_normalization_86[0][0]     
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 384)    0           batch_normalization_90[0][0]     
__________________________________________________________________________________________________
conv2d_140 (Conv2D)             (None, 8, 8, 384)    442368      activation_86[0][0]              
__________________________________________________________________________________________________
conv2d_141 (Conv2D)             (None, 8, 8, 384)    442368      activation_86[0][0]              
__________________________________________________________________________________________________
conv2d_144 (Conv2D)             (None, 8, 8, 384)    442368      activation_90[0][0]              
__________________________________________________________________________________________________
conv2d_145 (Conv2D)             (None, 8, 8, 384)    442368      activation_90[0][0]              
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 8, 8, 2048)   0           mixed9[0][0]                     
__________________________________________________________________________________________________
conv2d_138 (Conv2D)             (None, 8, 8, 320)    655360      mixed9[0][0]                     
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_140[0][0]                 
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_141[0][0]                 
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_144[0][0]                 
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_145[0][0]                 
__________________________________________________________________________________________________
conv2d_146 (Conv2D)             (None, 8, 8, 192)    393216      average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 320)    960         conv2d_138[0][0]                 
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 192)    576         conv2d_146[0][0]                 
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 320)    0           batch_normalization_85[0][0]     
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_87[0][0]              
                                                                 activation_88[0][0]              
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 8, 8, 768)    0           activation_91[0][0]              
                                                                 activation_92[0][0]              
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 192)    0           batch_normalization_93[0][0]     
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_85[0][0]              
                                                                 mixed9_1[0][0]                   
                                                                 concatenate_3[0][0]              
                                                                 activation_93[0][0]              
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    
__________________________________________________________________________________________________
predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   
==================================================================================================
Total params: 23,851,784
Trainable params: 23,817,352
Non-trainable params: 34,432
__________________________________________________________________________________________________</code></pre>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>ResNet（Residual Neural Network）由微软研究院的Kaiming He等4名华人提出，通过使用Residual Unit成功训练152层深的神经网络，在ILSVRC 2015比赛中获得了冠军，取得3.57%的top-5错误率，同时参数量却比VGGNet低，效果非常突出。</p>
<p>假定某段神经网络的输入是x，期望输出是，如果我们直接把输入x传到输出作为初始结果，那么此时我们需要学习的目标就是。如图14所示，这就是一个ResNet的残差学习单元（Residual Unit），ResNet相当于将学习目标改变了，不再是学习一个完整的输出，只是输出和输入的差别，即残差。</p>
<p>可以看到普通直连的卷积神经网络和ResNet的最大区别在于，ResNet有很多旁路的支线将输入直接连到后面的层，使得后面的层可以直接学习残差，这种结构也被称为shortcut或skip connections。</p>
<p><img src="_image/residual.png"></p>
<p><strong>Inception V2</strong> 学习了VGGNet，用两个3´3的卷积代替5´5的大卷积（用以降低参数量并减轻过拟合），还提出了著名的Batch Normalization（以下简称BN）方法。</p>
<p><strong>Inception V3</strong> 网络则主要的改造：引入了Factorization into small convolutions的思想，将一个较大的二维卷积拆成两个较小的一维卷积，比如将7´7卷积拆成1´7卷积和7´1卷积，或者将3´3卷积拆成1´3卷积和3´1卷积，如图12所示。一方面节约了大量参数，加速运算并减轻了过拟合（比将7´7卷积拆成1´7卷积和7´1卷积，比拆成3个3´3卷积更节约参数）</p>
<p>总结：<br>在AlexNet之后，我们可以将卷积神经网络的发展分为两类，一类是网络结构上的改进调整（图18中的左侧分支），另一类是网络深度的增加（图18中的右侧分支）。<br><img src="_image/lenet-inception_restnet.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_channels, use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span>, **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(Residual, self).__init__()</span><br><span class="line">    self.zeropadding1 = tf.keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    self.conv1 = tf.keras.layers.Conv2D(num_channels, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=strides)</span><br><span class="line">    self.zeropadding2 = tf.keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    self.conv2 = tf.keras.layers.Conv2D(num_channels, kernel_size=(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">      self.conv3 = tf.keras.layers.Conv2D(num_channels, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=strides)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.conv3 = <span class="literal">None</span></span><br><span class="line">    self.bn1 = tf.keras.layers.BatchNormalization()</span><br><span class="line">    self.bn2 = tf.keras.layers.BatchNormalization()</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, X</span>):</span></span><br><span class="line">    Y = tf.keras.layers.ReLU()(self.bn1(self.conv1(self.zeropadding1(X))))</span><br><span class="line">    Y = self.bn2(self.conv2(self.zeropadding2(Y)))</span><br><span class="line">    <span class="keyword">if</span> self.conv3:</span><br><span class="line">      X = self.conv3(X)</span><br><span class="line">    <span class="keyword">return</span> tf.keras.layers.ReLU()(Y + X)</span><br><span class="line">residual = Residual(<span class="number">64</span>, use_1x1conv=<span class="literal">True</span>)</span><br><span class="line">test_inputs = tf.random.uniform(shape=(<span class="number">2</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>))</span><br><span class="line">residual(test_inputs).shape</span><br></pre></td></tr></table></figure>




<pre><code>TensorShape([2, 32, 32, 64])</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">resNet_app = tf.keras.applications.ResNet50(weights=<span class="literal">None</span>)</span><br><span class="line">resNet_app.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;resnet50&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_12 (InputLayer)           [(None, 224, 224, 3) 0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_12[0][0]                   
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           
__________________________________________________________________________________________________
predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   
==================================================================================================
Total params: 25,636,712
Trainable params: 25,583,592
Non-trainable params: 53,120
__________________________________________________________________________________________________</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.utils.plot_model(resNet_app)</span><br></pre></td></tr></table></figure>




<p><img src="_image/output_23_0.png" alt="png"></p>
<p>CNN技巧：</p>
<ol>
<li>“1Padding+3x3卷积” 与 “2Padding+5x5卷积”可以保持特征图长宽不变。</li>
<li>1x1卷积在不需要padding的情况下就可以保持特征图长宽不变，通常用来降低channel数量。</li>
<li>3x3卷积很有效（并能减少参数），2个3x3卷积=5x5卷积, 3个3x3卷积=7x7卷积（这里的等于指感受野）。</li>
<li>一个较大的二维卷积拆成两个较小的一维卷积，比如将7´7卷积拆成1´7卷积和7´1卷积。一方面节约了大量参数，加速运算并减轻了过拟合。</li>
<li>卷积在做特征提取时，通常不会改变特征图的长宽，则是通用pooling层来减小特征图长宽。</li>
<li>整个卷积网络变化趋势：特征图长宽逐渐减少（提高感受野），通道数量逐渐增加（提取更多特征信息）。</li>
</ol>
<h3 id="图片分类"><a href="#图片分类" class="headerlink" title="图片分类"></a>图片分类</h3><h4 id="数据下载"><a href="#数据下载" class="headerlink" title="数据下载"></a>数据下载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !wget https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip</span></span><br><span class="line"><span class="comment"># !unzip cats_and_dogs_filtered.zip</span></span><br></pre></td></tr></table></figure>

<h4 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">PATH = os.path.join(<span class="string">&#x27;./&#x27;</span>, <span class="string">&#x27;cats_and_dogs_filtered&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_dir = os.path.join(PATH, <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">validation_dir = os.path.join(PATH, <span class="string">&#x27;validation&#x27;</span>)</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">IMG_SIZE = (<span class="number">160</span>, <span class="number">160</span>)</span><br><span class="line"></span><br><span class="line">train_dataset = tf.keras.preprocessing.image_dataset_from_directory(train_dir,</span><br><span class="line">                        shuffle=<span class="literal">True</span>,</span><br><span class="line">                        batch_size=BATCH_SIZE,</span><br><span class="line">                        image_size=IMG_SIZE)</span><br><span class="line"></span><br><span class="line">validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(validation_dir,</span><br><span class="line">                          shuffle=<span class="literal">True</span>,</span><br><span class="line">                          batch_size=BATCH_SIZE,</span><br><span class="line">                          image_size=IMG_SIZE)</span><br><span class="line"></span><br><span class="line">class_names = train_dataset.class_names</span><br><span class="line">num_classes = <span class="built_in">len</span>(class_names)</span><br><span class="line">print(class_names)</span><br></pre></td></tr></table></figure>

<pre><code>Found 2000 files belonging to 2 classes.
Found 1000 files belonging to 2 classes.
[&#39;cats&#39;, &#39;dogs&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_data</span>(<span class="params">dataset</span>):</span></span><br><span class="line">  plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">  <span class="keyword">for</span> images, labels <span class="keyword">in</span> dataset.take(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">      ax = plt.subplot(<span class="number">3</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">      plt.imshow(images[i].numpy().astype(<span class="string">&quot;uint8&quot;</span>))</span><br><span class="line">      plt.title(class_names[labels[i]])</span><br><span class="line">      plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line">show_data(train_dataset)</span><br></pre></td></tr></table></figure>


<p><img src="_image/output_29_0.png" alt="png"></p>
<h4 id="构建模型并训练"><a href="#构建模型并训练" class="headerlink" title="构建模型并训练"></a>构建模型并训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_model</span>():</span></span><br><span class="line">  model = Sequential([</span><br><span class="line">    layers.experimental.preprocessing.Rescaling(<span class="number">1.</span>/<span class="number">255</span>, input_shape=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">3</span>)),</span><br><span class="line">    layers.Conv2D(<span class="number">16</span>, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.MaxPooling2D(),</span><br><span class="line">    layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.MaxPooling2D(),</span><br><span class="line">    layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.MaxPooling2D(),</span><br><span class="line">    layers.Flatten(),</span><br><span class="line">    layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(num_classes)</span><br><span class="line">  ])</span><br><span class="line">  <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">model, train_dataset, validation_dataset, epochs=<span class="number">10</span>,</span>):</span></span><br><span class="line">  model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">          metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  history = model.fit(</span><br><span class="line">    train_dataset,</span><br><span class="line">    validation_data=validation_dataset,</span><br><span class="line">    epochs=epochs</span><br><span class="line">  )</span><br><span class="line">  <span class="keyword">return</span> history</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_history</span>(<span class="params">history, epochs=<span class="number">10</span></span>):</span></span><br><span class="line">  acc = history.history[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">  val_acc = history.history[<span class="string">&#x27;val_accuracy&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  loss=history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">  val_loss=history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  epochs_range = <span class="built_in">range</span>(epochs)</span><br><span class="line"></span><br><span class="line">  plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">  plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">  plt.plot(epochs_range, acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">  plt.plot(epochs_range, val_acc, label=<span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">  plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Training and Validation Accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line">  plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">  plt.plot(epochs_range, loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">  plt.plot(epochs_range, val_loss, label=<span class="string">&#x27;Validation Loss&#x27;</span>)</span><br><span class="line">  plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Training and Validation Loss&#x27;</span>)</span><br><span class="line">  plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sample_model = generate_model()</span><br><span class="line">history = train_model(sample_model, train_dataset, validation_dataset, epochs=<span class="number">10</span>)</span><br><span class="line">plot_history(history)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/10
63/63 [==============================] - 69s 1s/step - loss: 0.7187 - accuracy: 0.5605 - val_loss: 0.6372 - val_accuracy: 0.6250
Epoch 2/10
63/63 [==============================] - 32s 516ms/step - loss: 0.6184 - accuracy: 0.6575 - val_loss: 0.6171 - val_accuracy: 0.6930
Epoch 3/10
63/63 [==============================] - 36s 564ms/step - loss: 0.5688 - accuracy: 0.7185 - val_loss: 0.6320 - val_accuracy: 0.6750
Epoch 4/10
63/63 [==============================] - 36s 566ms/step - loss: 0.5011 - accuracy: 0.7600 - val_loss: 0.6163 - val_accuracy: 0.6820
Epoch 5/10
63/63 [==============================] - 37s 580ms/step - loss: 0.4381 - accuracy: 0.7865 - val_loss: 0.6432 - val_accuracy: 0.7060
Epoch 6/10
63/63 [==============================] - 38s 607ms/step - loss: 0.3843 - accuracy: 0.8220 - val_loss: 0.6558 - val_accuracy: 0.7250
Epoch 7/10
63/63 [==============================] - 37s 583ms/step - loss: 0.2740 - accuracy: 0.8825 - val_loss: 0.7358 - val_accuracy: 0.7180
Epoch 8/10
63/63 [==============================] - 38s 604ms/step - loss: 0.1906 - accuracy: 0.9180 - val_loss: 0.8276 - val_accuracy: 0.7210
Epoch 9/10
63/63 [==============================] - 40s 629ms/step - loss: 0.1391 - accuracy: 0.9480 - val_loss: 0.9111 - val_accuracy: 0.7340
Epoch 10/10
63/63 [==============================] - 41s 648ms/step - loss: 0.0952 - accuracy: 0.9665 - val_loss: 1.0884 - val_accuracy: 0.7120</code></pre>
<p><img src="_image/output_33_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_model_with_data_augmentation_and_dropout</span>():</span></span><br><span class="line">  data_augmentation = keras.Sequential(</span><br><span class="line">    [</span><br><span class="line">      layers.experimental.preprocessing.RandomFlip(<span class="string">&quot;horizontal&quot;</span>, input_shape=(<span class="number">160</span>, <span class="number">160</span>,<span class="number">3</span>)),</span><br><span class="line">      layers.experimental.preprocessing.RandomRotation(<span class="number">0.1</span>),</span><br><span class="line">      layers.experimental.preprocessing.RandomZoom(<span class="number">0.1</span>),</span><br><span class="line">    ]</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  model = Sequential([</span><br><span class="line">    data_augmentation,</span><br><span class="line">    layers.experimental.preprocessing.Rescaling(<span class="number">1.</span>/<span class="number">255</span>),</span><br><span class="line">    layers.Conv2D(<span class="number">16</span>, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.MaxPooling2D(),</span><br><span class="line">    layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.MaxPooling2D(),</span><br><span class="line">    layers.Conv2D(<span class="number">64</span>, <span class="number">3</span>, padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.MaxPooling2D(),</span><br><span class="line">    layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    layers.Flatten(),</span><br><span class="line">    layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    layers.Dense(num_classes)</span><br><span class="line">  ])</span><br><span class="line">  <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with_data_augmentation_and_dropout_model = generate_model_with_data_augmentation_and_dropout()</span><br><span class="line">history = train_model(with_data_augmentation_and_dropout_model, train_dataset, validation_dataset, epochs=<span class="number">15</span>)</span><br><span class="line">plot_history(history, epochs=<span class="number">15</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/15
63/63 [==============================] - 35s 563ms/step - loss: 0.7666 - accuracy: 0.5060 - val_loss: 0.6933 - val_accuracy: 0.5000
Epoch 2/15
63/63 [==============================] - 36s 570ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6924 - val_accuracy: 0.5900
Epoch 3/15
63/63 [==============================] - 35s 552ms/step - loss: 0.6944 - accuracy: 0.5090 - val_loss: 0.6920 - val_accuracy: 0.5860
Epoch 4/15
63/63 [==============================] - 35s 555ms/step - loss: 0.6894 - accuracy: 0.5420 - val_loss: 0.6890 - val_accuracy: 0.5810
Epoch 5/15
63/63 [==============================] - 34s 546ms/step - loss: 0.6824 - accuracy: 0.5685 - val_loss: 0.6842 - val_accuracy: 0.5640
Epoch 6/15
63/63 [==============================] - 35s 550ms/step - loss: 0.6776 - accuracy: 0.5735 - val_loss: 0.6683 - val_accuracy: 0.5840
Epoch 7/15
63/63 [==============================] - 35s 559ms/step - loss: 0.6499 - accuracy: 0.6380 - val_loss: 0.6459 - val_accuracy: 0.6110
Epoch 8/15
63/63 [==============================] - 35s 560ms/step - loss: 0.6231 - accuracy: 0.6710 - val_loss: 0.6170 - val_accuracy: 0.6690
Epoch 9/15
63/63 [==============================] - 36s 567ms/step - loss: 0.6075 - accuracy: 0.6870 - val_loss: 0.5888 - val_accuracy: 0.7010
Epoch 10/15
63/63 [==============================] - 35s 558ms/step - loss: 0.5903 - accuracy: 0.6930 - val_loss: 0.5931 - val_accuracy: 0.6900
Epoch 11/15
63/63 [==============================] - 36s 573ms/step - loss: 0.5764 - accuracy: 0.7070 - val_loss: 0.5924 - val_accuracy: 0.6880
Epoch 12/15
63/63 [==============================] - 35s 562ms/step - loss: 0.5679 - accuracy: 0.7160 - val_loss: 0.5671 - val_accuracy: 0.7040
Epoch 13/15
63/63 [==============================] - 35s 559ms/step - loss: 0.5727 - accuracy: 0.7105 - val_loss: 0.5832 - val_accuracy: 0.6940
Epoch 14/15
63/63 [==============================] - 36s 571ms/step - loss: 0.5539 - accuracy: 0.7225 - val_loss: 0.5918 - val_accuracy: 0.7000
Epoch 15/15
63/63 [==============================] - 36s 565ms/step - loss: 0.5497 - accuracy: 0.7160 - val_loss: 0.5694 - val_accuracy: 0.6990</code></pre>
<p><img src="_image/output_35_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_model_with_vgg</span>():</span></span><br><span class="line">  vgg16 = tf.keras.applications.VGG16(include_top=<span class="literal">False</span>, weights=<span class="literal">None</span>, input_shape=(<span class="number">160</span>, <span class="number">160</span>, <span class="number">3</span>))</span><br><span class="line">  vgg16.load_weights(<span class="string">&#x27;vgg16_weights_tf_dim_ordering_tf_kernels.h5&#x27;</span>, by_name=<span class="literal">True</span>)</span><br><span class="line">  vgg16.trainable=<span class="literal">False</span></span><br><span class="line">  data_augmentation = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.experimental.preprocessing.RandomFlip(<span class="string">&#x27;horizontal&#x27;</span>),</span><br><span class="line">    tf.keras.layers.experimental.preprocessing.RandomRotation(<span class="number">0.2</span>),</span><br><span class="line">  ])</span><br><span class="line"></span><br><span class="line">  preprocess_input = tf.keras.applications.vgg16.preprocess_input</span><br><span class="line"></span><br><span class="line">  inputs = tf.keras.Input(shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line">  x = data_augmentation(inputs)</span><br><span class="line">  x = preprocess_input(x)</span><br><span class="line">  x = vgg16(x, training=<span class="literal">True</span>)</span><br><span class="line">  x = tf.keras.layers.GlobalAveragePooling2D()(x)</span><br><span class="line">  x = tf.keras.layers.Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">  outputs = tf.keras.layers.Dense(num_classes)(x)</span><br><span class="line">  model = tf.keras.Model(inputs, outputs)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with_vgg_model = generate_model_with_vgg()</span><br><span class="line">history = train_model(with_vgg_model, train_dataset, validation_dataset, epochs=<span class="number">10</span>)</span><br><span class="line">plot_history(history, epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<pre><code>WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input Tensor(&quot;input_9:0&quot;, shape=(None, 160, 160, 3), dtype=float32), but it was called on an input with incompatible shape (None, 224, 224, 3).
Epoch 1/10
WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(&quot;input_10:0&quot;, shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 160, 160, 3).
WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(&quot;input_10:0&quot;, shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 160, 160, 3).
63/63 [==============================] - ETA: 0s - loss: 2.2022 - accuracy: 0.7145WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(&quot;input_10:0&quot;, shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 160, 160, 3).
63/63 [==============================] - 335s 5s/step - loss: 2.2022 - accuracy: 0.7145 - val_loss: 0.6007 - val_accuracy: 0.8890
Epoch 2/10
63/63 [==============================] - 321s 5s/step - loss: 0.8997 - accuracy: 0.8455 - val_loss: 0.3511 - val_accuracy: 0.9260
Epoch 3/10
63/63 [==============================] - 317s 5s/step - loss: 0.6832 - accuracy: 0.8770 - val_loss: 0.2706 - val_accuracy: 0.9440
Epoch 4/10
63/63 [==============================] - 379s 6s/step - loss: 0.5846 - accuracy: 0.8890 - val_loss: 0.2178 - val_accuracy: 0.9620
Epoch 5/10
63/63 [==============================] - 335s 5s/step - loss: 0.5474 - accuracy: 0.9040 - val_loss: 0.2843 - val_accuracy: 0.9450
Epoch 6/10
63/63 [==============================] - 353s 6s/step - loss: 0.4610 - accuracy: 0.9095 - val_loss: 0.1814 - val_accuracy: 0.9570
Epoch 7/10
63/63 [==============================] - 399s 6s/step - loss: 0.4025 - accuracy: 0.9105 - val_loss: 0.1666 - val_accuracy: 0.9640
Epoch 8/10
63/63 [==============================] - 369s 6s/step - loss: 0.3679 - accuracy: 0.9250 - val_loss: 0.1574 - val_accuracy: 0.9610
Epoch 9/10
63/63 [==============================] - 344s 5s/step - loss: 0.3023 - accuracy: 0.9200 - val_loss: 0.1745 - val_accuracy: 0.9650
Epoch 10/10
63/63 [==============================] - 336s 5s/step - loss: 0.3916 - accuracy: 0.9170 - val_loss: 0.1473 - val_accuracy: 0.9650</code></pre>
<p><img src="_image/output_37_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evalation</span>(<span class="params">model, dataset</span>):</span></span><br><span class="line">  image_batch, label_batch = dataset.as_numpy_iterator().<span class="built_in">next</span>()</span><br><span class="line">  predictions = model.predict(image_batch)</span><br><span class="line">  predictions = tf.math.argmax(predictions, axis=-<span class="number">1</span>)</span><br><span class="line">  print(<span class="string">&#x27;Predictions:\n&#x27;</span>, predictions.numpy())</span><br><span class="line">  print(<span class="string">&#x27;Labels:\n&#x27;</span>, label_batch)</span><br><span class="line"></span><br><span class="line">  plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">    ax = plt.subplot(<span class="number">3</span>, <span class="number">3</span>, i + <span class="number">1</span>)</span><br><span class="line">    plt.imshow(image_batch[i].astype(<span class="string">&quot;uint8&quot;</span>))</span><br><span class="line">    plt.title(class_names[predictions[i]])</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">evalation(with_vgg_model, validation_dataset)</span><br></pre></td></tr></table></figure>

<pre><code>Predictions:
 [1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0]
Labels:
 [1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0]</code></pre>
<p><img src="_image/output_39_1.png" alt="png"></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tensorflow/" rel="tag"># 深度学习 Tensorflow</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/07/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-07-29-Batch_Normalization%E4%B8%8ELayer_Normalization%E7%9A%84%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86/" rel="prev" title="Batch Normalization与Layer Normalization的理解整理">
      <i class="fa fa-chevron-left"></i> Batch Normalization与Layer Normalization的理解整理
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/10/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-10-16-DeepLearning%E4%B8%ADCRF%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86/" rel="next" title="DeepLearning中CRF计算原理">
      DeepLearning中CRF计算原理 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%AE%BA%E9%83%A8%E5%88%86"><span class="nav-number">1.</span> <span class="nav-text">理论部分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CNN%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.1.</span> <span class="nav-text">CNN解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">1.2.</span> <span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">1.3.</span> <span class="nav-text">池化层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%88%98%E9%83%A8%E5%88%86"><span class="nav-number">2.</span> <span class="nav-text">实战部分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LeNet"><span class="nav-number">2.1.</span> <span class="nav-text">LeNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AlexNet"><span class="nav-number">2.2.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG"><span class="nav-number">2.3.</span> <span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GoogleNet"><span class="nav-number">2.4.</span> <span class="nav-text">GoogleNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet"><span class="nav-number">2.5.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB"><span class="nav-number">2.6.</span> <span class="nav-text">图片分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%B8%8B%E8%BD%BD"><span class="nav-number">2.6.1.</span> <span class="nav-text">数据下载</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96"><span class="nav-number">2.6.2.</span> <span class="nav-text">数据读取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%AE%AD%E7%BB%83"><span class="nav-number">2.6.3.</span> <span class="nav-text">构建模型并训练</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hwyoung"
      src="/avatar.png">
  <p class="site-author-name" itemprop="name">hwyoung</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">77</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hwyoung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
