<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="大家好，今天是Tensorflow实战系列的第一次分享，本次分享共有3个主题：  Tensorflow介绍 Tensorflow核心概念 使用Tensorflow实现手写数字识别任务  由于本次分享内容以实战为主不会涉及过多理论的讲解，但是分享过程中涉及到的理论知识也会做一个快速回顾。 Tensorflow IntroductionTensorflow 是由Google研发的开源软件库，它既是一个">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络">
<meta property="og:url" content="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Writing Lite">
<meta property="og:description" content="大家好，今天是Tensorflow实战系列的第一次分享，本次分享共有3个主题：  Tensorflow介绍 Tensorflow核心概念 使用Tensorflow实现手写数字识别任务  由于本次分享内容以实战为主不会涉及过多理论的讲解，但是分享过程中涉及到的理论知识也会做一个快速回顾。 Tensorflow IntroductionTensorflow 是由Google研发的开源软件库，它既是一个">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/_image/framework.png">
<meta property="og:image" content="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/_image/tensor.png">
<meta property="og:image" content="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/_image/grapth_compute.png">
<meta property="og:image" content="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/_image/output_17_0.png">
<meta property="og:image" content="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/_image/fully_connect_model.png">
<meta property="og:image" content="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/_image/fully_connect_nn.png">
<meta property="og:image" content="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/_image/tensorflow2.png">
<meta property="og:image" content="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/_image/output_63_0.png">
<meta property="article:published_time" content="2020-06-24T20:43:00.000Z">
<meta property="article:modified_time" content="2020-12-02T06:52:04.933Z">
<meta property="article:author" content="hwyoung">
<meta property="article:tag" content="深度学习 Tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/_image/framework.png">

<link rel="canonical" href="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络 | Writing Lite</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Writing Lite</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/06/24/Tensorflow2.x%E5%AE%9E%E6%88%98/Tensorflow%E5%AE%9E%E6%88%98(1)-Tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/avatar.png">
      <meta itemprop="name" content="hwyoung">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Writing Lite">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-24 20:43:00" itemprop="dateCreated datePublished" datetime="2020-06-24T20:43:00+00:00">2020-06-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-12-02 06:52:04" itemprop="dateModified" datetime="2020-12-02T06:52:04+00:00">2020-12-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>大家好，今天是Tensorflow实战系列的第一次分享，本次分享共有3个主题：</p>
<ul>
<li>Tensorflow介绍</li>
<li>Tensorflow核心概念</li>
<li>使用Tensorflow实现手写数字识别任务</li>
</ul>
<p>由于本次分享内容以实战为主不会涉及过多理论的讲解，但是分享过程中涉及到的理论知识也会做一个快速回顾。</p>
<h2 id="Tensorflow-Introduction"><a href="#Tensorflow-Introduction" class="headerlink" title="Tensorflow Introduction"></a>Tensorflow Introduction</h2><p>Tensorflow 是由Google研发的开源软件库，它既是一个实现机器学习算法的接口，同时也是执行机器学习算法的框架，它对深度学习中常用的神经网络结构等算法进行了封装，因此开发人员可以快速的进行模型搭建。</p>
<h3 id="1、Tensorflow-发展史"><a href="#1、Tensorflow-发展史" class="headerlink" title="1、Tensorflow 发展史"></a>1、Tensorflow 发展史</h3><ul>
<li>2011 年，Google Brain内部孵化出一个项目叫做DistBelief, 它是为深度神经网络构建的一个机器学习系统，是Tensorflow的前身。</li>
<li>2015年11月，Google正式发布了Tensorflow的白皮书并开源TensorFlow 0.1 版本。</li>
<li>2017年02月，Tensorflow正式发布了1.0.0版本，同时也标志着稳定版的诞生。</li>
<li>2019年10月，TensorFlow在经历七个多月(2019年3月1日-2019年10月1日)的2.0 Alpha 版本的更新迭代后发布 2.0 正式版。</li>
</ul>
<p>通过上面的发展史我们可以看到，虽然经过了9年时间Tensorflow依然是目前最流行的深度学习框架之一。</p>
<p><img src="./_image/framework.png"></p>
<h3 id="2、Tensorflow-VS-Pytorch"><a href="#2、Tensorflow-VS-Pytorch" class="headerlink" title="2、Tensorflow VS Pytorch"></a>2、Tensorflow VS Pytorch</h3><p>上面说到Tensorflow是目前最流行的深度学习框架之一，那另一款可以和Tensorflow一较高下的深度学习框架就是-Pytorch了。Pytorch是由Facebook研发的一款开源的机器学习库，自16年发布以来发展非常迅猛。Tensorflow和Pytorch如何选择呢，我的看法是：都可以，虽然刚开始时Pytorch和Tensorflow还是差别较大的，比较Pytorch有动态图、类python的编程方式，Tensorflow则支持可视化，生产部署更加简单易用，但通过这几年的发展Pytorch和Tensorfow越来越像了，Tensorflow添加了动态图，而Pytorch也在工业部署上有了很大改善。因此在两都的选择上不必太过纠结。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">print(sys.version)</span><br></pre></td></tr></table></figure>

<pre><code>3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> tf, np, mpl:</span><br><span class="line">    print(module.__name__, module.__version__)</span><br></pre></td></tr></table></figure>

<pre><code>tensorflow 2.1.0
numpy 1.18.1
matplotlib 3.2.1</code></pre>
<h2 id="Tensorflow-核心概念"><a href="#Tensorflow-核心概念" class="headerlink" title="Tensorflow 核心概念"></a>Tensorflow 核心概念</h2><h3 id="1、Tensor-张量"><a href="#1、Tensor-张量" class="headerlink" title="1、Tensor (张量)"></a>1、Tensor (张量)</h3><p>在Tensorflow中计算的数据都是以Tensor的形式来表示的。Tensor 可以把它看成是一个多维数组，并且它和NumPy中的np.arrays也非常相似。它有3个重要的属性：</p>
<ul>
<li>value </li>
<li>shape</li>
<li>dtype</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rank_3_tensor = tf.constant([</span><br><span class="line">  [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">   [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]],</span><br><span class="line">  [[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>],</span><br><span class="line">   [<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>]],</span><br><span class="line">  [[<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>],</span><br><span class="line">   [<span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>]],])</span><br><span class="line">                    </span><br><span class="line">print(rank_3_tensor)</span><br></pre></td></tr></table></figure>

<pre><code>tf.Tensor(
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]

 [[10 11 12 13 14]
  [15 16 17 18 19]]

 [[20 21 22 23 24]
  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)</code></pre>
<p><img src="./_image/tensor.png" alt="tensor"></p>
<p>tensor的dypte和编程语言中的变量类型非常相似，只是因此模型计算过程中运算量非常大对精度和粒度的要求更高。</p>
<ul>
<li>tf.float16: 16-bit half-precision floating-point.</li>
<li>tf.float32: 32-bit single-precision floating-point.</li>
<li>tf.float64: 64-bit double-precision floating-point.</li>
<li>tf.int8: 8-bit signed integer.</li>
<li>tf.int16: 16-bit signed integer.</li>
<li>tf.int32: 32-bit signed integer.</li>
<li>tf.int64: 64-bit signed integer.</li>
<li>tf.bool: Boolean.</li>
<li>tf.string: String.</li>
</ul>
<p>更好可参考：<a target="_blank" rel="noopener" href="https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/dtypes/DType">https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/dtypes/DType</a></p>
<h3 id="2、Compute-Grapth-计算图-与Autograd-自动求导"><a href="#2、Compute-Grapth-计算图-与Autograd-自动求导" class="headerlink" title="2、Compute Grapth (计算图) 与Autograd(自动求导)"></a>2、Compute Grapth (计算图) 与Autograd(自动求导)</h3><p>计算图是一种描述计算过程的语言。图的节点由事先定义的运算构成，图的各个节点之间由张量（tensor）来连接，Tensorflow的计算过程就是张量（tensor）在节点之间从前到后的流动传输过程。另外在图上计算变量的梯度也非常容易，在使用梯度下降求解模型参数时非常方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = tf.Variable(<span class="number">3.</span>)</span><br><span class="line">b = tf.Variable(<span class="number">2.</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    c = a + b</span><br><span class="line">    d = b + <span class="number">1</span></span><br><span class="line">    e = c * d</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;e :&#x27;</span>, e.numpy())</span><br><span class="line"></span><br><span class="line">a_gradient, b_gradient = tape.gradient(e, [a, b])</span><br><span class="line">print(<span class="string">&#x27;b_gradient : &#x27;</span>, b_gradient.numpy())</span><br><span class="line">print(<span class="string">&#x27;a_gradient : &#x27;</span>, a_gradient.numpy())</span><br></pre></td></tr></table></figure>

<pre><code>e : 15.0
b_gradient :  8.0
a_gradient :  3.0</code></pre>
<p><img src="./_image/grapth_compute.png" alt="计算图"></p>
<p>以求e关于变量b的偏导数为例，从e到b的路径有两条，将每条路径中的值相乘再将种路径的值相加即可得到最终的偏导数结果。</p>
<p>即使不了解计算图等概念也不会影响我们使用Tensorflow来完成各式各样任务，但是它可以帮忙我们了解Tensorflow的运作机制，在遇到问题时可以更合理的问题问题原因。</p>
<h2 id="模型和数据说明"><a href="#模型和数据说明" class="headerlink" title="模型和数据说明"></a>模型和数据说明</h2><p>在讲解后面的内容之前，先对之后会使用的模型和数据做一个回顾和说明。后面的例子中会搭建一个全连接的神经网络来实现对手写数字识别任务。</p>
<h3 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h3><p>本次分享我们主要使用的数据是 MNIST， MNIST Dataset 是一个手写数字数据集，其包含 60,000 个示例训练集和 10,000 个示例测试集，它主要用于机器视觉领域的图像分类，每个样本是28*28的灰度图片，共0-9 10个类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train_all, y_train_all), (x_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果第一层layer的activation是relu的话，这里要做规一化</span></span><br><span class="line">x_train_all, x_test = x_train_all/<span class="number">255.0</span>, x_test/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练集中取前5000做为验证集</span></span><br><span class="line">x_valid, x_train = x_train_all[:<span class="number">5000</span>], x_train_all[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_all[:<span class="number">5000</span>], y_train_all[<span class="number">5000</span>:]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(x_valid.shape, y_valid.shape)</span><br><span class="line">print(x_train.shape, y_train.shape)</span><br><span class="line">print(x_test.shape, y_test.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(5000, 28, 28) (5000,)
(55000, 28, 28) (55000,)
(10000, 28, 28) (10000,)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotImages</span>(<span class="params">images</span>):</span></span><br><span class="line">    fig, axes = plt.subplots(<span class="number">1</span>, <span class="built_in">len</span>(images), figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> img, ax <span class="keyword">in</span> <span class="built_in">zip</span>(images, axes):</span><br><span class="line">        ax.imshow(img, interpolation=<span class="string">&quot;nearest&quot;</span>, cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">        ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plotImages(x_train[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>


<p><img src="./_image/output_17_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;label ：&#x27;</span>, y_train[<span class="number">0</span>:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<pre><code>label ： [7 3 4 6 1]</code></pre>
<h3 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h3><p>全连接神经网络(fully connected neural network)，顾名思义，就是相邻两层之间任意两个节点之间都有连接。全连接神经网络是最为普通的一种模型（比如和CNN相比），由于是全连接，所以会有更多的权重值和连接，因此也意味着占用更多的内存和计算。</p>
<p><img src="./_image/fully_connect_model.png" alt="全连接神经网络模型"></p>
<p>全连接层的计算方式：<br><img src="./_image/fully_connect_nn.png" alt="全连接层"></p>
<p>推荐演示工具：<a target="_blank" rel="noopener" href="https://playground.tensorflow.org/">https://playground.tensorflow.org/</a></p>
<h2 id="Tensorlfow2搭建一个全连接神经网络"><a href="#Tensorlfow2搭建一个全连接神经网络" class="headerlink" title="Tensorlfow2搭建一个全连接神经网络"></a>Tensorlfow2搭建一个全连接神经网络</h2><h3 id="0、Why-Tensorflow2-X"><a href="#0、Why-Tensorflow2-X" class="headerlink" title="0、Why Tensorflow2.X"></a>0、Why Tensorflow2.X</h3><pre><code>Tensorflow1.X 版本的编程方式更像是在画一张计算图，通常构建一个模型的时候是先定义一张图，然后在图中添加计算结点，最终将这个张图拿去做计算。而2.X将计算图的构建过程隐藏在底层中了，这使得它的语法看起来更加友好也比较符合正常的编程思维。</code></pre>
<p><img src="./_image/tensorflow2.png" alt="tensorflow_keras"></p>
<p>在TF2版本中，有两种高级API，分别是Estimator和tf.keras，Estimator早在TF1版本就已经出现而tf.keras是TF2中新增加的。后面的内容主要会以tf.keras为主。</p>
<h3 id="1、定义模型结构"><a href="#1、定义模型结构" class="headerlink" title="1、定义模型结构"></a>1、定义模型结构</h3><p>**Sequential API (连续)**：Sequential API 通过model.add方法添加神经网络层，适合链式结构的神经网络。</p>
<p>**Functional API (函数式)**：Functional API 通过指定inputs和outputs，将inputs到outputs的中间计算过程做为模型计算逻辑。它可以让计算逻辑更加灵活不局限于链式这样的简单结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_sequential_model</span>():</span></span><br><span class="line">    model = tf.keras.Sequential(name=<span class="string">&#x27;sequential_model&#x27;</span>)</span><br><span class="line">    model.add(tf.keras.layers.Flatten(input_shape=[<span class="number">28</span>, <span class="number">28</span>], name=<span class="string">&#x27;flatten&#x27;</span>))</span><br><span class="line">    model.add(tf.keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&#x27;h1&#x27;</span>))</span><br><span class="line">    model.add(tf.keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&#x27;h2&#x27;</span>)) </span><br><span class="line">    model.add(tf.keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&#x27;h3&#x27;</span>))</span><br><span class="line">    model.add(tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>, name=<span class="string">&#x27;outputs&#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">sequential_model = generate_sequential_model()</span><br><span class="line">sequential_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
h1 (Dense)                   (None, 300)               235500    
_________________________________________________________________
h2 (Dense)                   (None, 200)               60200     
_________________________________________________________________
h3 (Dense)                   (None, 100)               20100     
_________________________________________________________________
outputs (Dense)              (None, 10)                1010      
=================================================================
Total params: 316,810
Trainable params: 316,810
Non-trainable params: 0
_________________________________________________________________</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_functional_model</span>():</span></span><br><span class="line">    inputs = tf.keras.Input(shape=[<span class="number">28</span>, <span class="number">28</span>], name=<span class="string">&#x27;inputs&#x27;</span>)</span><br><span class="line">    flatten = tf.keras.layers.Flatten(name=<span class="string">&#x27;flatten&#x27;</span>)(inputs)</span><br><span class="line">    h1 = tf.keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&#x27;h1&#x27;</span>)(flatten)</span><br><span class="line">    h2 = tf.keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&#x27;h2&#x27;</span>)(h1)</span><br><span class="line">    h3 = tf.keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&#x27;h3&#x27;</span>)(h2)</span><br><span class="line">    outputs = tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>, name=<span class="string">&#x27;outputs&#x27;</span>)(h3)</span><br><span class="line">    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=<span class="string">&quot;functional_model&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line">functioal_model = generate_functional_model()</span><br><span class="line">functioal_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;functional_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
inputs (InputLayer)          [(None, 28, 28)]          0         
_________________________________________________________________
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
h1 (Dense)                   (None, 300)               235500    
_________________________________________________________________
h2 (Dense)                   (None, 200)               60200     
_________________________________________________________________
h3 (Dense)                   (None, 100)               20100     
_________________________________________________________________
outputs (Dense)              (None, 10)                1010      
=================================================================
Total params: 316,810
Trainable params: 316,810
Non-trainable params: 0
_________________________________________________________________</code></pre>
<h3 id="2、Loss-Function-损失函数"><a href="#2、Loss-Function-损失函数" class="headerlink" title="2、Loss Function (损失函数)"></a>2、Loss Function (损失函数)</h3><p>交叉熵损失：tf.keras.losses.SparseCategoricalCrossentropy，常用的适用于分类任务的Loss。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cce = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">loss = cce(</span><br><span class="line">  tf.convert_to_tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]),</span><br><span class="line">  tf.convert_to_tensor([[<span class="number">.9</span>, <span class="number">.05</span>, <span class="number">.05</span>], [<span class="number">.5</span>, <span class="number">.89</span>, <span class="number">.6</span>], [<span class="number">.05</span>, <span class="number">.01</span>, <span class="number">.94</span>]]))</span><br><span class="line">print(<span class="string">&#x27;Loss: &#x27;</span>, loss.numpy())  <span class="comment"># Loss: 0.3239</span></span><br></pre></td></tr></table></figure>

<pre><code>Loss:  0.32396814</code></pre>
<h3 id="3、Optimaizer-优化方法"><a href="#3、Optimaizer-优化方法" class="headerlink" title="3、Optimaizer (优化方法)"></a>3、Optimaizer (优化方法)</h3><pre><code>在确定了损失函数之后，我们就可以选择一个优化算法来让损失函数最小化。以SGD举例：</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">a = tf.Variable(<span class="number">3.</span>)</span><br><span class="line">b = tf.Variable(<span class="number">2.</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    c = a + b</span><br><span class="line">    d = b + <span class="number">1</span></span><br><span class="line">    e = c * d</span><br><span class="line"></span><br><span class="line">a_gradient, b_gradient = tape.gradient(e, [a, b])</span><br><span class="line">print(<span class="string">&#x27;b_gradient : &#x27;</span>, b_gradient.numpy())</span><br><span class="line">print(<span class="string">&#x27;a_gradient : &#x27;</span>, a_gradient.numpy())</span><br><span class="line"></span><br><span class="line">sgd_optimizer.apply_gradients([(a_gradient, a), (b_gradient, b)])</span><br><span class="line">print(<span class="string">&#x27;a update: &#x27;</span>, a.numpy())</span><br><span class="line">print(<span class="string">&#x27;b update: &#x27;</span>, b.numpy())</span><br></pre></td></tr></table></figure>

<pre><code>b_gradient :  8.0
a_gradient :  3.0
a update:  2.97
b update:  1.92</code></pre>
<h3 id="4、Metrics-评价方法"><a href="#4、Metrics-评价方法" class="headerlink" title="4、Metrics (评价方法)"></a>4、Metrics (评价方法)</h3><p> 评价函数和<strong>损失函数</strong>相似，只不过评价函数的结果不会用于训练过程中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sca = tf.keras.metrics.SparseCategoricalAccuracy() </span><br><span class="line">_ = sca.update_state([<span class="number">2</span>, <span class="number">1</span>], [[<span class="number">0.02</span>, <span class="number">0.9</span>, <span class="number">0.08</span>], [<span class="number">0.05</span>, <span class="number">0.95</span>, <span class="number">0</span>]]) </span><br><span class="line">print(sca.result().numpy() )</span><br></pre></td></tr></table></figure>

<pre><code>0.5</code></pre>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>上面提到的模型结构、损失函数、优化方法是构成模型必不可少的3个要素。在集齐这些要素之后就要可以拿数据训练模型了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = functioal_model</span><br><span class="line">model.<span class="built_in">compile</span>(loss=cce, optimizer=sgd_optimizer, metrics=[sca])</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">2</span>, validation_data=(x_valid, y_valid))</span><br></pre></td></tr></table></figure>

<pre><code>Train on 55000 samples, validate on 5000 samples
Epoch 1/2
55000/55000 [==============================] - 5s 94us/sample - loss: 0.6173 - sparse_categorical_accuracy: 0.8341 - val_loss: 0.2859 - val_sparse_categorical_accuracy: 0.9198
Epoch 2/2
55000/55000 [==============================] - 5s 87us/sample - loss: 0.2660 - sparse_categorical_accuracy: 0.9233 - val_loss: 0.2109 - val_sparse_categorical_accuracy: 0.9404</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在测试集上的效果</span></span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>

<pre><code>10000/10000 [==============================] - 0s 50us/sample - loss: 0.2149 - sparse_categorical_accuracy: 0.9377





[0.2148841947108507, 0.9377]</code></pre>
<h2 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h2><h3 id="通过callbacks的方式进行保存"><a href="#通过callbacks的方式进行保存" class="headerlink" title="通过callbacks的方式进行保存"></a>通过callbacks的方式进行保存</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output_model_file = <span class="string">&quot;./mnist_model_callback.h5&quot;</span></span><br><span class="line">callbacks = [</span><br><span class="line">    tf.keras.callbacks.ModelCheckpoint(output_model_file, save_best_only=<span class="literal">True</span>, save_weights_only=<span class="literal">False</span>)</span><br><span class="line">]</span><br><span class="line">history = model.fit(x_train, y_train, epochs=<span class="number">2</span>, validation_data=(x_valid, y_valid), callbacks=callbacks)</span><br></pre></td></tr></table></figure>

<pre><code>Train on 55000 samples, validate on 5000 samples
Epoch 1/2
55000/55000 [==============================] - 5s 90us/sample - loss: 0.2079 - sparse_categorical_accuracy: 0.9392 - val_loss: 0.1790 - val_sparse_categorical_accuracy: 0.9520
Epoch 2/2
55000/55000 [==============================] - 5s 89us/sample - loss: 0.1709 - sparse_categorical_accuracy: 0.9506 - val_loss: 0.1563 - val_sparse_categorical_accuracy: 0.9576</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;训练后的模型参数：&#x27;</span>, model.variables[<span class="number">0</span>].numpy()[<span class="number">0</span>, :<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<pre><code>训练后的模型参数： [ 0.01215426 -0.05889301  0.04321195  0.05594952 -0.00950153]</code></pre>
<h3 id="保存成SavedModel"><a href="#保存成SavedModel" class="headerlink" title="保存成SavedModel"></a>保存成SavedModel</h3><p>在使用TensorFlow Serving时，会用到这种格式的模型，模型目录结构如下所示：</p>
<ul>
<li>assets是一个可选目录，用于存放预测时的辅助文档信息；</li>
<li>variables保存的变量信息；</li>
<li>saved_model.pb或saved_model.pbtxt存放MetaGraphDef，存储训练预测模型的程序逻辑</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saved_model_file = <span class="string">&quot;mnist_model_saved_model&quot;</span></span><br><span class="line">model.save(saved_model_file)</span><br></pre></td></tr></table></figure>

<pre><code>WARNING:tensorflow:From /home/hwyang/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
INFO:tensorflow:Assets written to: mnist_model_saved_model/assets</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型保存的输入输入格式</span></span><br><span class="line">!saved_model_cli show --<span class="built_in">dir</span> ./mnist_model_saved_model --tag_set serve --signature_def serving_default</span><br></pre></td></tr></table></figure>

<pre><code>2020-06-24 11:13:37.498139: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library &#39;libnvinfer.so.6&#39;; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-06-24 11:13:37.498235: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library &#39;libnvinfer_plugin.so.6&#39;; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-06-24 11:13:37.498296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
The given SavedModel SignatureDef contains the following input(s):
  inputs[&#39;inputs&#39;] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 28, 28)
      name: serving_default_inputs:0
The given SavedModel SignatureDef contains the following output(s):
  outputs[&#39;outputs&#39;] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 10)
      name: StatefulPartitionedCall:0
Method name is: tensorflow/serving/predict</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用模型</span></span><br><span class="line">!saved_model_cli run --<span class="built_in">dir</span> ./mnist_model_saved_model\</span><br><span class="line">    --tag_set serve --signature_def serving_default \</span><br><span class="line">    --input_exprs <span class="string">&#x27;inputs=np.ones((2, 28, 28))&#x27;</span></span><br></pre></td></tr></table></figure>

<pre><code>2020-06-24 11:13:39.641330: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library &#39;libnvinfer.so.6&#39;; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-06-24 11:13:39.641450: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library &#39;libnvinfer_plugin.so.6&#39;; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-06-24 11:13:39.641460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-06-24 11:13:40.425721: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library &#39;libcuda.so.1&#39;; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-06-24 11:13:40.425767: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2020-06-24 11:13:40.425785: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (work-computer): /proc/driver/nvidia/version does not exist
2020-06-24 11:13:40.426013: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-06-24 11:13:40.433525: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1799995000 Hz
2020-06-24 11:13:40.435146: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x402f9a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-06-24 11:13:40.435196: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /home/hwyang/.local/lib/python3.6/site-packages/tensorflow_core/python/tools/saved_model_cli.py:420: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
Result for output key outputs:
[[6.3519548e-07 4.2883544e-11 3.6132155e-04 3.7582448e-01 7.1212423e-15
  1.1317922e-03 1.6414268e-12 1.1638527e-12 6.2268174e-01 2.5810540e-10]
 [6.3519548e-07 4.2883544e-11 3.6132155e-04 3.7582448e-01 7.1212423e-15
  1.1317922e-03 1.6414268e-12 1.1638527e-12 6.2268174e-01 2.5810540e-10]]</code></pre>
<h3 id="两种保存方式的使用场景"><a href="#两种保存方式的使用场景" class="headerlink" title="两种保存方式的使用场景"></a>两种保存方式的使用场景</h3><p>1、在需要间断的训练模型时使用callbacks的方式更加合适，例如模型运行中被意外中断了就可以通过callbacks过程中保存的模型快速恢复之前的训练状态。</p>
<p>2、而当我们要部署模型时使用SavedModel可以将模型快速的部署tensorflow serving中。</p>
<h2 id="模型加载"><a href="#模型加载" class="headerlink" title="模型加载"></a>模型加载</h2><p>1、只加载参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model_only_weiths = generate_functional_model()</span><br><span class="line">print(<span class="string">&#x27;加载前的参数值：&#x27;</span>, model_only_weiths.variables[<span class="number">0</span>].numpy()[<span class="number">0</span>, :<span class="number">5</span>])</span><br><span class="line">model_only_weiths.load_weights(output_model_file)</span><br><span class="line">print(<span class="string">&#x27;加载后的参数值：&#x27;</span>, model_only_weiths.variables[<span class="number">0</span>].numpy()[<span class="number">0</span>, :<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<pre><code>加载前的参数值： [-0.0438241   0.07230127 -0.00713674 -0.06280634  0.01845549]
加载后的参数值： [ 0.01215426 -0.05889301  0.04321195  0.05594952 -0.00950153]</code></pre>
<p>2、加载模型结构和参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_weithts_and_model = tf.keras.models.load_model(output_model_file)</span><br><span class="line">print(<span class="string">&#x27;加载后的参数值：&#x27;</span>, model_weithts_and_model.variables[<span class="number">0</span>].numpy()[<span class="number">0</span>, :<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<pre><code>加载后的参数值： [ 0.01215426 -0.05889301  0.04321195  0.05594952 -0.00950153]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印模型结构</span></span><br><span class="line">model_weithts_and_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;functional_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
inputs (InputLayer)          [(None, 28, 28)]          0         
_________________________________________________________________
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
h1 (Dense)                   (None, 300)               235500    
_________________________________________________________________
h2 (Dense)                   (None, 200)               60200     
_________________________________________________________________
h3 (Dense)                   (None, 100)               20100     
_________________________________________________________________
outputs (Dense)              (None, 10)                1010      
=================================================================
Total params: 316,810
Trainable params: 316,810
Non-trainable params: 0
_________________________________________________________________</code></pre>
<h2 id="完整的图片分类任务实例"><a href="#完整的图片分类任务实例" class="headerlink" title="完整的图片分类任务实例"></a>完整的图片分类任务实例</h2><p>1、加载数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.keras.datasets.fashion_mnist</span></span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">(x_train_all, y_train_all), (x_test, y_test) = mnist.load_data()</span><br><span class="line">print(<span class="built_in">type</span>(x_train_all), <span class="built_in">type</span>(y_train_all))</span><br><span class="line"><span class="comment"># 如果第一层layer的activation是relu的话，这里要做规一化</span></span><br><span class="line">x_train_all, x_test = x_train_all/<span class="number">255.0</span>, x_test/<span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">x_valid, x_train = x_train_all[:<span class="number">5000</span>], x_train_all[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train_all[:<span class="number">5000</span>], y_train_all[<span class="number">5000</span>:]</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;numpy.ndarray&#39;&gt; &lt;class &#39;numpy.ndarray&#39;&gt;</code></pre>
<p>2、定义模型，forward时的计算（神经网络结构）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mnist_model = tf.keras.Sequential()</span><br><span class="line">mnist_model.add(tf.keras.layers.Flatten(input_shape=[<span class="number">28</span>, <span class="number">28</span>]))</span><br><span class="line"><span class="comment"># 下面的activation 如果是relu的话，那x一定要做规一化即除上255，如果是sigmoid的话就不会有问题</span></span><br><span class="line">mnist_model.add(tf.keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>)) </span><br><span class="line">mnist_model.add(tf.keras.layers.Dense(<span class="number">200</span>, activation=<span class="string">&quot;relu&quot;</span>))</span><br><span class="line">mnist_model.add(tf.keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;relu&quot;</span>))</span><br><span class="line">mnist_model.add(tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>3、定义损失函数，并选择优化器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mnist_model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>, </span><br><span class="line">             optimizer=<span class="string">&quot;sgd&quot;</span>, </span><br><span class="line">             metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>

<p>4、迭代的对数据进行模型训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = mnist_model.fit(x_train, y_train, epochs=<span class="number">10</span>, validation_data=(x_valid, y_valid))</span><br></pre></td></tr></table></figure>

<pre><code>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 5s 93us/sample - loss: 0.6206 - accuracy: 0.8366 - val_loss: 0.2858 - val_accuracy: 0.9184
Epoch 2/10
55000/55000 [==============================] - 5s 97us/sample - loss: 0.2665 - accuracy: 0.9234 - val_loss: 0.2190 - val_accuracy: 0.9372
Epoch 3/10
55000/55000 [==============================] - 5s 93us/sample - loss: 0.2078 - accuracy: 0.9398 - val_loss: 0.1702 - val_accuracy: 0.9504
Epoch 4/10
55000/55000 [==============================] - 5s 93us/sample - loss: 0.1704 - accuracy: 0.9508 - val_loss: 0.1489 - val_accuracy: 0.9592
Epoch 5/10
55000/55000 [==============================] - 5s 97us/sample - loss: 0.1436 - accuracy: 0.9584 - val_loss: 0.1306 - val_accuracy: 0.9626
Epoch 6/10
55000/55000 [==============================] - 5s 97us/sample - loss: 0.1242 - accuracy: 0.9647 - val_loss: 0.1179 - val_accuracy: 0.9686
Epoch 7/10
55000/55000 [==============================] - 5s 87us/sample - loss: 0.1082 - accuracy: 0.9687 - val_loss: 0.1086 - val_accuracy: 0.9696
Epoch 8/10
55000/55000 [==============================] - 5s 90us/sample - loss: 0.0958 - accuracy: 0.9727 - val_loss: 0.1006 - val_accuracy: 0.9708
Epoch 9/10
55000/55000 [==============================] - 5s 99us/sample - loss: 0.0860 - accuracy: 0.9751 - val_loss: 0.0964 - val_accuracy: 0.9730
Epoch 10/10
55000/55000 [==============================] - 5s 96us/sample - loss: 0.0768 - accuracy: 0.9784 - val_loss: 0.0865 - val_accuracy: 0.9764</code></pre>
<p>5、在测试集上对模型进行评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curves</span>(<span class="params">history</span>):</span></span><br><span class="line">    pd.DataFrame(history.history).plot(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    plt.gca().set_ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">plot_learning_curves(history)</span><br></pre></td></tr></table></figure>


<p><img src="./_image/output_63_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss, acc = mnist_model.evaluate(x_test, y_test)</span><br><span class="line">print(loss, acc)</span><br></pre></td></tr></table></figure>

<pre><code>10000/10000 [==============================] - 1s 54us/sample - loss: 0.0923 - accuracy: 0.9716
0.09230228984989226 0.9716</code></pre>
<h2 id="Tensorflow-Keras-Custom"><a href="#Tensorflow-Keras-Custom" class="headerlink" title="Tensorflow Keras - Custom"></a>Tensorflow Keras - Custom</h2><p>在之前构建模型时使用的都是Tensorflow已经封装好的方法，下面通过实例来展示如何封装自己的计算方法。</p>
<p>1、Custom Layer (自定义层)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomizedDenseLayer</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, units, activation=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line">        self.units = units</span><br><span class="line">        self.activation = tf.keras.layers.Activation(activation)</span><br><span class="line">        <span class="built_in">super</span>(CustomizedDenseLayer, self).__init__(**kwargs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">        <span class="comment"># 构建所需要的参数</span></span><br><span class="line">        self.kernel = self.add_weight(name=<span class="string">&quot;kernel&quot;</span>,</span><br><span class="line">                                      shape=(input_shape[<span class="number">1</span>], self.units),</span><br><span class="line">                                      initializer=<span class="string">&quot;uniform&quot;</span>,</span><br><span class="line">                                      dtype=tf.float32,</span><br><span class="line">                                      trainable=<span class="literal">True</span>)</span><br><span class="line">        self.bias = self.add_weight(name=<span class="string">&quot;bias&quot;</span>, shape=(self.units, ),</span><br><span class="line">                                   initializer = <span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">                                   dtype=tf.float32,</span><br><span class="line">                                   trainable=<span class="literal">True</span>)</span><br><span class="line">        <span class="built_in">super</span>(CustomizedDenseLayer, self).build(input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 正向计算</span></span><br><span class="line">        <span class="keyword">return</span> self.activation(x @ self.kernel + self.bias)</span><br></pre></td></tr></table></figure>

<p>2、Custom Model (自定义模型)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomizedModel</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CustomizedModel, self).__init__(**kwargs)</span><br><span class="line">        self.flatten = tf.keras.layers.Flatten()</span><br><span class="line">        self.h1 = CustomizedDenseLayer(units=<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&#x27;custom_h1&#x27;</span>)</span><br><span class="line">        self.h2 = CustomizedDenseLayer(units=<span class="number">200</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&#x27;custom_h2&#x27;</span>)</span><br><span class="line">        self.h3 = CustomizedDenseLayer(units=<span class="number">100</span>, activation=<span class="string">&quot;relu&quot;</span>, name=<span class="string">&#x27;custom_h3&#x27;</span>)</span><br><span class="line">        self.output_layer = tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        ret = self.flatten(<span class="built_in">input</span>)</span><br><span class="line">        ret = self.h1(ret)</span><br><span class="line">        ret = self.h2(ret)</span><br><span class="line">        ret = self.h3(ret)</span><br><span class="line">        <span class="keyword">return</span> self.output_layer(ret)</span><br><span class="line">    </span><br><span class="line">custom_model = CustomizedModel()</span><br><span class="line">custom_model.build(input_shape=(<span class="literal">None</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">custom_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;customized_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          multiple                  0         
_________________________________________________________________
custom_h1 (CustomizedDenseLa multiple                  235500    
_________________________________________________________________
custom_h2 (CustomizedDenseLa multiple                  60200     
_________________________________________________________________
custom_h3 (CustomizedDenseLa multiple                  20100     
_________________________________________________________________
dense_4 (Dense)              multiple                  1010      
=================================================================
Total params: 316,810
Trainable params: 316,810
Non-trainable params: 0
_________________________________________________________________</code></pre>
<p>3、Custom Train (自定义训练过程)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">custom_fit</span>(<span class="params">model, x_train, y_train, x_valid, y_valid, epochs=<span class="number">1</span>, batch_size=<span class="number">1</span></span>):</span></span><br><span class="line">    steps_per_epoch = <span class="built_in">len</span>(x_train) // batch_size</span><br><span class="line">    </span><br><span class="line">    optimizer = tf.keras.optimizers.SGD()</span><br><span class="line">    train_metric = tf.keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">    valid_metric = tf.keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">random_batch</span>(<span class="params">x, y, batch_size=batch_size</span>):</span></span><br><span class="line">        idx = np.random.randint(<span class="number">0</span>, <span class="built_in">len</span>(x_train), size=batch_size)</span><br><span class="line">        <span class="keyword">return</span> x[idx], y[idx]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;steps_per_epoch is: &quot;</span> , steps_per_epoch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(steps_per_epoch):</span><br><span class="line">            x_batch, y_batch = random_batch(x_train, y_train, batch_size)</span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                y_pred = model(x_batch)</span><br><span class="line">                loss = loss_obj(y_batch, y_pred)</span><br><span class="line">            grads = tape.gradient(loss, model.variables)</span><br><span class="line">            optimizer.apply_gradients(<span class="built_in">zip</span>(grads, model.variables))</span><br><span class="line">            train_metric(y_batch, y_pred)</span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">&quot;\rEpoch&quot;</span>, epoch,<span class="string">&quot; train loss: &quot;</span>,  loss.numpy(), <span class="string">&quot; train mse: &quot;</span>, train_metric.result().numpy())</span><br><span class="line">        y_valid_pred = model(x_valid)</span><br><span class="line">        valid_metric(y_valid, y_valid_pred)</span><br><span class="line">        valid_loss = loss_obj(y_valid, y_valid_pred)</span><br><span class="line">        print(<span class="string">&quot;\t&quot;</span>, <span class="string">&quot;valid mse: &quot;</span>, valid_metric.result().numpy())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">custom_fit(model=custom_model, x_train=x_train, y_train=y_train, x_valid=x_valid, y_valid=y_valid, batch_size=<span class="number">64</span>, epochs=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>steps_per_epoch is:  859
WARNING:tensorflow:Layer customized_model is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it&#39;s dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(&#39;float64&#39;)`. To change just this layer, pass dtype=&#39;float64&#39; to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Epoch 0  train loss:  2.3038578  train mse:  0.1875
Epoch 0  train loss:  2.280334  train mse:  0.21395421
Epoch 0  train loss:  2.2446647  train mse:  0.31584266
Epoch 0  train loss:  2.1839218  train mse:  0.3867317
Epoch 0  train loss:  2.092816  train mse:  0.42627805
Epoch 0  train loss:  1.8289787  train mse:  0.4548715
Epoch 0  train loss:  1.4188151  train mse:  0.48367304
Epoch 0  train loss:  1.1096661  train mse:  0.51466656
Epoch 0  train loss:  0.912598  train mse:  0.5451389
     valid mse:  0.8054
Epoch 1  train loss:  0.86221695  train mse:  0.8125
Epoch 1  train loss:  0.7114984  train mse:  0.80522895
Epoch 1  train loss:  0.5244288  train mse:  0.8143657
Epoch 1  train loss:  0.4718449  train mse:  0.82340115
Epoch 1  train loss:  0.44886717  train mse:  0.8302681
Epoch 1  train loss:  0.49274105  train mse:  0.8369199
Epoch 1  train loss:  0.3548026  train mse:  0.8425801
Epoch 1  train loss:  0.5724619  train mse:  0.8459567
Epoch 1  train loss:  0.34557572  train mse:  0.8497581
     valid mse:  0.8914</code></pre>
<p><a target="_blank" rel="noopener" href="https://playground.tensorflow.org/">https://playground.tensorflow.org/</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Tensorflow/" rel="tag"># 深度学习 Tensorflow</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-05-09-Transformers%E7%9A%84%E4%B8%80%E4%BA%9B%E8%BF%B7%E6%80%9D/" rel="prev" title="Transformers的一些迷思">
      <i class="fa fa-chevron-left"></i> Transformers的一些迷思
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-07-16-%E9%80%9A%E8%BF%87Tensorflow2%E4%BD%BF%E7%94%A8Bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/" rel="next" title="通过Tensorflow2使用Bert预训练模型的两种方式">
      通过Tensorflow2使用Bert预训练模型的两种方式 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensorflow-Introduction"><span class="nav-number">1.</span> <span class="nav-text">Tensorflow Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81Tensorflow-%E5%8F%91%E5%B1%95%E5%8F%B2"><span class="nav-number">1.1.</span> <span class="nav-text">1、Tensorflow 发展史</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81Tensorflow-VS-Pytorch"><span class="nav-number">1.2.</span> <span class="nav-text">2、Tensorflow VS Pytorch</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensorflow-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="nav-number">2.</span> <span class="nav-text">Tensorflow 核心概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81Tensor-%E5%BC%A0%E9%87%8F"><span class="nav-number">2.1.</span> <span class="nav-text">1、Tensor (张量)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81Compute-Grapth-%E8%AE%A1%E7%AE%97%E5%9B%BE-%E4%B8%8EAutograd-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="nav-number">2.2.</span> <span class="nav-text">2、Compute Grapth (计算图) 与Autograd(自动求导)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E8%AF%B4%E6%98%8E"><span class="nav-number">3.</span> <span class="nav-text">模型和数据说明</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%B4%E6%98%8E"><span class="nav-number">3.1.</span> <span class="nav-text">数据说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">3.2.</span> <span class="nav-text">全连接神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensorlfow2%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">Tensorlfow2搭建一个全连接神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#0%E3%80%81Why-Tensorflow2-X"><span class="nav-number">4.1.</span> <span class="nav-text">0、Why Tensorflow2.X</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">4.2.</span> <span class="nav-text">1、定义模型结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81Loss-Function-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.</span> <span class="nav-text">2、Loss Function (损失函数)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81Optimaizer-%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">4.4.</span> <span class="nav-text">3、Optimaizer (优化方法)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81Metrics-%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95"><span class="nav-number">4.5.</span> <span class="nav-text">4、Metrics (评价方法)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">5.</span> <span class="nav-text">模型训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98"><span class="nav-number">6.</span> <span class="nav-text">模型保存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E8%BF%87callbacks%E7%9A%84%E6%96%B9%E5%BC%8F%E8%BF%9B%E8%A1%8C%E4%BF%9D%E5%AD%98"><span class="nav-number">6.1.</span> <span class="nav-text">通过callbacks的方式进行保存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E6%88%90SavedModel"><span class="nav-number">6.2.</span> <span class="nav-text">保存成SavedModel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A4%E7%A7%8D%E4%BF%9D%E5%AD%98%E6%96%B9%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">6.3.</span> <span class="nav-text">两种保存方式的使用场景</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD"><span class="nav-number">7.</span> <span class="nav-text">模型加载</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E7%9A%84%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E5%AE%9E%E4%BE%8B"><span class="nav-number">8.</span> <span class="nav-text">完整的图片分类任务实例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensorflow-Keras-Custom"><span class="nav-number">9.</span> <span class="nav-text">Tensorflow Keras - Custom</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hwyoung"
      src="/avatar.png">
  <p class="site-author-name" itemprop="name">hwyoung</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">77</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hwyoung</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
