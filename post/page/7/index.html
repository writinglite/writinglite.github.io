<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Posts - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Posts"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="/post/"><meta itemprop=name content="Posts"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel=alternate type=application/rss+xml href=/post/index.xml title="Writing Lite"><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><header class=main__header><h1 class=main__title>Posts</h1></header><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/reading/2018-10-01-%E6%B4%BB%E6%B3%95/ rel=bookmark>活法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div></div></header><div class="content list__excerpt post__content clearfix">杰出的才能，由这才能创造的成果，属于我却不归我所有。才能和劳动不应由个人独占，而应该用来为世人社会谋利。就是说自己的才能用来为“公”是第一义，用来为“私”是第二义。我认为这就是谦虚这一美德的本质所在。
死亡时的灵魂比出生时略有进步，就是心灵稍经磨炼的状态。
深秋时节某一天，在落叶和寒风中，有位旅人行色匆匆，赶路回家。走到某处突然低头一看，脚下一片白乎乎的东西，仔细一瞧，竟是人骨。此处怎么会有如此大量的人骨呢？他不禁毛骨悚然，又不得其解。只顾往前奔走，抬头看时，迎面走来一只体格巨大的老虎，咆哮着向他逼近。旅人直惊得魂飞魄散：“啊！那么多人骨原来是老虎吃剩的残物。”他急忙调头逃命，然而慌不择路，一阵猛跑竟然跑上了悬崖峭壁，峭壁之下是怒涛汹涌的大海。前无去路，后有猛虎追逼，进退维谷之际，他爬上了崖边仅有的一棵松树，但那恶虎紧随其后，张开骇人的巨爪，也开始爬树。“今天我命休矣”，正当他万念俱灰时，忽然看见树上垂下一根藤条，他顺着藤条往下滑去，那藤条却不着底，旅人被悬在半空之中。上面是老虎伸着舌头、流着口水盯着他，正所谓“虎视眈眈”。再看下面，狂风大浪之中出现红、黑、蓝三条巨龙，正等着他掉下去时可一饱口福。忽又听到上方有窸窣之声，定睛一看，黑白两只老鼠正在交替啃咬那藤条的根部。藤条一旦被鼠牙咬断，旅人只好落入张开巨口的恶龙腹中。命悬一线之际，旅人想到将老鼠赶跑。于是拼命摇晃那根藤条，摇摆之下，有湿漉漉、暖烘烘的液体落到他脸上，用嘴一舔，是甜美的蜂蜜。原来藤条根部有一蜂巢，一经摇动，蜂蜜就掉落下来。舔着甘露般的蜂蜜，旅人居然陶醉起来，以致忘记了自己身处绝境——虎龙夹击，得以苟延残喘的藤条正被老鼠啃噬——还在一次又一次摇动救命的藤条，忘情地享用美味的蜂蜜。
在这里，老虎暗喻死亡和疾病；松树代表世上的地位、财产和名誉；白黑两鼠表示白昼和黑夜，也就是时间的流逝。人在不断逼近的死亡的威胁中拼命求生，而维系生机的仅是一根藤条而已。 而这根藤条了随着时间的推移不断磨损。我们想逃离死亡，但死神却一年一年逼近我们。然而，即使折寿，哪怕缩短生命，也要去吸食“蜜汁”——切不断那可怜、可鄙又可悲的欲望。释迦告诉我们，这就是赤裸裸的人类本性。
磨炼心声、提升以改，要点在于日常生活中的“精进”</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/deep-learning/2019-07-17-%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ rel=bookmark>深度神经网络</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">全连接神经网络 neroun的输出 用 $a_i^l$ 表示，l表示layer，i表示第i个neuron，同一层output用 vector $a^l$表示
两层网络之间的weit用$w_{ij}^l$表示l-1层的第i个neroun到l层的第j个neuron
$\sigma$ 表示激活函数
循环神经网络 基础网络架构 单层RNN $x^1、h^0、y^1、h^1$都是vector
多层RNN 双向RNN $f_1、f_2、f_3$没有强制规定可以自己设计
Native RNN LSTM c 的变化慢，可以记忆很久以前的数据
GRU 李宏毅
未完待续&mldr;</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/machine-learning/2017-03-12-%E6%B7%B7%E5%90%88%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B/ rel=bookmark>混合朴素贝叶斯模型</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div></div></header><div class="content list__excerpt post__content clearfix">训练样本，${x_1,x_2,x_3 &mldr; x_m}$ m个文本。 $$ x^{(i)} \in {0,1}^n \
x^{(i)}_j = I{词j是否在文档x^{(i)}中 } \
z^{(i)} \in {0,1} \
z^{(i)} \sim 伯努利(\phi) $$ zi是隐含随机变量，也就是聚类类别，这里假设是两类，也因此分布为伯努利分布，也可以根据情况有更多类别。
条件独立假设： $$ P(x^{(i)} \mid z^{(i)}) = \prod_i^n P(x^{(i)}_j \mid z^{(i)}) $$ 表示，在给定类别z生成文档x的概率。
$$ P(x^{(i)}j=1 \mid z^{(i)} = 0) = \phi{j \mid z=0} $$ 对应高斯判别分析中的 $$ \begin{aligned} \phi_{i|y=1} &= P(x_i=1|y=1) \
\phi_{i|y=0} &= P(x_i=1|y=0) \
\phi_y&=P(y=1) \end{aligned} $$
E-step: $$ w^{(i)} = P(z^{(i)}=1 \mid x^{(i)};\phi_{j \mid z},\phi) \</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/machine-learning/2017-02-18-%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/ rel=bookmark>混合高斯模型</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div></div></header><div class="content list__excerpt post__content clearfix">混合高斯模型 假设z为隐变量，x,z存在如下关系。 $$ P(x^{(i)},z^{(i)}) = P(x^{(i)}|z^{(i)})P(z^{(i)}) $$ $$ z^{(i)} \sim Multinomial(\phi)$ \
\phi_j=P(z^{(i)}=j)>0 \
\sum_j\phi_j=1 \
(x^{(i)} \mid {z^{(i)}=j} ) \sim N(\mu_j ,\Sigma_j) $$ 这里的j的聚类的一个类别之一。
E-step: 对z的猜测 $$ \begin{aligned} w_j^{(i)} & := P(z^{(i)}_j \mid x^{(i)}=j;\phi,\mu_j,\Sigma_j) \
&= \frac{ P(x^{(i)} \mid z^{(i)}_j=j)P(z^{(i)}_j=j)}{\sum_l^n P(x^{(i)} \mid z^{(i)}_j=l)P(z^{(i)}_j=l)} \end{aligned} $$
其中，$P(z^{(i)}_j \mid x^{(i)}=j;\phi,\mu_j,\Sigma_j)$表示第i个样本生成z为类别j的概率。这里的n表示聚类总类别数。
M-step: 对参数的估计 $$ \phi_j:=\frac{1}{m} \sum_{i=1}^m w_j^{(i)} \
\mu_j:= \frac{\sum_{i=1}^m w_j^{(i)} x^{(i)} }{\sum_{i=1}^m w_j^{(i)}} \
\Sigma_j:=\frac{ \sum_{i=1}^m w_j^{(i)} (x^{(i)} - \mu_j) (x^{(i)} - \mu_j)^T }{\sum_{i=1}^n w_j^{(i)}} $$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/machine-learning/2017-03-12-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/ rel=bookmark>生成模型与高斯判别分析</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div></div></header><div class="content list__excerpt post__content clearfix">生成模型与判别模型 判别模型 判别模型对P(y|x)或者说对y建模，直接学习得到y。 常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。
生成模型 生成模型对P(x|y)或者说对x建模，通过计算 $$ \begin{aligned} \arg\max\limits_{y}p(y\vert x) &= \arg\max\limits_y\frac{p(x\vert y)p(y)}{p(x)} \
&= \arg\max\limits_y p(x\vert y)p(y) \end{aligned} $$ 得到y。 因此给定x进行比较时，P(x)为固定值，所以P(x)可省略。 常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等。
高斯分布 若随机变量 X, X服从一个位置参数为 $\mu$ 、尺度参数为$\sigma$ 的概率分布，记为： $$ \displaystyle X \sim N(\mu ,\sigma ^{2}) $$ 则其概率密度函数为 $$ \displaystyle f(x)={1 \over \sigma {\sqrt {2\pi }}},e^{-{(x-\mu )^{2} \over 2\sigma ^{2}}} $$
多元高斯分布 $$ X ∼ N(\mu,\Sigma) $$
$$ \begin{aligned} E[X] &= \mu \
Cov(X) &= \Sigma \end{aligned} $$ 概率密度函数为 $$ \begin{equation} p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \end{equation} $$ 其中$|\Sigma|$是$\Sigma$的行列式，$\Sigma$是协方差矩阵，而且是对称半正定的。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/mathematics/2017-01-12-%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%95%B0%E6%8D%AE-%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/ rel=bookmark>程序员的数据-概率统计</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div></div></header><div class="content list__excerpt post__content clearfix">概率的定义 上帝视角 $\Omega$：平行世界的集合
$\omega$：具体某一个世界 A：$\Omega$的子集 P(A)：A的面积，即A的概率
随机变量 通俗来讲它是一种会随机改变的不确定量。
从上帝视角来看，随机变量只是$\Omega$中的函数而已。对于$\Omega$中的各元素$\omega$，函数$f(\omega)$将返回相应的整数，这些整数值即为随机变量。 $$ X(u,v)\equiv \begin{cases} 中选\ (0\leq &lt; 1/4)\
落选\ (1/4\leq v &lt;1)\
\end{cases} $$ 需要注意的是，随机变量并不等同于程序中的变量，它蕴含的是一种函数，$随机变量的值=f(\omega)$。
概率分布 随机变量涉及具体的平行世界。与这相对地，概率分布的概念更为宽泛，它只考虑面积，不涉及具体的平行世界。 对于随机变量，哪一个世界中将得到哪一个值都已确定，而概率分布不涉及具体发生在哪一个世界。 只要得到随机变量的X，我们就能求出相应的概率分布，但反过来却并不成立，仅凭概率分布我们无法求出随机变量的值。 $$ P(X=k)=``X(\omega)=k时区域\omega的面积,即是说满足f(\omega)=k的所有\omega的面积" $$
多个随机变量之间的关系 联合概率与边缘概率 联合概率：多个随机变量，包含多个条件且所有条件同时成立的概率称为联合概率。 边缘概率：单个随机变量有关的概率称为边缘概率。边缘概率是一个相对概念。
通过联合概率分布可以计算联合概率分布，然而，如果只知道边缘分布，无法求得相应的联合概率分布。
条件概率 在研究理工科问题时，我们学会采用控制变量法分析变量之间的关系，讨论变量X取特定值时变量Y的取值情况。如果没有误差，我们可以用函数Y=f(X)来表示它们的关系。不过在现实中，我们很难去除所有影响因素，测量到绝对精确的值。所以，即使X的测定值不变，Y的测定值也会发生细微变化，此时我们需要研究在X为某个特定值时Y的概率分布。也就是说，我们将研究条件概率P(Y=b,|X=a),而不是函数Y=f(x)。+ 0000000000000000000</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/reading/2017-03-18-%E7%B2%BE%E8%BF%9B/ rel=bookmark>精进</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div></div></header><div class="content list__excerpt post__content clearfix">第一章 时间之尺——我们应该怎样对待时间 一、 活在“全部的现在” （从当下出发，联结过去与未来）
向孩子学习“郑重”的态度 郑重是这样一种态度：因恪守配以而知事情轻重缓急，因尽全力无保留而使其事竟成、光阴未曾虚度。 不同场合，不同的时间视角 五种时间视角 1、积极过去视角 2、消极过去视角 3、享乐主义视角 4、宿命论视角 5、未来视角 建议 最好采用平衡和折中的方式，根据不同的现实场景加以灵活选择 由当下向过去与未来延伸 多瑞典心理学家林德沃（Lindvall）提出，具有平衡式时间视角的人，在内心具有一种“延伸的当下感”（extendednow），既可以“从当下来审视过去”，也可以“视未来存在于当下”，他应具有囊括“过和“未来”的包容性。 二、 对五年后的自己提问 （如何解决远期与近期未来的冲突？）
对人 生来 说，“ 五年” 意味着 什么？ 五年的时间通常会越过人生的“下一个阶段”，进入“下下个阶段”。 如果把五年作为做成一件事的时间跨度，那么也意味着你需要忍受头几年的挫败、煎熬和孤独，乃至别人的误解、嘲笑和攻击。 未来 远期未来 远期未来的视角下，人们倾向于用抽象、概括的方式去思考。 近期未来 而在近期未来视角下，人们更容易到具体的情境中去考虑，想得更多的不是“要不要做”，而是“怎么去做”。 拖延症：当近期未来遇到了阻碍，而产生了对远期未来焦虑的放大和大量的负面情绪，因而发生了关注点偏倚 三、 我们总是在重复地抓起沙子 （把时间花在值得做的事情上）
如何判断一件事情是否值得做 两个因素 收益值 收益半衰期 少做短半衰期事件 长半衰期的事件，可以累积和叠加 我们为什么要读经典 超长半衰期 经典的价值，就在于你总是会从中找到新的东西，所以经典是怎么读都读不尽的。 一部经典作品必然包含了某种接近“带我本质”的东西，也就是某种根源性的东西。 辨别生活中的信息噪音 调整评估信息价值的时间尺度 法国历史学家布罗代尔提出记述历史的三个时间尺度，其中最长的时间尺度，关注的是一个地区的地理和气候环境；中等的时间惊讶，关注的是社会和文化层面的因素；而短时间尺度，才是传统的历史学所关注的具体的出现。布罗代尔认为，具体的历史事件的出现，往往具有有随机性，在表面现象背后，是更深刻和稳定 的导致该事件发生的中、长时间尺度的因素，这些因素才是更值得历史学家记述的。 四、 “快”与“慢”的自由切换 （为什么我们的时间永远不够用？）
时间管理，让我们越来越快 我们原本就是因为快而痛苦，可时间管理却教我们如何更快。 侯世达定律：实际做事花费的时间总是比预期的要长，即使预期中考虑了侯世达定律 工作要快，生活要慢 有快有慢便是节奏 提升时间的使用深度 时间悖论：半个世纪以来，人们可自由支配 的闲暇时间总体上一直呈增加的趋势，但人们主观上却觉得自己的闲暇时间在减少，也就是说人们实际拥有的赶时间越来越多，主观感受拥有的时间却越少 从闲暇时间中获得放松和满足的程序并不取决于闲暇时间的长度，而是取决于质量。 事业与生活的秘诀无外乎是处理好时间的快与慢，深与浅的关系 第二章 寻找心中的“巴拿马”——如何做出比好更好的选择 一、 从终极问题出发 （以人生终极目标作为首要原则）</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/machine-learning/2017-01-07-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/ rel=bookmark>统计学习方法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div></div></header><div class="content list__excerpt post__content clearfix">统计学习 如果一个系统能够通过执行某个过程改进它的性能，这就是学习。
统计学习的前提 统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这就是统计学习的前提。
统计学习方法组成 统计计算由监督学习，非监督学习，半监督学习和强化学习等组成。
监督学习概括 从给定的、有限的、用于学习的训练数据集合出发，假设数据是独立同分布产生的；并且假设要学习模型属于某个函数集合，称为假设空间；应用某个评价准则，从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。
统计学习方法的三要素 模型 策略 算法 实现统计学习方法的步骤 得到一个有限的训练数据集合 确定包含所有可能模型的假设空间，即学习模型的集合 确定模型选择的准则，即学习的策略 实现求解最优模型的算法，取出学习的算法 通过学习方法选择最做强模型 利用学习的最优模型对新数据进行预测和分析 基本概念 输入空间 输入所有可能的空间
输出空间 输出所有可能的空间
特征空间 每个具体的输入是一个实例，通常由特征向量表示。这时所有特征向量存在的空间称为特征空间。
样本 输入输出对又称为样本或样本点。
问题的分类 回归问题 输入变量与输出变量均为连续变量的预测问题称为回归问题。
分类问题 输出变量为有限个离散变量的预测问题称为分类问题。
标注问题 输入变量与输出变量均为变量序列的预测问题称为标注问题。
联合概率分布 监督学习假设输入与输出的随机变量X与Y遵循联合概率分布。训练数据与测试数据被看作是依联合概率分布P(X,Y)独立同分布产生的。
假设空间 模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。
监督学习的模型可以是概率模型或非概率模型，由条件概率分布P(Y|X)或决策函数Y=f(X)表示，随具体学习方法而定。
统计学习的三要素 方法=模型+策略+算法
模型 策略 损失函数:度量一次预测的好坏 风险函数：度量平均意义下模型预测的好坏</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/machine-learning/2017-06-04-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/ rel=bookmark>统计学习方法概论</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div></div></header><div class="content list__excerpt post__content clearfix">统计学习的基本概念 学习的定义 如果一个系统能够通过执行某个过程改进它的性能，这就是学习。统计学习的对象是数据；统计学习关于数据的基本假设是同类数据具有一定的统计规律，这是统计学习的前提。统计学习的上的是对未知数据进行预测和分析；对数据的预测可以使计算机更加智能化；对数据的分析可以让人们获取新的知识，给人们带来新的发现。
监督学习的学习方法 从给定的、有限的、用于学习的训练数据集合出发，假设数据是独立同分布产生的；并且假设学习的模型属于某个函数集合，称为假设空间；应用某个评价准则，从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。
实现统计学习方法的步骤 得到一个有限的训练数据集合 确定包含所有可能模型的假设空间，即学习模型的集合 确定模型选择的准则，即学习的策略 实现求解最优模型的算法，取出学习的算法 通过学习方法选择最做强模型 利用学习的最优模型对新数据进行预测和分析 不同的预测任务名称 输入变量与输出变量均为连续变量的预测问题称为回归问题。 输出变量为有限个离散变量的预测问题称为分类问题。 输入变量与输出变量均为变量序列的预测问题称为标注问题。 联合概率分布 监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)，P(X,Y)表示分布函数，或分布密度函数。训练数据与测试数据被看作是依联合概率分布P(X,Y)独立同分布产生的。
判别模型 判别模型对P(y|x)或者说对y建模，直接学习得到y。 在计算学习算法时，一般使用candidation似然 就是直接用的P(y|x) 常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。
生成模型 生成模型对P(x|y)或者说对x建模（从下面的公式可以看到），通过如下计算得到 y。 $$ \begin{aligned} \arg\max\limits_{y}p(y\vert x) &= \arg\max\limits_y\frac{p(x,y)}{p(x)} \
&= \arg\max\limits_y\frac{p(x\vert y)p(y)}{p(x)} \
&= \arg\max\limits_y p(x\vert y)p(y) \end{aligned} $$ 在给定x进行比较时，P(x)为固定值，所以P(x)可省略。在计算学习算法时，一般使用joint似然，就是用的P(x,y)=P(x|y)*P(y)，其实和P(y|x) 一样的。 生成模型表示的是数据生成的方式 ，就是P(x,y) x和y的联合概率。一般来说数据的生成方式是比较复杂的，所以一般都会对数据的生成方式做一定的假设（比如隐马尔科夫模型、朴素贝叶斯模型）。 常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等
统计学习的三要素 统计学习方法包括模型的假设空间、模型选择的准则以及模型的学习算法，称其为统计学习方法的三要素，简称为模型、策略、算法。
模型 在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。 假设空间用F表示，假设空间可以定义为决策函数的集合 $$ F={f|Y=f(x)} $$ 假设空间也可以定义为条件概率的集合 $$ F={P|P(Y|X)} $$
策略 首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。
常用的损失函数 **0-1损失函数 ** $$ L(Y,f(X))= \begin{cases} 1,Y \neq f(X)\</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/deep-learning/2020-07-16-%E9%80%9A%E8%BF%87tensorflow2%E4%BD%BF%E7%94%A8bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/ rel=bookmark>通过Tensorflow2使用Bert预训练模型的两种方式</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">以中文Bert为例
下面的例子均以中文Bert预训练模型为例
方式1：使用Tensorflow Hub import tensorflow as tf import tensorflow_hub as hub hub_url_or_local_path = "https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/2" # 或者将模型下载到本地 # !wget "https://storage.googleapis.com/tfhub-modules/tensorflow/bert_zh_L-12_H-768_A-12/2.tar.gz" # !tar -xzvf 2.tar.gz -C bert_zh_L-12_H-768_A-12 # hub_url_or_local_path = "./bert_zh_L-12_H-768_A-12" def build_model(): input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_word_ids") input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_mask") segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="segment_ids") bert_layer = hub.KerasLayer(hub_url_or_local_path, name='bert', trainable=True) pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output) return model model = build_model() model.summary() 使用这种方式的时候在执行build_model时bert的参数已经被加载进来了。</div></article></main><div class=pagination><a class="pagination__item pagination__item--prev btn" href=/post/page/6/>«</a>
<span class="pagination__item pagination__item--current">7/8</span>
<a class="pagination__item pagination__item--next btn" href=/post/page/8/>»</a></div></div><aside class="sidebar sidebar--left"><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/post/other/2020-03-02-%E5%9F%BA%E4%BA%8Enginx%E7%9A%84acme%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E6%96%B9%E6%A1%88/>2020-03-02-基于nginx的acme免费证书方案</a></li><li class=widget__item><a class=widget__link href=/post/other/2019-11-04-%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E5%AE%89%E8%A3%85-certbot/>免费证书安装-certbot</a></li><li class=widget__item><a class=widget__link href=/post/other/2019-07-27-%E6%A0%91%E8%8E%93%E6%B4%BE3b+-%E5%AE%89%E8%A3%85openwrt/>树莓派3b+安装openwrt 18.06.4</a></li><li class=widget__item><a class=widget__link href=/post/other/2019-05-18-git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Git 常用命令</a></li><li class=widget__item><a class=widget__link href=/post/other/2019-05-18-linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Linux 常用命令</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/categories/algorithm/>algorithm</a></li><li class=widget__item><a class=widget__link href=/categories/deep-learning/>deep learning</a></li><li class=widget__item><a class=widget__link href=/categories/git/>Git</a></li><li class=widget__item><a class=widget__link href=/categories/hexo/>hexo</a></li><li class=widget__item><a class=widget__link href=/categories/linux/>Linux</a></li><li class=widget__item><a class=widget__link href=/categories/openwrt/>openwrt</a></li><li class=widget__item><a class=widget__link href=/categories/ubuntu/>ubuntu</a></li><li class=widget__item><a class=widget__link href=/categories/web/>web</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=/tags/acme/ title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=/tags/albert/ title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/batch-normalization/ title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/bert/ title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/crf/ title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=/tags/git/ title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=/tags/guice/ title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hexo/ title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/kaggle/ title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=/tags/keras/ title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=/tags/layer-normalization/ title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/linux/ title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=/tags/log/ title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=/tags/logback/ title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=/tags/nginx/ title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=/tags/openwrt/ title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=/tags/slf4j/ title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=/tags/stanford-corenlp/ title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow/ title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow-hub/ title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow2.0/ title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tra/ title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=/tags/transformers/ title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=/tags/ubuntu/ title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%88%86%E8%AF%8D/ title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%93%88%E5%B8%8C/ title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/ title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%B9%B6%E5%8F%91/ title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%8E%92%E5%BA%8F/ title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%A3%E5%88%97%E8%A1%A8/ title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/ title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%97%A0%E7%9B%91%E7%9D%A3/ title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%86%B5/ title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%AE%97%E6%B3%95/ title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/ title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/ title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script></body></html>