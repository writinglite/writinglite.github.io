<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Posts - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Posts"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="writinglite.com/post/"><meta itemprop=name content="Posts"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/writinglite.com/css/style.css><link rel=stylesheet href=/writinglite.com/css/custom.css><link rel=alternate type=application/rss+xml href=writinglite.com/post/index.xml title="Writing Lite"><link rel="shortcut icon" href=/writinglite.com/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/writinglite.com title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><header class=main__header><h1 class=main__title>Posts</h1></header><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/algorithm/2017-04-16-%E5%A4%9A%E6%A8%A1%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95-ahocorasick/ rel=bookmark>多模字符串匹配算法-Aho–Corasick</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/algorithm/ rel=category>algorithm</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 在做实际工作中，最简单也最常用的一种自然语言处理方法就是关键词匹配，例如我们要对n条文本进行过滤，那本身是一个过滤词表的，通常进行过滤的代码如下
for (String document : documents) { for (String filterWord : filterWords) { if (document.contains(filterWord)) { //process ... } } } 如果文本的数量是n，过滤词的数量是k，那么复杂度为O(nk)；如果关键词的数量较多，那么支行效率是非常低的。
计算机科学中，Aho–Corasick算法是由Alfred V. Aho和Margaret J.Corasick 发明的字符串搜索算法，用于在输入的一串字符串中匹配有限组“字典”中的子串。它与普通字符串匹配的不同点在于同时与所有字典串进行匹配。算法均摊情况下具有近似于线性的时间复杂度，约为字符串的长度加所有匹配的数量。然而由于需要找到所有匹配数，如果每个子串互相匹配（如字典为a，aa，aaa，aaaa，输入的字符串为aaaa），算法的时间复杂度会近似于匹配的二次函数。
原理 在一般的情况下，针对一个文本进行关键词匹配，在匹配的过程中要与每个关键词一一进行计算。也就是说，每与一个关键词进行匹配，都要重新从文档的开始到结束进行扫描。AC自动机的思想是，在开始时先通过词表，对以下三种情况进行缓存：
按照字符转移成功进行跳转（success表） 按照字符转移失败进行跳转（fail表） 匹配成功输出表（output表） 因此在匹配的过程中，无需从新从文档的开始进行匹配，而是通过缓存直接进行跳转，从而实现近似于线性的时间复杂度。
构建 构建的过程分三个步骤，分别对success表，fail表，output表进行构建。其中output表在构建sucess和fail表进行都进行了补充。fail表是一对一的，output表是一对多的。
按照字符转移成功进行跳转（success表） sucess表实际就是一棵trie树，构建的方式和trie树是一样的，这里就不赘述。
按照字符转移失败进行跳转（fail表） 设这个节点上的字母为C，沿着他父亲的失败指针走，直到走到一个节点，他的儿子中也有字母为C的节点。然后把当前节点的失败指针指向那个字母也为C的儿子。如果一直走到了root都没找到，那就把失败指针指向root。 使用广度优先搜索BFS，层次遍历节点来处理，每一个节点的失败路径。
匹配成功输出表（output表） 匹配 举例说明，按顺序先后添加关键词he，she，,his，hers。在匹配ushers过程中。先构建三个表，如下图，实线是sucess表，虚线是fail表，结点后的单词是ourput表。
代码 gist 2fed6f4569d4da8029e7ef08458cad6b</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/other/2016-10-22-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E6%BC%94%E5%8F%98%E5%8F%B2/ rel=bookmark>字符编码演变史</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/other/ rel=category>other</a></span></div></div></header><div class="content list__excerpt post__content clearfix">本文聊一聊各个国家是如何让计算机显示自己国家语言的。
字符集（Charset） 字符(Character)是各种文字和符号的总称
字符编码（Character Encoding） 因为计算机的存储方式毕竟是二进制，因此从二进制转换成字符的规则称为字符编码。
美国 第一个遇到这个问题的自然而然就是计算机的发源地——美国。英文一共只有26个字母，加上一些标点和控制符（制表符、回车符等等），字符集较小，因此ASCII只使用了个128个字符就完美表示了英文。
ASCII(1967) ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）是基于拉丁字母的一套电脑编码系统。它主要用于显示现代英语。
ASCII第一次以规范标准的型态发表是在1967年，最后一次更新则是在1986年，使用一个字节存储，注意ASCII只使用了一个字节中的7个比特，故ASCII定义的字符共128个；就这样英文完美解决。
法国等西欧国家 说完了美国再来聊聊西欧国家。像英国就很开心，用的都是英文，问题已经被美国人解决了。但是像法国、德国等国家就比较郁闷了。比如法国人，“é”像这样符号我们怎么表示啊！但是问题不大，ASCII占一个字节的前七个比特，不是还剩一个呢吗，这个一个比特用上又能多表示128个字符，正好我拿来用。
EASCII EASCII（Extended ASCII，延伸美国标准信息交换码）是将ASCII码由7位扩充为8位而成。EASCII的内码是由0到255共有256个符组成。EASCII码比ASCII码扩充出来的符号包括表格符号、计算符号、希腊字母和特殊的拉丁符号。
ASCII足以给英语使用。但是，其他使用拉丁字母的语言（主要是欧洲国家的语言），都有一定数量的附加符号字母，因此每下的一个比特就发挥了作用，因此EASCII诞生了。但EASCII只是一种方式，具体的实现多种多样。由此也得知EASCII是兼容ASCII的。
ISO8859系列(1985) ISO 8859，全称ISO/IEC 8859，是国际标准化组织（ISO）及国际电工委员会（IEC）联合制定的一系列8位元字符集的标准，现时定义了15个字符集。
中文 无论是英文还是西欧等国家，它们的语言的文字较少，用一个字节就解决了问题，但是汉语、日语等语言就没这么容易了，他们的字符集要大得多，一个字节已经不能满足。因此必然要用多个字节来存储。
GB2312(1981、6763) GB 2312 或 GB 2312–80 是中华人民共和国国家标准简体中文字符集，全称《信息交换用汉字编码字符集·基本集》，又称GB0，由中国国家标准总局发布，1981年5月1日实施。
GBK(1995、20,902) 汉字内码扩展规范，称GBK，全名为《汉字内码扩展规范(GBK)》1.0版，由中华人民共和国全国信息技术标准化技术委员会1995年12月1日制订，国家技术监督局标准化司和电子工业部科技与质量监督司1995年12月15日联合以《技术标函[1995]229号》文件的形式公布。
GB18030(2000、26000) GB 18030，全称：国家标准GB 18030-2005《信息技术　中文编码字符集》，是中华人民共和国现时最新的内码字集，是GB 18030-2000《信息技术　信息交换用汉字编码字符集　基本集的扩充》的修订版。与GB 2312-1980完全兼容，与GBK基本兼容；支持GB 13000（93版等同于Unicode 1.1；2010版等同于Unicode 4.0）及Unicode的全部统一汉字，共收录汉字70,244个。
统一 以上的字符集只包括某个语言或某几个语言的字符。一但在不同国家间就会出现不兼容的情况，因此统一所有字符集大势所趋。
UNICODE(1991) Unicode（中文：万国码、国际码、统一码、单一码）是计算机科学领域里的一项业界标准。它对世界上大部分的文字系统进行了整理、编码，使得电脑可以用更为简单的方式来呈现和处理文字。
UTF-8 本着节约的精神，又出现了把Unicode编码转化为“可变长编码”的UTF-8编码。UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间
https://www.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-02-12-%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B/ rel=bookmark>感知机模型</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">感知机使用的函数： $$ g(z) = \begin{cases} 1, & \text{if z ≥ 0} \
0, & \text{if z &lt; 0} \
\end{cases} $$
因此， $$ h(\theta) = g(\theta^Tx) $$
学习算法和logistic一样 $$ \theta_j := \theta_j+ \alpha(y^i-h_\theta(x^i))x_j^i $$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-02-10-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/ rel=bookmark>拉格朗日乘数法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">求下面，约束最优化问题：
$$ \underset{w}{min}f(w) \
s.t. \ h_i(w) = 0,i=1&mldr;l $$
首先创建一个拉格朗日算子 $$ L(w,\beta)=f(w)+\sum_i\beta_ih_i(w) $$ 其中$\beta_i$被称为拉格朗日乘数
然后令 $$ \frac{\partial L}{\partial w} = 0 \
\frac{\partial L}{\partial \beta} = 0
$$ 求方程组的解
广义拉格朗日乘数法 求下面，约束最优化问题： $$ \underset{w}{min}f(w) \
s.t. \ g_i(w) \le 0,i=1&mldr;k \
s.t. \ h_i(w) = 0,i=1&mldr;l $$
首先创建一个拉格朗日算子 $$ L(w,\alpha,\beta)=f(w)+\sum_{i=1}^k\alpha_ig_i(w) +\sum_{i=1}^l\beta_ih_i(w) $$
定义 $$ \theta_p(w) = \underset{\alpha,\beta;\alpha_i>0}{max}L(w,\alpha,\beta) $$
$$ p^*=\underset{w}{min}\ \underset{\alpha,\beta;\alpha_i>0}{max}L(w,\alpha,\beta) = \underset{w}{min}\ \theta_p(w) $$
p代表primal，这类问题称为原始问题
$$ \theta_p(w) = \begin{cases} f(w), & \text{符合约束条件} \</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/algorithm/2017-03-12-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/ rel=bookmark>排序算法总结</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/algorithm/ rel=category>algorithm</a></span></div></div></header><div class="content list__excerpt post__content clearfix">排序算法分为比较排序算法与非比较排序算法，通过决策树可以证明，比较排序算法性能的下界为nlogn。但非比较排序算法就没有此限制，可以达到线性复杂度，但也会有一定的限制。本篇会先介绍常见的几种比较排序算法，然后再介绍几个非比较排序算法。
比较排序算法 插入排序（INSERTION-SORT） 插入排序采用增量方法是一种原址排序，它的工作方式像排序一手扑克牌。我人每次从桌子上拿走一张牌将它插入左手中正确的位置。
伪码 INSERTION-SORT(A) for j=2 to A.length key=A[j] //将A[j]插入到已排序的序列中 i=j-1 while i>0 and A[i]>key A[i+1]=A[i] i=i-1 A[i+j]=key 时间复杂度 插入排序的最坏情况运行时间为$\Theta(n^2)$，最好情况的运行时间为$\theta(n)$
java代码 gist 60af1c98917c6bc6115e639d1d491f9c
归并排序（MERGE-SORT） 归并排序采用分治方法，它需要额外的空间，所以不是原址排序的。归并排序的思想是，将数据分成两个子数组，分别对这两个子数组进行排序，最后再将其合并。
伪码 MERGE-SORT(A,p,r) if p&lt;r q=(p+r)/2//向下取整 MERGE-SORT(A,p,q) MERGE-SORT(A,q+1,r) MERGE(A,p,q,r) MERGE(A,p,q,r) 时间复杂度 归并排序的递归式为： $$ T(n) = \begin{cases} c, & \text{n=1} \
2T(n/2)+cn, & \text{n>1} \
\end{cases} $$ c代表求解规模为1的问题所需的时间以及在分解步骤与合并步骤处理每个数组元素所需的时间。 因此，它的时间复杂度为$\Theta(n\log n)$
java代码 gist 0159937f2bc9a536f3685ebc5bfbbad1
快度排序（QUICKSORT）（非随机） 快速排序也采用分治方法，是原址排序的。在比较排序中（本篇介绍），是最快的排序算法。快速排序的思想是，先找到一个主元，根据该主元将数组划分成两个子数组，左子数组中的数小于等于该主元，右数组的数大于主元，其实相当于每一次划分确定了这次划分该主元的位置。
伪码 QUICKSORT(A,p,r) if p&lt;r q=PARTITION(A,p,r) QUICKSORT(A,p,q-1) QUICKSORT(A,q+1,r) PARTITION(A,p,r) x=A[r] i=p-1 for j=p to r-1 if A[j]&lt;=x i=i+1 exchange A[i] with A[j] exchange A[i+1] wotj A[r] return i+1 时间复杂度 最坏情况下，快速排序的时间复杂度为$\Theta(n^2)$，不过在使用随机数方法，通过指示器向量可以证明，随机版本的快速速排序的期望时间复杂度是$\Theta(nlgn)$ 递归式： $$ T(n)=2T(n/2)+\Theta(n) $$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/algorithm/2017-05-14-%E6%95%A3%E5%88%97%E8%A1%A8/ rel=bookmark>散列表</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/algorithm/ rel=category>algorithm</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背影 散列表是普通数组概念的推广。由于对普通数组可以直接寻址，使得能在O(1)时间内访问数组中的任意位置。
如果存储空间允许，我们可以提供一个数组，为每个关键字保留一个位置，以利用直接寻址的技术。
当实际关键字数目比全部的可能关键字总数要小时，采用散列表就成为直接数组寻址的一种有效替代，因为散列表使用一个长度与实际存储的关键字数目成比例的数组来存储在散列表中，不是直接把关键字作为数组下标，而是根据关键字计算出相应的下标。
当实际关键字数目比全部的可能关键字总数要小时，可能会导致多个关键字映射到同一个下标。解决这种冲突有以下三种方式：
链接方法 利用散列函数 开放寻址法 当关键字集合是静态存储时（即关键字一但存入后不再改变）时，通过“完全散列”在最坏时间为O(1)的情况下完成关键字查找。
直接寻址法 链接法 开放寻址法 完全散列 参考 《算法导论》</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/reading/2018-06-19-%E6%96%9C%E6%9D%A0%E9%9D%92%E5%B9%B4/ rel=bookmark>斜杠青年</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/reading/ rel=category>reading</a></span></div></div></header><div class="content list__excerpt post__content clearfix">“斜杠青年”代表的是一种全新的人生价值观，它的核心不在于多重收，也不在于多重身份，而在于多元化的人生。
我绝对没有想要否定上班这种生活方式，毕竟朝九晚五的上班生活是现在的主流生活方式，但我们需要意识到，它既不是赚钱的唯一方式，也不是实现自我价值的唯一方式。
我们应该允许不一样的生活方式的存在，不去过分评判他人的人生选择，因为当我们把自己的价值观强加于他人时，我们也在限制自己，同时推动了本可以打拥有更多生活的可能性。
当你的才华还撑不起你的野心的时候，你就应该静下心来学习；当你的能力还驾驭不了你的目标时，就应该沉下心来历练。梦想，不是浮躁，而是沉淀和积累。
我们在工作中无法获得快乐最核心的原因之一，就在于我们被剥夺了独立自主的权利，因为根据美国心理学家德西和瑞安提出的自我决定理论，人类有自主、独立、寻求归属感的内在动机。
价格实际上是实现将源高配的一种重要手段，价格反应了商品供求关系的变化。
按照现代经济学理论，企业本质上只不过是一种将源配置机制，它能够按照一定的组织和管理方式实现整个社会经济资源的优化配置，降低整个社会的“交易成本”。尽管企业本身存在着管理成本，但只要管理成本低于交易成本，企业就有存在的价值。
一个人的收入不是和他的劳动时间成正比，而是和他的劳动的不可替代性成正比。
真正的自由不是“拥有”的自由而是“拒绝”的自由，当我们不再需要为了钱而去做自己不喜欢或者不愿意做的事情的时候，我们才获得了真正意义上的财务自由。
在以后的日子里，当我再次遇到恶意的贬低和攻击时，我也不再以愤怒、反驳或者对抗来回应，而是大大方方承认：我的确不够好，但我在进步。
内心和头脑的丰富让我越来越淡定和从容；自我认可让我不再在意别人对我的看法；而对自身价值和能力的肯定给了我足够的安全感。我不再害怕失去，因为我知道自己内在所拥有的是别人夺不走的，也不会因为外在的变化而有所增减。
所有能够快速获得或者能够用金钱换来的都无法成为核心竞争力。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-01-12-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%9B%9E%E5%BD%92/ rel=bookmark>最小二乘回归</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">最小二乘线性回归与梯度下降算法 线性回归 预测函数： $$ h_\theta(x)=\theta_0 + \theta_1x_1 + \theta_2x_2 $$
将$x_0 = 1$
$$ h_\theta(x) = \sum_{i=0}^n\theta_ix_i $$
其中 $\theta$这参数，n表示特征数量
成本函数： $$ J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 $$
其中m表示样本数量，$(x^{(i)},y^{(i)})$表示第i个样本对儿，1/2是为了方便计算
我们目标： $$ \min_\theta J(\theta) $$
最小二乘成本函数的概率学解释（并不是唯一的解释） 假设该线性模型符合高斯分布 $$ p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\sqrt{(y^{(i)}-\theta^Tx^{(i)^2})}{2\sigma^2}\right) $$
似然函数为： $$ L(\theta) = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) $$ 因此，线性回归的似然函数： $$ \begin{align} L(\theta) & = \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \
& = \prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{((y^{(i)}) - \theta^Tx^{(i)})^2}{2\sigma^2}\right) \end{align} $$
对数似然度为 $$ \begin{align} l(\theta) & = \log L(\theta)\</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-03-12-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/ rel=bookmark>朴素贝叶斯</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">朴素贝叶斯是一种分类模型，属于生成模型。比较常见的应用是垃圾邮件分类；
在垃圾邮件识别为例
$$ y\in{0,1} $$ y=1表示垃圾邮件
将每封邮件表示成一个向量，向量中每一维都是字典中的一个词的0/1值，1表示该词在邮件中出现，0表示未出现。 假设有一个50000个词的词表，一封邮件x可表示（多元伯努利时间模型） $$ x= \begin{bmatrix} 0 \
1 \
1 \
… \
\end{bmatrix} $$
$$ x=\in{0,1}^n $$ n=50000
假设 我们还是对P(X|Y)建模
$$ \begin{aligned} & P(x_1,x_2,&mldr;x_{50000}|y) \
&= P(x_1|y)*P(x_2|y,x_1)*P(x_3|y,x_1,x_2)&mldr; \
&= P(x_1|y)*P(x_2|y)*P(x_3|y)&mldr;P(x_50000|y) \
&= \prod_{i=1}^{m}P(x_i|y) \end{aligned} $$ NLP中的n元语法模型有点类似，这里相当于一元文法unigram。
这里有个假设，条件独立性假设，形式化表示为，（如果给定Z的情况下，X和Y条件独立）
这里我们发现朴素贝叶斯假设是约束性很强的假设，“buy”从通常上讲与“price”是有关系，我们这里假设的是条件独立。
模型参数 $$ \begin{aligned} \phi_{i|y=1} &= P(x_i=1|y=1) \
\phi_{i|y=0} &= P(x_i=1|y=0) \
\phi_y&=P(y=1) \end{aligned} $$
joint似然函数为： $$ L(\phi_y,\phi_{i|y=0},\phi_{i|y=1})= \prod_{i=1}^{m}P(x^{(i)},y^{(i)}) $$
求该 Joint似然最大化得： $$ \begin{aligned} \phi_{i|y=1} &= \frac{\sum_{i=1}^mI{x_j^{(i)}=1,y^{(i)}=1}}{\sum_{i=1}^mI{y^{(i)}=1}} \</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-03-12-%E6%A8%A1%E5%9E%8B%E7%AE%80%E8%A6%81%E6%80%BB%E7%BB%93/ rel=bookmark>模型简要总结</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">模型简要总结 决策树 介绍 决策树是一种分类和回归方法。下面主要介绍分类部分。决策树模型呈树状结构，在分类问题中，表示基于特征对实例进行分类的过程。它也可以认为是if-then规则的集合。
模型的学习 决策树的学习本质上是从训练数据中归纳出一组分类规则。能对训练数据进行正确分类的决策树可能有多个，我们需要的是一个与训练数据矛盾较少，同时具有很好的泛化能力。决策树学习通常需要3个步骤：
特征选择 信息增益：特征对训练集的信息增益，等价于互信息。信息增益大的特征具有更强的分类能力。 $$ g(D,A)=H(D)-H(D \mid A) $$ 信息增益比：其信息增益与训练数据集关于特征A的值的熵$H_A(D)$之比 $$ g_R(D,A)=\frac{g(D,A)}{H_A(D)} $$ 决策树生成 决策树的剪枝 优缺点： logstic回归 最大熵 支持向量机 隐马尔可夫模型 条件随机场 关于熵 我们要搞清楚一件非常不确定的事，或是我们一无所知的事情，就需要了解大量的信息。相反，如果我们对某件事情已经有了较多的了解，那么不需要太多的信息就能把它搞清楚。所以，从这个角度来看，可以认为，信息量就等于不确定性的多少。
熵 $$ H(X)=-\sum_{x \in X} P(x) \log P(x) $$
条件熵 $$ H(X \mid Y)=-\sum_{x \in X,y \in Y} P(x,y) \log P(x \mid y) $$ 如果H(X)>=H(X|Y)，也就是说多了Y的信息，关于X的不确定性下降了。
互信息 $$ H(X ; Y)= H(X) - H(X \mid Y) $$ 两件事相关性的量化度量。</div></article></main><div class=pagination><a class="pagination__item pagination__item--prev btn" href=writinglite.com/post/page/5/>«</a>
<span class="pagination__item pagination__item--current">6/8</span>
<a class="pagination__item pagination__item--next btn" href=writinglite.com/post/page/7/>»</a></div></div><aside class="sidebar sidebar--left"><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=writinglite.com></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2020-03-02-%E5%9F%BA%E4%BA%8Enginx%E7%9A%84acme%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E6%96%B9%E6%A1%88/>基于nginx的acme免费证书方案</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-11-04-%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E5%AE%89%E8%A3%85-certbot/>免费证书安装-certbot</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-07-27-%E6%A0%91%E8%8E%93%E6%B4%BE3b+-%E5%AE%89%E8%A3%85openwrt/>树莓派3b+安装openwrt 18.06.4</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-05-18-git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Git 常用命令</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-05-18-linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Linux 常用命令</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=writinglite.com/categories/algorithm/>algorithm</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/deep-learning/>deep learning</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/machine-learning/>machine learning</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/mathematics/>mathematics</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/nlp/>NLP</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/other/>other</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/reading/>reading</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/tensorflow2.x-keras/>Tensorflow2.x keras</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/acme/ title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/albert/ title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/batch-normalization/ title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/bert/ title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/crf/ title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/git/ title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/guice/ title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/hexo/ title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/kaggle/ title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/keras/ title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/layer-normalization/ title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/linux/ title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/log/ title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/logback/ title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/nginx/ title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/openwrt/ title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/slf4j/ title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/stanford-corenlp/ title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow/ title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow-hub/ title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow2.0/ title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tra/ title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/transformers/ title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/ubuntu/ title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%88%86%E8%AF%8D/ title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%93%88%E5%B8%8C/ title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/ title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%B9%B6%E5%8F%91/ title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%8E%92%E5%BA%8F/ title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%95%A3%E5%88%97%E8%A1%A8/ title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/ title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3/ title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%86%B5/ title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%AE%97%E6%B3%95/ title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/ title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/ title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/writinglite.com/js/menu.js></script><script src=/writinglite.com/js/custom.js></script></body></html>