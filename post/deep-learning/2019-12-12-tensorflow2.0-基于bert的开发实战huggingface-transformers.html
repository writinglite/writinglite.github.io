<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Tensorflow2.0 基于BERT的开发实战（huggingface-transformers） - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Tensorflow2.0 基于BERT的开发实战（huggingface-transformers）"><meta property="og:description" content="tensorflow2.0 刚刚发布正式版，网上关于tf2.0使用bert的文章较少。之前想tf2.0做基于bert的NER任务，想了不少资料踩了许多坑，因此将期间的过程总结成文。
在tf2.0上使用bert，主要的难点还是：自己写tf2.0代码适配google的官方源码的方案工作量大对tf熟练度的要求比较高。因此选择了第三方的实现，这里使用的是huggingface的transformers。
除了transformers，其它兼容tf2.0的bert项目还有：
  keras-bert（Star:1.3k, Kashgari在使用）, 现在也开始兼容tf2.0了，但它只支持bert一种预训练模型
  bert-for-tf2（Star:280），缺点是不是很正规，只给了tf2.0 pipeline示例
  在tf2.0正式版发布前后huggingface的transformers也发布了transformers2.0，开始支持tf.2.0的各个预训练模型，虽然没有对pytorch支持的那么全面但在我们的场景已经足够适用了。
环境  tensorflow版本：2.0.0
  transformers版本：2.2.1
 构建模型 class BertNerModel(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self.bert_layer = TFBertMainLayer(config, name='bert') self.bert_layer.trainable = False self.concat_layer = tf.keras.layers.Concatenate(name='concat_bert') def call(self, inputs): outputs = self.bert_layer(inputs) #将后n层的结果相连 tensor = self.concat_layer(list(outputs[2][-4:])) 这里给出的是简要的代码，可以自行根据任务在bert_layer之后加入RNN等
自定义模型的写法可以参考官方源码里的TFBertForSequenceClassification， 继承TFBertPreTrainedModel
self.bert_layer(inputs)的返回值为tuple类型：
 最后1层隐藏层的输出值，shape=(batch_size, max_length, hidden_dimention) [CLS] 对应的输出值，shape=(batch_size, hidden_dimention) 只有设置了config.output_hidden_states = True，才有该值，所有隐藏层的输出值，返回值类型是list 每个list里的值的shape是`(batch_size, max_length, hidden_dimention)``  模型的初始化 bert_ner_model = BertNerModel."><meta property="og:type" content="article"><meta property="og:url" content="/post/deep-learning/2019-12-12-tensorflow2.0-%E5%9F%BA%E4%BA%8Ebert%E7%9A%84%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98huggingface-transformers.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-12-12T08:07:00+00:00"><meta property="article:modified_time" content="2019-12-12T08:07:00+00:00"><meta itemprop=name content="Tensorflow2.0 基于BERT的开发实战（huggingface-transformers）"><meta itemprop=description content="tensorflow2.0 刚刚发布正式版，网上关于tf2.0使用bert的文章较少。之前想tf2.0做基于bert的NER任务，想了不少资料踩了许多坑，因此将期间的过程总结成文。
在tf2.0上使用bert，主要的难点还是：自己写tf2.0代码适配google的官方源码的方案工作量大对tf熟练度的要求比较高。因此选择了第三方的实现，这里使用的是huggingface的transformers。
除了transformers，其它兼容tf2.0的bert项目还有：
  keras-bert（Star:1.3k, Kashgari在使用）, 现在也开始兼容tf2.0了，但它只支持bert一种预训练模型
  bert-for-tf2（Star:280），缺点是不是很正规，只给了tf2.0 pipeline示例
  在tf2.0正式版发布前后huggingface的transformers也发布了transformers2.0，开始支持tf.2.0的各个预训练模型，虽然没有对pytorch支持的那么全面但在我们的场景已经足够适用了。
环境  tensorflow版本：2.0.0
  transformers版本：2.2.1
 构建模型 class BertNerModel(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self.bert_layer = TFBertMainLayer(config, name='bert') self.bert_layer.trainable = False self.concat_layer = tf.keras.layers.Concatenate(name='concat_bert') def call(self, inputs): outputs = self.bert_layer(inputs) #将后n层的结果相连 tensor = self.concat_layer(list(outputs[2][-4:])) 这里给出的是简要的代码，可以自行根据任务在bert_layer之后加入RNN等
自定义模型的写法可以参考官方源码里的TFBertForSequenceClassification， 继承TFBertPreTrainedModel
self.bert_layer(inputs)的返回值为tuple类型：
 最后1层隐藏层的输出值，shape=(batch_size, max_length, hidden_dimention) [CLS] 对应的输出值，shape=(batch_size, hidden_dimention) 只有设置了config.output_hidden_states = True，才有该值，所有隐藏层的输出值，返回值类型是list 每个list里的值的shape是`(batch_size, max_length, hidden_dimention)``  模型的初始化 bert_ner_model = BertNerModel."><meta itemprop=datePublished content="2019-12-12T08:07:00+00:00"><meta itemprop=dateModified content="2019-12-12T08:07:00+00:00"><meta itemprop=wordCount content="234"><meta itemprop=keywords content="深度学习,tensorflow2.0,bert,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Tensorflow2.0 基于BERT的开发实战（huggingface-transformers）"><meta name=twitter:description content="tensorflow2.0 刚刚发布正式版，网上关于tf2.0使用bert的文章较少。之前想tf2.0做基于bert的NER任务，想了不少资料踩了许多坑，因此将期间的过程总结成文。
在tf2.0上使用bert，主要的难点还是：自己写tf2.0代码适配google的官方源码的方案工作量大对tf熟练度的要求比较高。因此选择了第三方的实现，这里使用的是huggingface的transformers。
除了transformers，其它兼容tf2.0的bert项目还有：
  keras-bert（Star:1.3k, Kashgari在使用）, 现在也开始兼容tf2.0了，但它只支持bert一种预训练模型
  bert-for-tf2（Star:280），缺点是不是很正规，只给了tf2.0 pipeline示例
  在tf2.0正式版发布前后huggingface的transformers也发布了transformers2.0，开始支持tf.2.0的各个预训练模型，虽然没有对pytorch支持的那么全面但在我们的场景已经足够适用了。
环境  tensorflow版本：2.0.0
  transformers版本：2.2.1
 构建模型 class BertNerModel(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self.bert_layer = TFBertMainLayer(config, name='bert') self.bert_layer.trainable = False self.concat_layer = tf.keras.layers.Concatenate(name='concat_bert') def call(self, inputs): outputs = self.bert_layer(inputs) #将后n层的结果相连 tensor = self.concat_layer(list(outputs[2][-4:])) 这里给出的是简要的代码，可以自行根据任务在bert_layer之后加入RNN等
自定义模型的写法可以参考官方源码里的TFBertForSequenceClassification， 继承TFBertPreTrainedModel
self.bert_layer(inputs)的返回值为tuple类型：
 最后1层隐藏层的输出值，shape=(batch_size, max_length, hidden_dimention) [CLS] 对应的输出值，shape=(batch_size, hidden_dimention) 只有设置了config.output_hidden_states = True，才有该值，所有隐藏层的输出值，返回值类型是list 每个list里的值的shape是`(batch_size, max_length, hidden_dimention)``  模型的初始化 bert_ner_model = BertNerModel."><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Tensorflow2.0 基于BERT的开发实战（huggingface-transformers）</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-12-12T08:07:00Z>2019-12-12</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><ul><li><a href=#环境>环境</a></li><li><a href=#构建模型>构建模型</a></li><li><a href=#模型的初始化>模型的初始化</a></li><li><a href=#踩过的坑>踩过的坑</a></li><li><a href=#使用huggingface-transformers-的一些技巧>使用huggingface transformers 的一些技巧</a></li></ul></li></ul></nav></div></div><div class="content post__content clearfix"><p>tensorflow2.0 刚刚发布正式版，网上关于tf2.0使用bert的文章较少。之前想tf2.0做基于bert的NER任务，想了不少资料踩了许多坑，因此将期间的过程总结成文。</p><p>在tf2.0上使用bert，主要的难点还是：自己写tf2.0代码适配google的官方源码的方案工作量大对tf熟练度的要求比较高。因此选择了第三方的实现，这里使用的是huggingface的transformers。</p><p>除了transformers，其它兼容tf2.0的bert项目还有：</p><ol><li><p>keras-bert（Star:1.3k, Kashgari在使用）, 现在也开始兼容tf2.0了，但它只支持bert一种预训练模型</p></li><li><p>bert-for-tf2（Star:280），缺点是不是很正规，只给了tf2.0 pipeline示例</p></li></ol><p>在tf2.0正式版发布前后huggingface的transformers也发布了transformers2.0，开始支持tf.2.0的各个预训练模型，虽然没有对pytorch支持的那么全面但在我们的场景已经足够适用了。</p><h3 id=环境>环境</h3><blockquote><p>tensorflow版本：2.0.0</p></blockquote><blockquote><p>transformers版本：2.2.1</p></blockquote><h3 id=构建模型>构建模型</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BertNerModel</span>(TFBertPreTrainedModel):
    <span style=color:#66d9ef>def</span> __init__(self, config, <span style=color:#f92672>*</span>inputs, <span style=color:#f92672>**</span>kwargs):
      super(BERT_NER, self)<span style=color:#f92672>.</span>__init__(config, <span style=color:#f92672>*</span>inputs, <span style=color:#f92672>**</span>kwargs)
      self<span style=color:#f92672>.</span>bert_layer <span style=color:#f92672>=</span> TFBertMainLayer(config, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bert&#39;</span>)
      self<span style=color:#f92672>.</span>bert_layer<span style=color:#f92672>.</span>trainable <span style=color:#f92672>=</span> False
      self<span style=color:#f92672>.</span>concat_layer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Concatenate(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;concat_bert&#39;</span>)
    
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call</span>(self, inputs):
      outputs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>bert_layer(inputs)
      <span style=color:#75715e>#将后n层的结果相连</span>
      tensor <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>concat_layer(list(outputs[<span style=color:#ae81ff>2</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>:]))
</code></pre></div><p>这里给出的是简要的代码，可以自行根据任务在<code>bert_layer</code>之后加入<code>RNN</code>等</p><p>自定义模型的写法可以参考官方源码里的<code>TFBertForSequenceClassification</code>， 继承<code>TFBertPreTrainedModel</code></p><p><code>self.bert_layer(inputs)</code>的返回值为<code>tuple</code>类型：</p><ol><li>最后1层隐藏层的输出值，<code>shape=(batch_size, max_length, hidden_dimention)</code></li><li><code>[CLS]</code> 对应的输出值，<code>shape=(batch_size, hidden_dimention)</code></li><li>只有设置了<code>config.output_hidden_states = True</code>，才有该值，所有隐藏层的输出值，返回值类型是<code>list</code> 每个<code>list</code>里的值的<code>shape</code>是`(batch_size, max_length, hidden_dimention)``</li></ol><h3 id=模型的初始化>模型的初始化</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>bert_ner_model <span style=color:#f92672>=</span> BertNerModel<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-chinese&#34;</span>, output_hidden_states<span style=color:#f92672>=</span>True)
</code></pre></div><p>因为是模型继承的<code>TFBertPreTrainedModel</code>因此这里初始化使用的父类的方式。第一个参数是要加载预训练好的模型参数</p><h3 id=踩过的坑>踩过的坑</h3><ol><li>通过设置：<code>self.bert.trainable = False</code>， 模型可以更快收敛，减少训练时间</li><li>预测的时候，输出的数据一定要与<code>max_length</code>一致，否则效果完全不可用，猜测可能是我们只给了， 没有给<code>input_mask</code>，有看到transformers的源码，如果不给<code>attention_mask </code>，默认全是1的</li><li>通过设置 <code>output_hidden_states=True </code>， 可以得到隐藏层的结果</li></ol><h3 id=使用huggingface-transformers-的一些技巧>使用huggingface transformers 的一些技巧</h3><h4 id=使用google原始预训练模型>使用google原始预训练模型</h4><p>1、先通过<code>transformers </code>命令将原始google预训练的模型文件转换成pytorch格式。这个命令在安装transformers时会回到环境变量中。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>transformers bert <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  chinese_L-12_H-768_A-12/bert_model.ckpt <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  chinese_L-12_H-768_A-12/bert_config.json <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>  chinese_L-12_H-768_A-12/pytorch_model.bin
</code></pre></div><p>output:</p><pre><code>...
Save PyTorch model to chinese_L-12_H-768_A-12/pytorch_model.bin
</code></pre><p>2、加载转换后的模型</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 加载模型</span>
config <span style=color:#f92672>=</span> BertConfig<span style=color:#f92672>.</span>from_json_file(<span style=color:#e6db74>&#39;chinese_L-12_H-768_A-12/bert_config.json&#39;</span>)
model <span style=color:#f92672>=</span> TFBertModel<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;chinese_L-12_H-768_A-12/&#39;</span>,from_pt<span style=color:#f92672>=</span>True, config<span style=color:#f92672>=</span>config)
</code></pre></div><p>在开源代码库下面有好多有关转换的py文件，应该是该功能的源码。例如：<code>convert_bert_original_tf_checkpoint_to_pytorch.py</code>、<code>convert_pytorch_checkpoint_to_tf2.py</code></p><p>那么，确认下该命令的小源码，在<code>setup.py</code>中可以找到如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=color:#960050;background-color:#1e0010>entry_points=</span>{
    <span style=color:#960050;background-color:#1e0010>&#39;console_scripts&#39;:</span> <span style=color:#960050;background-color:#1e0010>[</span>
        <span style=color:#f92672>&#34;transformers=transformers.__main__:main&#34;</span>,
    <span style=color:#960050;background-color:#1e0010>]</span>
}
</code></pre></div><p>可以看到实际执行的<code>__main__</code>文件的<code>main</code>方法，可以找到源码：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>if</span> sys<span style=color:#f92672>.</span>argv[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;bert&#34;</span>:
	<span style=color:#66d9ef>try</span>:
		<span style=color:#f92672>from</span> .convert_bert_original_tf_checkpoint_to_pytorch <span style=color:#f92672>import</span> convert_tf_checkpoint_to_pytorch
	<span style=color:#66d9ef>except</span> <span style=color:#a6e22e>ImportError</span>:
		<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;transformers can only be used from the commandline to convert TensorFlow models in PyTorch, &#34;</span>
			<span style=color:#e6db74>&#34;In that case, it requires TensorFlow to be installed. Please see &#34;</span>
			<span style=color:#e6db74>&#34;https://www.tensorflow.org/install/ for installation instructions.&#34;</span>)
		<span style=color:#66d9ef>raise</span>

	<span style=color:#66d9ef>if</span> len(sys<span style=color:#f92672>.</span>argv) <span style=color:#f92672>!=</span> <span style=color:#ae81ff>5</span>:
		<span style=color:#75715e># pylint: disable=line-too-long</span>
		<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;Should be used as `transformers bert TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`&#34;</span>)
	<span style=color:#66d9ef>else</span>:
		PYTORCH_DUMP_OUTPUT <span style=color:#f92672>=</span> sys<span style=color:#f92672>.</span>argv<span style=color:#f92672>.</span>pop()
		TF_CONFIG <span style=color:#f92672>=</span> sys<span style=color:#f92672>.</span>argv<span style=color:#f92672>.</span>pop()
		TF_CHECKPOINT <span style=color:#f92672>=</span> sys<span style=color:#f92672>.</span>argv<span style=color:#f92672>.</span>pop()
		convert_tf_checkpoint_to_pytorch(TF_CHECKPOINT, TF_CONFIG, PYTORCH_DUMP_OUTPUT)
</code></pre></div><hr><p>更新：</p><ul><li><p>2020-01-17</p><p>huggingface 已经将之前的<code>transformers</code> 命令改为 <code>transformers-cli</code> 根据 <code>setup.py</code> 中 <code>scripts=["transformers-cli"]</code> 找到<code>transformers-cli</code>文件；修改后转换模型的命令为：</p><pre><code># 将tensorflow checkpoint 转换成 pytorch
!transformers-cli convert --model_type bert \
  --tf_checkpoint  chinese_L-12_H-768_A-12/bert_model.ckpt \
  --config  chinese_L-12_H-768_A-12/bert_config.json \
  --pytorch_dump_output chinese_L-12_H-768_A-12/pytorch_model.bin
</code></pre><p>ps : 话说这改的有点勤啊， 还不如直接找源码执行（<code>transformers/commands/convert.py</code> ）：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> transformers.convert_bert_original_tf_checkpoint_to_pytorch <span style=color:#f92672>import</span> (
                    convert_tf_checkpoint_to_pytorch,
                )
convert_tf_checkpoint_to_pytorch(tf_checkpoint, config, pytorch_dump_output)
</code></pre></div></li></ul></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ rel=tag>深度学习</a></li><li class=tags__item><a class="tags__link btn" href=/tags/tensorflow2.0/ rel=tag>tensorflow2.0</a></li><li class=tags__item><a class="tags__link btn" href=/tags/bert/ rel=tag>bert</a></li></ul></div></footer></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/deep-learning/2019-11-22-tensorflow2.0%E4%BD%BF%E7%94%A8berttransformers%E4%B8%8Ekashgari.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>Tensorflow2.0使用bert：transformers与kashgaria</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/deep-learning/2020-02-20-albert%E5%9C%A8bert%E5%9F%BA%E7%A1%80%E4%B8%8A%E7%9A%84%E5%87%A0%E7%82%B9%E6%94%B9%E8%BF%9B.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>Albert在Bert基础上的几点改进</p></a></div></nav><section class=comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//writinglite.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c',repo:'writinglite.com',owner:'writinglite',admin:['writinglite'],id:location.pathname,distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>