<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Tensorflow2.0使用bert：transformers与kashgaria - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Tensorflow2.0使用bert：transformers与kashgaria"><meta property="og:description" content="**背景：**最近打算做NER任务，打算使用kashgaria当baseline，然后使用transformers来做模型开发。
**问题：**使用kashgaria来做Baseline，使用人民日报语料可以得到99.99%的准确率，但是使用transformers的TFBertForTokenClassification来做的话只能得到75%左右的效果， 看了两个项目的源码开始怀疑是模型结构的问题，因此使用transformers模拟kashgaria的结果又做了一次，效果还是在75%左右。相关代码如下：
kashgaria代码：
from kashgari.corpus import ChineseDailyNerCorpus from kashgari.tasks.labeling import BiLSTM_Model from kashgari.embeddings import BERTEmbedding import kashgari train_x, train_y = ChineseDailyNerCorpus.load_data('train') test_x, test_y = ChineseDailyNerCorpus.load_data('test') valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid') bert_embed = BERTEmbedding('chinese_L-12_H-768_A-12', task=kashgari.LABELING, sequence_length=100) model = BiLSTM_Model(bert_embed) model.fit(train_x, train_y, valid_x, valid_y, epochs=1) transformers代码：
# 数据的处理省略，但使用的同样语料 model = TFBertForTokenClassification.from_pretrained('bert-base-chinese') model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') model.fit(all_input_ids, tf.constant(all_label_ids_ca, dtype=tf.float32), epochs=2, batch_size=32) transformers 模拟kashgaria代码：
class BERT_NER(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self."><meta property="og:type" content="article"><meta property="og:url" content="/post/deep-learning/2019-11-22-tensorflow2.0%E4%BD%BF%E7%94%A8berttransformers%E4%B8%8Ekashgari.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-11-22T11:07:00+00:00"><meta property="article:modified_time" content="2019-11-22T11:07:00+00:00"><meta itemprop=name content="Tensorflow2.0使用bert：transformers与kashgaria"><meta itemprop=description content="**背景：**最近打算做NER任务，打算使用kashgaria当baseline，然后使用transformers来做模型开发。
**问题：**使用kashgaria来做Baseline，使用人民日报语料可以得到99.99%的准确率，但是使用transformers的TFBertForTokenClassification来做的话只能得到75%左右的效果， 看了两个项目的源码开始怀疑是模型结构的问题，因此使用transformers模拟kashgaria的结果又做了一次，效果还是在75%左右。相关代码如下：
kashgaria代码：
from kashgari.corpus import ChineseDailyNerCorpus from kashgari.tasks.labeling import BiLSTM_Model from kashgari.embeddings import BERTEmbedding import kashgari train_x, train_y = ChineseDailyNerCorpus.load_data('train') test_x, test_y = ChineseDailyNerCorpus.load_data('test') valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid') bert_embed = BERTEmbedding('chinese_L-12_H-768_A-12', task=kashgari.LABELING, sequence_length=100) model = BiLSTM_Model(bert_embed) model.fit(train_x, train_y, valid_x, valid_y, epochs=1) transformers代码：
# 数据的处理省略，但使用的同样语料 model = TFBertForTokenClassification.from_pretrained('bert-base-chinese') model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') model.fit(all_input_ids, tf.constant(all_label_ids_ca, dtype=tf.float32), epochs=2, batch_size=32) transformers 模拟kashgaria代码：
class BERT_NER(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self."><meta itemprop=datePublished content="2019-11-22T11:07:00+00:00"><meta itemprop=dateModified content="2019-11-22T11:07:00+00:00"><meta itemprop=wordCount content="236"><meta itemprop=keywords content="深度学习,tensorflow2.0,bert,tra,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Tensorflow2.0使用bert：transformers与kashgaria"><meta name=twitter:description content="**背景：**最近打算做NER任务，打算使用kashgaria当baseline，然后使用transformers来做模型开发。
**问题：**使用kashgaria来做Baseline，使用人民日报语料可以得到99.99%的准确率，但是使用transformers的TFBertForTokenClassification来做的话只能得到75%左右的效果， 看了两个项目的源码开始怀疑是模型结构的问题，因此使用transformers模拟kashgaria的结果又做了一次，效果还是在75%左右。相关代码如下：
kashgaria代码：
from kashgari.corpus import ChineseDailyNerCorpus from kashgari.tasks.labeling import BiLSTM_Model from kashgari.embeddings import BERTEmbedding import kashgari train_x, train_y = ChineseDailyNerCorpus.load_data('train') test_x, test_y = ChineseDailyNerCorpus.load_data('test') valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid') bert_embed = BERTEmbedding('chinese_L-12_H-768_A-12', task=kashgari.LABELING, sequence_length=100) model = BiLSTM_Model(bert_embed) model.fit(train_x, train_y, valid_x, valid_y, epochs=1) transformers代码：
# 数据的处理省略，但使用的同样语料 model = TFBertForTokenClassification.from_pretrained('bert-base-chinese') model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') model.fit(all_input_ids, tf.constant(all_label_ids_ca, dtype=tf.float32), epochs=2, batch_size=32) transformers 模拟kashgaria代码：
class BERT_NER(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self."><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Tensorflow2.0使用bert：transformers与kashgaria</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-11-22T11:07:00Z>2019-11-22</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content post__content clearfix"><p>**背景：**最近打算做NER任务，打算使用kashgaria当baseline，然后使用transformers来做模型开发。</p><p>**问题：**使用kashgaria来做Baseline，使用人民日报语料可以得到99.99%的准确率，但是使用transformers的TFBertForTokenClassification来做的话只能得到75%左右的效果， 看了两个项目的源码开始怀疑是模型结构的问题，因此使用transformers模拟kashgaria的结果又做了一次，效果还是在75%左右。相关代码如下：</p><p>kashgaria代码：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> kashgari.corpus <span style=color:#f92672>import</span> ChineseDailyNerCorpus
<span style=color:#f92672>from</span> kashgari.tasks.labeling <span style=color:#f92672>import</span> BiLSTM_Model
<span style=color:#f92672>from</span> kashgari.embeddings <span style=color:#f92672>import</span> BERTEmbedding
<span style=color:#f92672>import</span> kashgari

train_x, train_y <span style=color:#f92672>=</span> ChineseDailyNerCorpus<span style=color:#f92672>.</span>load_data(<span style=color:#e6db74>&#39;train&#39;</span>)
test_x, test_y <span style=color:#f92672>=</span> ChineseDailyNerCorpus<span style=color:#f92672>.</span>load_data(<span style=color:#e6db74>&#39;test&#39;</span>)
valid_x, valid_y <span style=color:#f92672>=</span> ChineseDailyNerCorpus<span style=color:#f92672>.</span>load_data(<span style=color:#e6db74>&#39;valid&#39;</span>)

bert_embed <span style=color:#f92672>=</span> BERTEmbedding(<span style=color:#e6db74>&#39;chinese_L-12_H-768_A-12&#39;</span>,
                           task<span style=color:#f92672>=</span>kashgari<span style=color:#f92672>.</span>LABELING,
                           sequence_length<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
model <span style=color:#f92672>=</span> BiLSTM_Model(bert_embed)
model<span style=color:#f92672>.</span>fit(train_x, train_y, valid_x, valid_y, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)

</code></pre></div><p>transformers代码：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 数据的处理省略，但使用的同样语料</span>
model <span style=color:#f92672>=</span> TFBertForTokenClassification<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-chinese&#39;</span>)
model<span style=color:#f92672>.</span>compile(loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;categorical_crossentropy&#39;</span>, metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>], optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;adam&#39;</span>)
model<span style=color:#f92672>.</span>fit(all_input_ids, tf<span style=color:#f92672>.</span>constant(all_label_ids_ca, dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32), epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>)
</code></pre></div><p>transformers 模拟kashgaria代码：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>BERT_NER</span>(TFBertPreTrainedModel):
  <span style=color:#66d9ef>def</span> __init__(self, config, <span style=color:#f92672>*</span>inputs, <span style=color:#f92672>**</span>kwargs):
      super(BERT_NER, self)<span style=color:#f92672>.</span>__init__(config, <span style=color:#f92672>*</span>inputs, <span style=color:#f92672>**</span>kwargs)
      self<span style=color:#f92672>.</span>num_labels <span style=color:#f92672>=</span> config<span style=color:#f92672>.</span>num_labels
      self<span style=color:#f92672>.</span>bert <span style=color:#f92672>=</span> TFBertMainLayer(config, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bert&#39;</span>)      
      self<span style=color:#f92672>.</span>layer_blstm <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Bidirectional(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>LSTM(units<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, return_sequences<span style=color:#f92672>=</span>True),name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;layer_blstm&#39;</span>)

      self<span style=color:#f92672>.</span>layer_dropout <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.4</span>,name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;layer_dropout&#39;</span>)

      self<span style=color:#f92672>.</span>layer_time_distributed <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>TimeDistributed(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>8</span>), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;layer_time_distributed&#39;</span>)
      self<span style=color:#f92672>.</span>layer_activation <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Activation(<span style=color:#e6db74>&#39;softmax&#39;</span>)

  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call</span>(self, inputs, <span style=color:#f92672>**</span>kwargs):
      output_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>8</span>    
      outputs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>bert(inputs)
      sequence_output <span style=color:#f92672>=</span> outputs[<span style=color:#ae81ff>0</span>]
      tensor <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_blstm(sequence_output)
      tensor <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_dropout(tensor)
      tensor <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_time_distributed(tensor)
      output_tensor <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>layer_activation(tensor)

      <span style=color:#66d9ef>return</span> output_tensor

model <span style=color:#f92672>=</span> BERT_NER<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-chinese&#34;</span>)
model<span style=color:#f92672>.</span>compile(loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;categorical_crossentropy&#39;</span>, metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>], optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;adam&#39;</span>)
model<span style=color:#f92672>.</span>fit(all_input_ids, tf<span style=color:#f92672>.</span>constant(all_label_ids_ca, dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32), epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>)
</code></pre></div><p><strong>直接上结论：</strong></p><p>kashgari的BERTEmbedding默认是非fine-tune，而transformers默认是fine-tune， 也就是说kashgari默认bert的参数是不参加训练的。但效果却比bert参数参加训练的transformers，猜测可能是2个原因：bert模型参数参加训练那么训练的参数过多，在语料比较小的时候并且训练时间不足的情况下反而无法微调到比较过合适的值；或者是NER任务类别不均衡，导致学习偏了。</p><p><strong>解决方案：</strong></p><p>在BERT_NER中，<code>self.bert = TFBertMainLayer(config, name='bert')</code>下面加上<code>self.bert.trainable = False</code>不让bert参数参加训练。另外如果想让kashgari中的bert参数参加训练的话可以在BERTEmbedding初始化时添加参数<code>trainable=True</code></p><p><strong>关于调试：</strong></p><p>在tensorflow2.0中，可以通过<code>tf_model.trainable_variables</code>打印可训练的一些变量。</p><pre><code>&lt;tf.Variable 'layer_blstm_1/forward_lstm_1/kernel:0' shape=(3072, 512) dtype=float32&gt;
&lt;tf.Variable 'layer_blstm_1/forward_lstm_1/recurrent_kernel:0' shape=(128, 512) dtype=float32&gt;
&lt;tf.Variable 'layer_blstm_1/forward_lstm_1/bias:0' shape=(512,) dtype=float32&gt;
&lt;tf.Variable 'layer_blstm_1/backward_lstm_1/kernel:0' shape=(3072, 512) dtype=float32&gt;
&lt;tf.Variable 'layer_blstm_1/backward_lstm_1/recurrent_kernel:0' shape=(128, 512) dtype=float32&gt;
&lt;tf.Variable 'layer_blstm_1/backward_lstm_1/bias:0' shape=(512,) dtype=float32&gt;
&lt;tf.Variable 'layer_time_distributed_1/kernel:0' shape=(256, 8) dtype=float32&gt;
&lt;tf.Variable 'layer_time_distributed_1/bias:0' shape=(8,) dtype=float32&gt;
</code></pre><p>也可以看到里面具体值，可以观察bert参数是否有加载成功。</p><pre><code>&lt;tf.Variable 'bert_ner/bert/embeddings/word_embeddings/weight:0' shape=(21128, 768) dtype=float32, numpy=
array([[ 0.01929518,  0.00411201, -0.02079543, ...,  0.09468532,
         0.00853808,  0.00178786],
       [ 0.00211436,  0.02164099,  0.00108996, ...,  0.08090564,
         0.00178312,  0.02494784],
       [ 0.01467745,  0.00050856,  0.00283794, ...,  0.08360939,
         0.01208044,  0.02821462],
       ...,
       [ 0.03456404,  0.00210567,  0.00852101, ...,  0.00853979,
         0.03371229,  0.00985317],
       [ 0.05406349,  0.02890619,  0.02626012, ...,  0.0525924 ,
         0.06508742,  0.03532186],
       [ 0.02002425,  0.00229523, -0.00892451, ...,  0.07987329,
        -0.05615233,  0.02471835]], dtype=float32)&gt;
</code></pre></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ rel=tag>深度学习</a></li><li class=tags__item><a class="tags__link btn" href=/tags/tensorflow2.0/ rel=tag>tensorflow2.0</a></li><li class=tags__item><a class="tags__link btn" href=/tags/bert/ rel=tag>bert</a></li><li class=tags__item><a class="tags__link btn" href=/tags/tra/ rel=tag>tra</a></li></ul></div></footer></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/other/2019-11-04-%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E5%AE%89%E8%A3%85-certbot.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>免费证书安装-certbot</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/deep-learning/2019-12-12-tensorflow2.0-%E5%9F%BA%E4%BA%8Ebert%E7%9A%84%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98huggingface-transformers.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>Tensorflow2.0 基于BERT的开发实战（huggingface-transformers）</p></a></div></nav><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c',repo:'writinglite.github.io',owner:'writinglite',admin:['writinglite'],id:decodeURI(location.pathname.split("/").pop()).substring(0,49),distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>