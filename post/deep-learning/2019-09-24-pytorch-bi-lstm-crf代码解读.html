<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>pytorch BI-LSTM CRF 代码解读 - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="pytorch BI-LSTM CRF 代码解读"><meta property="og:description" content="词性标注是比较基本的NLP任务，通常我们可以使用BI-LSTM CRF模型来完成；CRF在这里起的作用是能够约束标签序列使结果的合法性更好。例如，如果不使用CRF可能的结果是B-LOC O E-LOC
BLSTM这里就没什么说的了，我们这篇文章主要通过pytorch官网给的代码，讲解下CRF的实现部分。
我们定义两种概率矩阵，发射概率（emission ）和转移概率（transition）。$\text{EMIT}(y_i \rightarrow x_i)$表示 $x_i$映射到$y_i$的非归一化概率，$\text{TRANS}(y_{i-1} \rightarrow y_i)$表示 $y_{i-1}$转移到$y_{i}$的概率。 $$ P(y|x) = \frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})} \
\text{Score}(x,y) = \sum_i \log \psi_i(x,y) \
\text{Score}(x,y) = \sum_i \log \psi_\text{EMIT}(y_i \rightarrow x_i) + \log \psi_\text{TRANS}(y_{i-1} \rightarrow y_i) $$
因而在训练中，我们只需要最大化似然概率$P(y|x)$即可，这里我们利用对数似然 $$ \log{P(y|x)} = \log{(\frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})})} \
= \text{Score}(x, y) - \log{(\sum_{y'} \exp{(\text{Score}(x, y')}))} $$ 所以我们将损失函数定义为$-\log{P(y|x)}$，就可以利用梯度下降法来进行网络的学习了。
在对损失函数进行计算的时候，$\text{Score}(x,y)$的计算很简单，而$\log{(\sum_{y'} \exp{(\text{Score}(x, y')}))}$（下面记作logsumexp）的计算稍微复杂一些，这里使用前向算法（forward algorithm）来进行计算。
如下代码中，feats 代表[W1, W2, W3]，next_tags代表[t1, t2, t3, t4]，当feat=W1，next_tag=t1时，feat里的值是[0.1, 0.2, 0."><meta property="og:type" content="article"><meta property="og:url" content="/post/deep-learning/2019-09-24-pytorch-bi-lstm-crf%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-09-24T10:07:00+00:00"><meta property="article:modified_time" content="2019-09-24T10:07:00+00:00"><meta itemprop=name content="pytorch BI-LSTM CRF 代码解读"><meta itemprop=description content="词性标注是比较基本的NLP任务，通常我们可以使用BI-LSTM CRF模型来完成；CRF在这里起的作用是能够约束标签序列使结果的合法性更好。例如，如果不使用CRF可能的结果是B-LOC O E-LOC
BLSTM这里就没什么说的了，我们这篇文章主要通过pytorch官网给的代码，讲解下CRF的实现部分。
我们定义两种概率矩阵，发射概率（emission ）和转移概率（transition）。$\text{EMIT}(y_i \rightarrow x_i)$表示 $x_i$映射到$y_i$的非归一化概率，$\text{TRANS}(y_{i-1} \rightarrow y_i)$表示 $y_{i-1}$转移到$y_{i}$的概率。 $$ P(y|x) = \frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})} \
\text{Score}(x,y) = \sum_i \log \psi_i(x,y) \
\text{Score}(x,y) = \sum_i \log \psi_\text{EMIT}(y_i \rightarrow x_i) + \log \psi_\text{TRANS}(y_{i-1} \rightarrow y_i) $$
因而在训练中，我们只需要最大化似然概率$P(y|x)$即可，这里我们利用对数似然 $$ \log{P(y|x)} = \log{(\frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})})} \
= \text{Score}(x, y) - \log{(\sum_{y'} \exp{(\text{Score}(x, y')}))} $$ 所以我们将损失函数定义为$-\log{P(y|x)}$，就可以利用梯度下降法来进行网络的学习了。
在对损失函数进行计算的时候，$\text{Score}(x,y)$的计算很简单，而$\log{(\sum_{y'} \exp{(\text{Score}(x, y')}))}$（下面记作logsumexp）的计算稍微复杂一些，这里使用前向算法（forward algorithm）来进行计算。
如下代码中，feats 代表[W1, W2, W3]，next_tags代表[t1, t2, t3, t4]，当feat=W1，next_tag=t1时，feat里的值是[0.1, 0.2, 0."><meta itemprop=datePublished content="2019-09-24T10:07:00+00:00"><meta itemprop=dateModified content="2019-09-24T10:07:00+00:00"><meta itemprop=wordCount content="138"><meta itemprop=keywords content="深度学习,crf,"><meta name=twitter:card content="summary"><meta name=twitter:title content="pytorch BI-LSTM CRF 代码解读"><meta name=twitter:description content="词性标注是比较基本的NLP任务，通常我们可以使用BI-LSTM CRF模型来完成；CRF在这里起的作用是能够约束标签序列使结果的合法性更好。例如，如果不使用CRF可能的结果是B-LOC O E-LOC
BLSTM这里就没什么说的了，我们这篇文章主要通过pytorch官网给的代码，讲解下CRF的实现部分。
我们定义两种概率矩阵，发射概率（emission ）和转移概率（transition）。$\text{EMIT}(y_i \rightarrow x_i)$表示 $x_i$映射到$y_i$的非归一化概率，$\text{TRANS}(y_{i-1} \rightarrow y_i)$表示 $y_{i-1}$转移到$y_{i}$的概率。 $$ P(y|x) = \frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})} \
\text{Score}(x,y) = \sum_i \log \psi_i(x,y) \
\text{Score}(x,y) = \sum_i \log \psi_\text{EMIT}(y_i \rightarrow x_i) + \log \psi_\text{TRANS}(y_{i-1} \rightarrow y_i) $$
因而在训练中，我们只需要最大化似然概率$P(y|x)$即可，这里我们利用对数似然 $$ \log{P(y|x)} = \log{(\frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})})} \
= \text{Score}(x, y) - \log{(\sum_{y'} \exp{(\text{Score}(x, y')}))} $$ 所以我们将损失函数定义为$-\log{P(y|x)}$，就可以利用梯度下降法来进行网络的学习了。
在对损失函数进行计算的时候，$\text{Score}(x,y)$的计算很简单，而$\log{(\sum_{y'} \exp{(\text{Score}(x, y')}))}$（下面记作logsumexp）的计算稍微复杂一些，这里使用前向算法（forward algorithm）来进行计算。
如下代码中，feats 代表[W1, W2, W3]，next_tags代表[t1, t2, t3, t4]，当feat=W1，next_tag=t1时，feat里的值是[0.1, 0.2, 0."><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>pytorch BI-LSTM CRF 代码解读</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-09-24T10:07:00Z>2019-09-24</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content post__content clearfix"><p>词性标注是比较基本的NLP任务，通常我们可以使用BI-LSTM CRF模型来完成；CRF在这里起的作用是能够约束标签序列使结果的合法性更好。例如，如果不使用CRF可能的结果是<code>B-LOC O E-LOC</code></p><p>BLSTM这里就没什么说的了，我们这篇文章主要通过pytorch官网给的代码，讲解下CRF的实现部分。</p><p>我们定义两种概率矩阵，发射概率（emission ）和转移概率（transition）。$\text{EMIT}(y_i \rightarrow x_i)$表示 $x_i$映射到$y_i$的非归一化概率，$\text{TRANS}(y_{i-1} \rightarrow y_i)$表示 $y_{i-1}$转移到$y_{i}$的概率。
$$
P(y|x) = \frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})} \</p><p>\text{Score}(x,y) = \sum_i \log \psi_i(x,y) \</p><p>\text{Score}(x,y) = \sum_i \log \psi_\text{EMIT}(y_i \rightarrow x_i) + \log \psi_\text{TRANS}(y_{i-1} \rightarrow y_i)
$$</p><p>因而在训练中，我们只需要最大化似然概率$P(y|x)$即可，这里我们利用对数似然
$$
\log{P(y|x)} = \log{(\frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})})} \<br>= \text{Score}(x, y) - \log{(\sum_{y'} \exp{(\text{Score}(x, y')}))}
$$
所以我们将损失函数定义为$-\log{P(y|x)}$，就可以利用梯度下降法来进行网络的学习了。</p><p>在对损失函数进行计算的时候，$\text{Score}(x,y)$的计算很简单，而$\log{(\sum_{y'} \exp{(\text{Score}(x, y')}))}$（下面记作logsumexp）的计算稍微复杂一些，这里使用前向算法（forward algorithm）来进行计算。</p><p><img src=_image/1569295153678.gif alt=1569295153678></p><p>如下代码中，feats 代表[W1, W2, W3]，next_tags代表[t1, t2, t3, t4]，当feat=W1，next_tag=t1时，feat里的值是[0.1, 0.2, 0.5, 0.3]，emit_score值为[0.1, 0.1, 0.1, 0.1]即在feat中取next_tag并extand到tag_size。transition 矩阵里的Entry i,j 是从j转移到i的值。所在 trans_score里的值是从t1-t4转移到t1的值。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_forward_alg</span>(self, feats):
        init_alphas <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>full((<span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>tagset_size), <span style=color:#f92672>-</span><span style=color:#ae81ff>10000.</span>)
        init_alphas[<span style=color:#ae81ff>0</span>][self<span style=color:#f92672>.</span>tag_to_ix[START_TAG]] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
        forward_var <span style=color:#f92672>=</span> init_alphas
        <span style=color:#66d9ef>for</span> feat <span style=color:#f92672>in</span> feats:
            alphas_t <span style=color:#f92672>=</span> []  
            <span style=color:#66d9ef>for</span> next_tag <span style=color:#f92672>in</span> range(self<span style=color:#f92672>.</span>tagset_size):
                emit_score <span style=color:#f92672>=</span> feat[next_tag]<span style=color:#f92672>.</span>view(
                    <span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>expand(<span style=color:#ae81ff>1</span>, self<span style=color:#f92672>.</span>tagset_size)
                trans_score <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>transitions[next_tag]<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
                next_tag_var <span style=color:#f92672>=</span> forward_var <span style=color:#f92672>+</span> trans_score <span style=color:#f92672>+</span> emit_score
                alphas_t<span style=color:#f92672>.</span>append(log_sum_exp(next_tag_var)<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>))
            forward_var <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat(alphas_t)<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
        terminal_var <span style=color:#f92672>=</span> forward_var <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>transitions[self<span style=color:#f92672>.</span>tag_to_ix[STOP_TAG]]
        alpha <span style=color:#f92672>=</span> log_sum_exp(terminal_var)
        <span style=color:#66d9ef>return</span> alpha
</code></pre></div><p><strong>参考：</strong></p><p><a href=https://zhuanlan.zhihu.com/p/27338210>https://zhuanlan.zhihu.com/p/27338210</a></p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ rel=tag>深度学习</a></li><li class=tags__item><a class="tags__link btn" href=/tags/crf/ rel=tag>crf</a></li></ul></div></footer></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/deep-learning/2019-09-14-bert%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>bert的基本使用</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/other/2019-11-04-%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E5%AE%89%E8%A3%85-certbot.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>免费证书安装-certbot</p></a></div></nav><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c',repo:'writinglite.github.io',owner:'writinglite',admin:['writinglite'],id:'aaaaa',distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>