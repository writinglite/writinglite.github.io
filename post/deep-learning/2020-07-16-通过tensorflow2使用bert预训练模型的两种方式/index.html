<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>通过Tensorflow2使用Bert预训练模型的两种方式 - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="通过Tensorflow2使用Bert预训练模型的两种方式"><meta property="og:description" content="以中文Bert为例
下面的例子均以中文Bert预训练模型为例
方式1：使用Tensorflow Hub import tensorflow as tf import tensorflow_hub as hub hub_url_or_local_path = &#34;https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/2&#34; # 或者将模型下载到本地 # !wget &#34;https://storage.googleapis.com/tfhub-modules/tensorflow/bert_zh_L-12_H-768_A-12/2.tar.gz&#34; # !tar -xzvf 2.tar.gz -C bert_zh_L-12_H-768_A-12 # hub_url_or_local_path = &#34;./bert_zh_L-12_H-768_A-12&#34; def build_model(): input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&#34;input_word_ids&#34;) input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&#34;input_mask&#34;) segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&#34;segment_ids&#34;) bert_layer = hub.KerasLayer(hub_url_or_local_path, name='bert', trainable=True) pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output) return model model = build_model() model.summary() 使用这种方式的时候在执行build_model时bert的参数已经被加载进来了。"><meta property="og:type" content="article"><meta property="og:url" content="/post/deep-learning/2020-07-16-%E9%80%9A%E8%BF%87tensorflow2%E4%BD%BF%E7%94%A8bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/"><meta property="article:section" content="post"><meta itemprop=name content="通过Tensorflow2使用Bert预训练模型的两种方式"><meta itemprop=description content="以中文Bert为例
下面的例子均以中文Bert预训练模型为例
方式1：使用Tensorflow Hub import tensorflow as tf import tensorflow_hub as hub hub_url_or_local_path = &#34;https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/2&#34; # 或者将模型下载到本地 # !wget &#34;https://storage.googleapis.com/tfhub-modules/tensorflow/bert_zh_L-12_H-768_A-12/2.tar.gz&#34; # !tar -xzvf 2.tar.gz -C bert_zh_L-12_H-768_A-12 # hub_url_or_local_path = &#34;./bert_zh_L-12_H-768_A-12&#34; def build_model(): input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&#34;input_word_ids&#34;) input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&#34;input_mask&#34;) segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&#34;segment_ids&#34;) bert_layer = hub.KerasLayer(hub_url_or_local_path, name='bert', trainable=True) pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output) return model model = build_model() model.summary() 使用这种方式的时候在执行build_model时bert的参数已经被加载进来了。"><meta itemprop=wordCount content="133"><meta itemprop=keywords content="bert,transformers,tensorflow hub,"><meta name=twitter:card content="summary"><meta name=twitter:title content="通过Tensorflow2使用Bert预训练模型的两种方式"><meta name=twitter:description content="以中文Bert为例
下面的例子均以中文Bert预训练模型为例
方式1：使用Tensorflow Hub import tensorflow as tf import tensorflow_hub as hub hub_url_or_local_path = &#34;https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/2&#34; # 或者将模型下载到本地 # !wget &#34;https://storage.googleapis.com/tfhub-modules/tensorflow/bert_zh_L-12_H-768_A-12/2.tar.gz&#34; # !tar -xzvf 2.tar.gz -C bert_zh_L-12_H-768_A-12 # hub_url_or_local_path = &#34;./bert_zh_L-12_H-768_A-12&#34; def build_model(): input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&#34;input_word_ids&#34;) input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&#34;input_mask&#34;) segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&#34;segment_ids&#34;) bert_layer = hub.KerasLayer(hub_url_or_local_path, name='bert', trainable=True) pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output) return model model = build_model() model.summary() 使用这种方式的时候在执行build_model时bert的参数已经被加载进来了。"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>通过Tensorflow2使用Bert预训练模型的两种方式</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><ul><li></li></ul></li></ul></nav></div></div><div class="content post__content clearfix"><p>以中文Bert为例</p><p>下面的例子均以中文Bert预训练模型为例</p><h4 id=方式1使用tensorflow-hub>方式1：使用Tensorflow Hub</h4><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> tensorflow <span style=color:#f92672>as</span> tf
<span style=color:#f92672>import</span> tensorflow_hub <span style=color:#f92672>as</span> hub

hub_url_or_local_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/2&#34;</span>

<span style=color:#75715e># 或者将模型下载到本地</span>
<span style=color:#75715e># !wget &#34;https://storage.googleapis.com/tfhub-modules/tensorflow/bert_zh_L-12_H-768_A-12/2.tar.gz&#34;</span>
<span style=color:#75715e># !tar -xzvf 2.tar.gz -C bert_zh_L-12_H-768_A-12</span>
<span style=color:#75715e># hub_url_or_local_path = &#34;./bert_zh_L-12_H-768_A-12&#34;</span>


<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_model</span>():
  input_word_ids <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Input(shape<span style=color:#f92672>=</span>(max_seq_length,), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>int32, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;input_word_ids&#34;</span>)
  input_mask <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Input(shape<span style=color:#f92672>=</span>(max_seq_length,), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>int32, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;input_mask&#34;</span>)
  segment_ids <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Input(shape<span style=color:#f92672>=</span>(max_seq_length,), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>int32, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;segment_ids&#34;</span>)
  bert_layer <span style=color:#f92672>=</span> hub<span style=color:#f92672>.</span>KerasLayer(hub_url_or_local_path, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bert&#39;</span>, trainable<span style=color:#f92672>=</span>True)
  pooled_output, sequence_output <span style=color:#f92672>=</span> bert_layer([input_word_ids, input_mask, segment_ids])
  model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Model(inputs<span style=color:#f92672>=</span>[input_word_ids, input_mask, segment_ids], outputs<span style=color:#f92672>=</span>sequence_output)
  <span style=color:#66d9ef>return</span> model

model <span style=color:#f92672>=</span> build_model()
model<span style=color:#f92672>.</span>summary()
</code></pre></div><p>使用这种方式的时候在执行build_model时bert的参数已经被加载进来了。</p><h4 id=方式2使用transformers>方式2：使用Transformers</h4><p>通过transformers使用bert模型的方式不只这一种，下面的方式是我个人比较喜欢的一种方式（尽量使用Tensorflow原生方式）。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> tensorflow <span style=color:#f92672>as</span> tf
<span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TFBertMainLayer

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_model</span>(cls_num):
    input_ids <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Input(shape<span style=color:#f92672>=</span>[None, ], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>int32, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;input_ids&#39;</span>)
    attention_mask <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Input(shape<span style=color:#f92672>=</span>[None, ], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>int32, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;attention_mask&#39;</span>)
    token_type_ids <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Input(shape<span style=color:#f92672>=</span>[None, ], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>int32, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;token_type_ids&#39;</span>)
    bert <span style=color:#f92672>=</span> TFBertMainLayer(bert_config, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;bert&#39;</span>)
    bert<span style=color:#f92672>.</span>trainable <span style=color:#f92672>=</span> True
    outputs <span style=color:#f92672>=</span> bert(input_ids, attention_mask, token_type_ids)
    pool <span style=color:#f92672>=</span> outputs[<span style=color:#ae81ff>1</span>]
    model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Model(inputs<span style=color:#f92672>=</span>[input_ids, attention_mask, token_type_ids], outputs<span style=color:#f92672>=</span>pool)
    <span style=color:#66d9ef>return</span> model

bert_config <span style=color:#f92672>=</span> BertConfig<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-chinese&#34;</span>)
model <span style=color:#f92672>=</span> build_model(bert_config<span style=color:#f92672>=</span>bert_config)
model<span style=color:#f92672>.</span>load_weights(pre_train_save_file, by_name<span style=color:#f92672>=</span>True)
</code></pre></div><p>使用这种方式的时候build_model时，bert的参数还没有加载，需要通过load_weigths方式将参数加载进来。</p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/bert/ rel=tag>bert</a></li><li class=tags__item><a class="tags__link btn" href=/tags/transformers/ rel=tag>transformers</a></li><li class=tags__item><a class="tags__link btn" href=/tags/tensorflow-hub/ rel=tag>tensorflow hub</a></li></ul></div></footer></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/machine-learning/2016-11-25-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/ rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>隐马尔可夫模型</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/machine-learning/2017-06-04-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/ rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>统计学习方法概论</p></a></div></nav></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>