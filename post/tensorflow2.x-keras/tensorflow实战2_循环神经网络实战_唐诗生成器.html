<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Tensorflow实战(2)_循环神经网络实战_唐诗生成器 - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Tensorflow实战(2)_循环神经网络实战_唐诗生成器"><meta property="og:description" content="本次分享内容：  上次分享内容回顾 RNN 理论知识 唐诗生成器实例讲解 本次课程内容总结  上次分享我们介绍了关于Tensorflow2的基础语法，以及通过手写数字识别任务讲解了如何通过Tensorflow2来搭建全连接神经网络模型。今天我们来介绍用于处理序列信息的网络结构-RNN。
RNN理论知识 循环神经网络（Recurrent neural network：RNN）是神经网络的一种。循环神经网络可以描述动态时间行为，与前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。 如果说一个全连接的神经网络的计算公式是：$y^t=f(x^t)$，那么RNN的公式可以这样表示$y^t, h^t = f(x^t, h^{t-1})$ 。一个单层的RNN可以用下图表示： Deep RNN 与全连接神经网络一样，RNN也可以叠加多层： Bidirectional RNN Bidirectional RNN 是将传统RNN的状态神经元拆分为两个部分，一个负责forward states，另一个负责backward states。Forward states的输出并不会连接到Backward states的输入。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。 RNN 计算方式 Native RNN $$ \begin{align} & h_t = \sigma(W^h h_{t-1} + W^i x_t) \
& y_t = \sigma(W^o h_t) \end{align} $$
LSTM $$ \begin{align} & i_t = sigm(W^{xi}x_t + W^{hi}h_{t-1}) \
& f_t = sigm(W^{xf}x_t + W^{hf}h_{t-1}) \"><meta property="og:type" content="article"><meta property="og:url" content="/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%982_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%94%90%E8%AF%97%E7%94%9F%E6%88%90%E5%99%A8.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-07-22T20:43:00+00:00"><meta property="article:modified_time" content="2020-07-22T20:43:00+00:00"><meta itemprop=name content="Tensorflow实战(2)_循环神经网络实战_唐诗生成器"><meta itemprop=description content="本次分享内容：  上次分享内容回顾 RNN 理论知识 唐诗生成器实例讲解 本次课程内容总结  上次分享我们介绍了关于Tensorflow2的基础语法，以及通过手写数字识别任务讲解了如何通过Tensorflow2来搭建全连接神经网络模型。今天我们来介绍用于处理序列信息的网络结构-RNN。
RNN理论知识 循环神经网络（Recurrent neural network：RNN）是神经网络的一种。循环神经网络可以描述动态时间行为，与前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。 如果说一个全连接的神经网络的计算公式是：$y^t=f(x^t)$，那么RNN的公式可以这样表示$y^t, h^t = f(x^t, h^{t-1})$ 。一个单层的RNN可以用下图表示： Deep RNN 与全连接神经网络一样，RNN也可以叠加多层： Bidirectional RNN Bidirectional RNN 是将传统RNN的状态神经元拆分为两个部分，一个负责forward states，另一个负责backward states。Forward states的输出并不会连接到Backward states的输入。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。 RNN 计算方式 Native RNN $$ \begin{align} & h_t = \sigma(W^h h_{t-1} + W^i x_t) \
& y_t = \sigma(W^o h_t) \end{align} $$
LSTM $$ \begin{align} & i_t = sigm(W^{xi}x_t + W^{hi}h_{t-1}) \
& f_t = sigm(W^{xf}x_t + W^{hf}h_{t-1}) \"><meta itemprop=datePublished content="2020-07-22T20:43:00+00:00"><meta itemprop=dateModified content="2020-07-22T20:43:00+00:00"><meta itemprop=wordCount content="952"><meta itemprop=keywords content="深度学习,Tensorflow,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Tensorflow实战(2)_循环神经网络实战_唐诗生成器"><meta name=twitter:description content="本次分享内容：  上次分享内容回顾 RNN 理论知识 唐诗生成器实例讲解 本次课程内容总结  上次分享我们介绍了关于Tensorflow2的基础语法，以及通过手写数字识别任务讲解了如何通过Tensorflow2来搭建全连接神经网络模型。今天我们来介绍用于处理序列信息的网络结构-RNN。
RNN理论知识 循环神经网络（Recurrent neural network：RNN）是神经网络的一种。循环神经网络可以描述动态时间行为，与前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。 如果说一个全连接的神经网络的计算公式是：$y^t=f(x^t)$，那么RNN的公式可以这样表示$y^t, h^t = f(x^t, h^{t-1})$ 。一个单层的RNN可以用下图表示： Deep RNN 与全连接神经网络一样，RNN也可以叠加多层： Bidirectional RNN Bidirectional RNN 是将传统RNN的状态神经元拆分为两个部分，一个负责forward states，另一个负责backward states。Forward states的输出并不会连接到Backward states的输入。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。 RNN 计算方式 Native RNN $$ \begin{align} & h_t = \sigma(W^h h_{t-1} + W^i x_t) \
& y_t = \sigma(W^o h_t) \end{align} $$
LSTM $$ \begin{align} & i_t = sigm(W^{xi}x_t + W^{hi}h_{t-1}) \
& f_t = sigm(W^{xf}x_t + W^{hf}h_{t-1}) \"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Tensorflow实战(2)_循环神经网络实战_唐诗生成器</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-07-22T20:43:00Z>2020-07-22</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/tensorflow2.x-keras.html rel=category>Tensorflow2.x keras</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#本次分享内容>本次分享内容：</a></li><li><a href=#rnn理论知识>RNN理论知识</a><ul><li><a href=#deep-rnn>Deep RNN</a></li><li><a href=#bidirectional-rnn>Bidirectional RNN</a></li><li><a href=#rnn-计算方式>RNN 计算方式</a></li><li><a href=#tensorflow-code>Tensorflow Code</a></li></ul></li><li><a href=#唐诗生成器实例>唐诗生成器实例</a><ul><li><a href=#数据处理>数据处理</a></li><li><a href=#搭建模型>搭建模型</a></li><li><a href=#模型训练>模型训练</a></li><li><a href=#查看效果>查看效果</a></li></ul></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=本次分享内容>本次分享内容：</h2><ul><li>上次分享内容回顾</li><li>RNN 理论知识</li><li>唐诗生成器实例讲解</li><li>本次课程内容总结</li></ul><p>上次分享我们介绍了关于Tensorflow2的基础语法，以及通过手写数字识别任务讲解了如何通过Tensorflow2来搭建全连接神经网络模型。今天我们来介绍用于处理序列信息的网络结构-RNN。</p><h2 id=rnn理论知识>RNN理论知识</h2><p>循环神经网络（Recurrent neural network：RNN）是神经网络的一种。循环神经网络可以描述动态时间行为，与前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。
如果说一个全连接的神经网络的计算公式是：$y^t=f(x^t)$，那么RNN的公式可以这样表示$y^t, h^t = f(x^t, h^{t-1})$ 。一个单层的RNN可以用下图表示：
<img src=_image/RNN.png alt></p><h3 id=deep-rnn>Deep RNN</h3><p>与全连接神经网络一样，RNN也可以叠加多层：
<img src=_image/Deep_RNN.png alt></p><h3 id=bidirectional-rnn>Bidirectional RNN</h3><p>Bidirectional RNN 是将传统RNN的状态神经元拆分为两个部分，一个负责forward states，另一个负责backward states。Forward states的输出并不会连接到Backward states的输入。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。
<img src=_image/Bidirectional_RNN.png alt></p><h3 id=rnn-计算方式>RNN 计算方式</h3><h4 id=native-rnn>Native RNN</h4><p>$$
\begin{align}
& h_t = \sigma(W^h h_{t-1} + W^i x_t) \<br>& y_t = \sigma(W^o h_t)
\end{align}
$$</p><h4 id=lstm>LSTM</h4><p>$$
\begin{align}
& i_t = sigm(W^{xi}x_t + W^{hi}h_{t-1}) \<br>& f_t = sigm(W^{xf}x_t + W^{hf}h_{t-1}) \<br>& o_t = sigm(W^{xo}x_t + W^{ho}h_{t-1}) \<br>& \tilde{c_t} = tanh(W^{xc}x_t + W^{hc}h_{t-1}) \<br>& c_t = f_t \bigodot c_{t-1} + i_t \bigodot \tilde{c_t} \<br>& h_t = o_t \bigodot tanh(c_t) \<br>& y_t = \sigma(W^o h_t)
\end{align}
$$
LSTM内部主要有阶段：</p><ol><li><p>忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。具体来说是通过计算得到的 $f_t$ （f表示forget）来作为忘记门控，来控制上一个状态的 $c_{t-1}$ 哪些需要留哪些需要忘。</p></li><li><p>选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 $\tilde{c_t}$ 表示。而选择的门控信号则是由 $i_t$ （i代表information）来进行控制。</p></li><li><p>将上面两步得到的结果相加，即可得到传输给下一个状态的 $c_{t}$ 。也就是上图中的第一个公式。</p></li><li><p>输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 $o_t$ 来进行控制的。并且对上一阶段得到的 $c_t$ 进行了缩放（通过一个tanh激活函数进行变化）。</p></li></ol><p>另外LSTM与标准RNN不同的是它有两个state，分别是$c_t, h_t$ 。</p><h4 id=gru>GRU</h4><p>$$
\begin{align}
& z_t = sigm(W^{xz}x_t + W^{hz}h_{t-1}) \<br>& r_t = sigm(W^{xr}x_t + W^{hr}h_{t-1}) \<br>& \tilde{h_t} = tanh(W^{xh}x_t + r_t \bigodot (W^{hh}h_{t-1}) \<br>& h_t = (1-z_t) \bigodot \tilde{h_t} + z_t \bigodot h_{t-1} \<br>& y_t = \sigma(W^o h_t)
\end{align}
$$</p><p>$z_t \bigodot h_{t-1}$ ：表示对原本隐藏状态的选择性“遗忘”。这里的 $z_t$ 可以想象成遗忘门（forget gate），忘记 $h_{t-1}$ 维度中一些不重要的信息。</p><p>$(1-z_t) \bigodot \tilde{h_t}$ ：表示对包含当前节点信息的 $\tilde{h_t}$ 进行选择性“记忆”。</p><p>GRU很聪明的一点就在于，我们使用了同一个门控 $z_t$ 就同时可以进行遗忘和选择记忆</p><h3 id=tensorflow-code>Tensorflow Code</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>import</span> random
<span style=color:#f92672>import</span> tensorflow <span style=color:#f92672>as</span> tf
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># RNN layer 与 RNNCell layer 的区别</span>

<span style=color:#75715e># 锄禾日当午 -&gt; [0, 1, 2, 3, 4]</span>
<span style=color:#75715e># 离离原上草 -&gt; [5, 6, 6, 7, 8]</span>

words <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>], [<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>]], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>int32)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;words :&#39;</span>, words<span style=color:#f92672>.</span>shape) <span style=color:#75715e># batch_size, sequence_length</span>

embedding_ret <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Embedding(input_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>9</span>, output_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)(words)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;embedding ret shape :&#39;</span>, embedding_ret<span style=color:#f92672>.</span>shape) <span style=color:#75715e># batch_size, sequence_length, embedding_size</span>

gru_cell <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>GRUCell(units<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>)

rnn <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>RNN(gru_cell)
initial_state<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>zeros(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
rnn_ret <span style=color:#f92672>=</span> rnn(embedding_ret, initial_state<span style=color:#f92672>=</span>initial_state)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;rnn ret is : </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>, rnn_ret) <span style=color:#75715e># batch_size, rnn_units</span>

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>self_rnn</span>(rnn_cell, embedding, initial_state):
    states <span style=color:#f92672>=</span> initial_state
    all_outputs <span style=color:#f92672>=</span> []
    all_states <span style=color:#f92672>=</span> []
    <span style=color:#66d9ef>for</span> position <span style=color:#f92672>in</span> range(embedding<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]): <span style=color:#75715e># embedding.shape[1] 对应字符串长度</span>
        inputs <span style=color:#f92672>=</span> embedding[: , position, : ] <span style=color:#75715e># 取每个batch上，对应位置上的embedding值 </span>
        outputs, states <span style=color:#f92672>=</span> rnn_cell(inputs, states)
        all_outputs<span style=color:#f92672>.</span>append(outputs)
        all_states<span style=color:#f92672>.</span>append(states)
    <span style=color:#66d9ef>return</span> all_outputs[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]

self_rnn_ret <span style=color:#f92672>=</span> self_rnn(gru_cell, embedding_ret, initial_state<span style=color:#f92672>=</span>initial_state)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;self rnn ret is : </span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>, self_rnn_ret)
</code></pre></div><pre><code>words : (2, 5)
embedding ret shape : (2, 5, 4)
rnn ret is : 
 tf.Tensor(
[[ 0.02378952  0.01149105  0.01898541  0.02086249]
 [-0.00381493  0.01658207  0.00360551  0.00680503]], shape=(2, 4), dtype=float32)
self rnn ret is : 
 tf.Tensor(
[[ 0.02378952  0.01149105  0.01898541  0.02086249]
 [-0.00381493  0.01658207  0.00360551  0.00680503]], shape=(2, 4), dtype=float32)
</code></pre><p>RNNCell是RNN是单步的执行单元，RNN layer 的执行原理是遍历每步并将state将向后转递</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>lstm_cell <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>LSTMCell(<span style=color:#ae81ff>4</span>)
inputs <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>ones(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
states1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>zeros(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
states2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>zeros(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
outpus, states <span style=color:#f92672>=</span> lstm_cell(inputs, (states1, states2))
<span style=color:#66d9ef>print</span>(states)
</code></pre></div><pre><code>[&lt;tf.Tensor: shape=(2, 4), dtype=float32, numpy=
array([[ 0.251225  , -0.09634911,  0.13881163, -0.04482196],
       [ 0.251225  , -0.09634911,  0.13881163, -0.04482196]],
      dtype=float32)&gt;, &lt;tf.Tensor: shape=(2, 4), dtype=float32, numpy=
array([[ 0.47264358, -0.22557989,  0.3101259 , -0.10946771],
       [ 0.47264358, -0.22557989,  0.3101259 , -0.10946771]],
      dtype=float32)&gt;]
</code></pre><p>可以看出代码结果与理论是对应的，LSTM 会返回两个state，一个是$c_t$， 另一个是$h_t$</p><h2 id=唐诗生成器实例>唐诗生成器实例</h2><p>古诗生成器：已知有一堆现成的古诗作为训练数据，我们的目标是生成一个可以写作古诗的模型。</p><p><img src=_image/task.png alt></p><h3 id=数据处理>数据处理</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>read_poem</span>(file_name):
    poems <span style=color:#f92672>=</span> []
    file <span style=color:#f92672>=</span> open(file_name, <span style=color:#e6db74>&#34;r&#34;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;utf-8&#39;</span>)
    words <span style=color:#f92672>=</span> []
    <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> file:  <span style=color:#75715e>#every line is a poem</span>
        title, author, poem <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>strip()<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;::&#34;</span>)  <span style=color:#75715e>#get title and poem</span>
        poem <span style=color:#f92672>=</span> poem<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39; &#39;</span>,<span style=color:#e6db74>&#39;&#39;</span>)
        <span style=color:#66d9ef>if</span> len(poem) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>10</span> <span style=color:#f92672>or</span> len(poem) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>64</span>:  <span style=color:#75715e>#filter poem</span>
            <span style=color:#66d9ef>continue</span>
        <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;_&#39;</span> <span style=color:#f92672>in</span> poem <span style=color:#f92672>or</span> <span style=color:#e6db74>&#39;《&#39;</span> <span style=color:#f92672>in</span> poem <span style=color:#f92672>or</span> <span style=color:#e6db74>&#39;[&#39;</span> <span style=color:#f92672>in</span> poem <span style=color:#f92672>or</span> <span style=color:#e6db74>&#39;(&#39;</span> <span style=color:#f92672>in</span> poem <span style=color:#f92672>or</span> <span style=color:#e6db74>&#39;（&#39;</span> <span style=color:#f92672>in</span> poem:
            <span style=color:#66d9ef>continue</span>
        <span style=color:#75715e># poem = &#39;[&#39; + poem + &#39;]&#39; #add start and end signs</span>
        <span style=color:#66d9ef>if</span> len(poem) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>100</span>:
            <span style=color:#66d9ef>continue</span>
        poems<span style=color:#f92672>.</span>append(list(poem))
        words <span style=color:#f92672>+=</span> list(poem)
  
    words <span style=color:#f92672>=</span> list(set(words))
    words<span style=color:#f92672>.</span>sort()
    words <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;[PAD]&#39;</span>, <span style=color:#e6db74>&#39;[BOS]&#39;</span>, <span style=color:#e6db74>&#39;[EOS]&#39;</span>] <span style=color:#f92672>+</span> words
  
    word_dict <span style=color:#f92672>=</span> {}
    <span style=color:#66d9ef>for</span> wid, word <span style=color:#f92672>in</span> enumerate(words):
        word_dict[word] <span style=color:#f92672>=</span> wid 
    <span style=color:#66d9ef>return</span> poems, words, word_dict

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_data_set</span>(data, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>):
    ret <span style=color:#f92672>=</span> []
    <span style=color:#66d9ef>for</span> text_list <span style=color:#f92672>in</span> data:
        ids <span style=color:#f92672>=</span> [word_dict[word] <span style=color:#66d9ef>for</span> word <span style=color:#f92672>in</span> text_list]
        ret<span style=color:#f92672>.</span>append(ids)
    <span style=color:#66d9ef>return</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>sequence<span style=color:#f92672>.</span>pad_sequences(ret, maxlen<span style=color:#f92672>=</span>maxlen, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;post&#39;</span>)


poems, words, word_dict <span style=color:#f92672>=</span> read_poem(<span style=color:#e6db74>&#39;data/poetryTang.txt&#39;</span>)

vocab_number <span style=color:#f92672>=</span> len(words) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;字典大小：&#39;</span>, str(vocab_number))

x_data <span style=color:#f92672>=</span> [[<span style=color:#e6db74>&#39;[BOS]&#39;</span>] <span style=color:#f92672>+</span> poem <span style=color:#66d9ef>for</span> poem <span style=color:#f92672>in</span> poems]
y_data <span style=color:#f92672>=</span> [poem <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;[EOS]&#39;</span>] <span style=color:#66d9ef>for</span> poem <span style=color:#f92672>in</span> poems]

x_valid_data <span style=color:#f92672>=</span> x_data[<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>1000</span>]
y_valid_data <span style=color:#f92672>=</span> y_data[<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>1000</span>]

x_train_data <span style=color:#f92672>=</span> x_data[<span style=color:#ae81ff>1000</span>:]
y_train_data <span style=color:#f92672>=</span> y_data[<span style=color:#ae81ff>1000</span>:]

x_mini_train_data <span style=color:#f92672>=</span> x_train_data[<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>1000</span>]
y_mini_train_data <span style=color:#f92672>=</span> y_train_data[<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>1000</span>]

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;训练集size :&#39;</span>, len(x_train_data))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;验证集size :&#39;</span>, len(x_valid_data))

x_train_dataset  <span style=color:#f92672>=</span> generate_data_set(x_train_data, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>65</span>)
y_train_dataset  <span style=color:#f92672>=</span> generate_data_set(y_train_data, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>65</span>)

x_valid_dataset  <span style=color:#f92672>=</span> generate_data_set(x_valid_data, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>65</span>)
y_valid_dataset  <span style=color:#f92672>=</span> generate_data_set(y_valid_data, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>65</span>)

x_mini_train_dataset  <span style=color:#f92672>=</span> generate_data_set(x_mini_train_data, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>65</span>)
y_mini_train_dataset  <span style=color:#f92672>=</span> generate_data_set(y_mini_train_data, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>65</span>)

<span style=color:#75715e># np.array(words)[x_train_dataset[0]]</span>
</code></pre></div><pre><code>字典大小： 6632
训练集size : 41136
验证集size : 1000
</code></pre><h3 id=搭建模型>搭建模型</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_model</span>(vocab_size):
    input <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Input(shape<span style=color:#f92672>=</span>[None,], name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;inputs&#39;</span>)
    embedding <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Embedding(input_dim<span style=color:#f92672>=</span>vocab_size, output_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>768</span>, mask_zero<span style=color:#f92672>=</span>True, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;embedding_layer&#39;</span>)
    output <span style=color:#f92672>=</span> embedding(input)
    output <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.5</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout_1&#39;</span>)(output)
    output <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>GRU(units <span style=color:#f92672>=</span> <span style=color:#ae81ff>1024</span>, return_sequences<span style=color:#f92672>=</span>True, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rnn_layer_1&#39;</span>)(output)
    output <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>LayerNormalization(epsilon<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;layernorm_layer_1&#39;</span>)(output)

    output <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>GRU(units<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>, return_sequences<span style=color:#f92672>=</span>True, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;rnn_layer_2&#39;</span>)(output)
    output <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>LayerNormalization(epsilon<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;layernorm_layer_2&#39;</span>)(output)

    output <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.5</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;dropout_2&#39;</span>)(output)
    output <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(vocab_number, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;output_layer&#39;</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;softmax&#34;</span>)(output)
    model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Model(inputs<span style=color:#f92672>=</span>input, outputs<span style=color:#f92672>=</span>output, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;rnn_genrater_model&#34;</span>)
    <span style=color:#66d9ef>return</span> model
model <span style=color:#f92672>=</span> build_model(vocab_size<span style=color:#f92672>=</span>vocab_number)
model<span style=color:#f92672>.</span>load_weights(<span style=color:#e6db74>&#39;./model_20.h5&#39;</span>, by_name<span style=color:#f92672>=</span>True)
model<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;rnn_genrater_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
inputs (InputLayer)          [(None, None)]            0         
_________________________________________________________________
embedding_layer (Embedding)  (None, None, 768)         5093376   
_________________________________________________________________
dropout_1 (Dropout)          (None, None, 768)         0         
_________________________________________________________________
rnn_layer_1 (GRU)            (None, None, 1024)        5511168   
_________________________________________________________________
layernorm_layer_1 (LayerNorm (None, None, 1024)        2048      
_________________________________________________________________
rnn_layer_2 (GRU)            (None, None, 512)         2362368   
_________________________________________________________________
layernorm_layer_2 (LayerNorm (None, None, 512)         1024      
_________________________________________________________________
dropout_2 (Dropout)          (None, None, 512)         0         
_________________________________________________________________
output_layer (Dense)         (None, None, 6632)        3402216   
=================================================================
Total params: 16,372,200
Trainable params: 16,372,200
Non-trainable params: 0
_________________________________________________________________
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>loss_object <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>losses<span style=color:#f92672>.</span>SparseCategoricalCrossentropy(reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;none&#39;</span>)
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>loss_function</span>(real, pred):
    mask <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>logical_not(tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>equal(real, <span style=color:#ae81ff>0</span>))
    loss_ <span style=color:#f92672>=</span> loss_object(real, pred) 

    mask <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>cast(mask, dtype<span style=color:#f92672>=</span>loss_<span style=color:#f92672>.</span>dtype)
    loss_ <span style=color:#f92672>*=</span> mask

    <span style=color:#66d9ef>return</span> tf<span style=color:#f92672>.</span>reduce_mean(loss_)
</code></pre></div><h3 id=模型训练>模型训练</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model<span style=color:#f92672>.</span>compile(loss<span style=color:#f92672>=</span>loss_function, metrics<span style=color:#f92672>=</span>[tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>metrics<span style=color:#f92672>.</span>SparseCategoricalAccuracy(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;train_accuracy&#39;</span>)], optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;adam&#39;</span>)
history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(x_mini_train_dataset, y_mini_train_dataset, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, validation_data<span style=color:#f92672>=</span>(x_valid_dataset, y_valid_dataset))
</code></pre></div><pre><code>16/16 [==============================] - 66s 4s/step - loss: 1.6746 - train_accuracy: 0.2262 - val_loss: 2.7021 - val_train_accuracy: 0.2201
</code></pre><h3 id=查看效果>查看效果</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>probs_to_word</span>(weights):
    <span style=color:#e6db74>&#34;&#34;&#34;probs to word&#34;&#34;&#34;</span>
    prefixSum <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>cumsum(weights) <span style=color:#75715e>#prefix sum</span>
    ratio <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>)
    index <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>searchsorted(prefixSum, ratio <span style=color:#f92672>*</span> prefixSum[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]) <span style=color:#75715e># large margin has high possibility to be sampled</span>
    <span style=color:#66d9ef>return</span> words[index[<span style=color:#ae81ff>0</span>]]

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>random_sample</span>(probs_to_word_fun):
    chars <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>] <span style=color:#75715e># [BOS]</span>
    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>100</span>):
        ret <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>call(tf<span style=color:#f92672>.</span>constant([chars], dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>int32))
        <span style=color:#75715e># print(&#39;ret:&#39;, ret.shape)</span>
        choice_word <span style=color:#f92672>=</span> probs_to_word_fun(ret<span style=color:#f92672>.</span>numpy()[<span style=color:#ae81ff>0</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])
        <span style=color:#66d9ef>if</span> choice_word <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;[EOS]&#39;</span>:
            <span style=color:#66d9ef>break</span>
        chars<span style=color:#f92672>.</span>append(word_dict[choice_word])
    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join([words[tmp] <span style=color:#66d9ef>for</span> tmp <span style=color:#f92672>in</span> chars])

<span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
    poetry <span style=color:#f92672>=</span> random_sample(probs_to_word)
    <span style=color:#66d9ef>print</span>(poetry)
</code></pre></div><pre><code>[BOS]去便天峰来，百忧忘道士。争来蜀水平，独与鱼台守。雨景能相和，茵间茶不歇。
[BOS]金凤曲兮南岩嚬，彤纱碧袅幽思。深妆绿蘋含浅碧，妍姿激暎流水中。花剡馆静吟满云，台边帆挂无得闲。疏谷似云远杀人，梁禽肠断长不还。
[BOS]桃花江北平，相望未能情。咂酒穿花晚，看书归马多。寒山倚谿寺，夕水落南陂。唯有南迁客，相游高隐难。
[BOS]微雨寺门安，覆井千峰绿。僧语寄佳空，西风见人怪。
[BOS]竹坞晚凉霁，药庭头李鸣。秋花入山尽，闻道应相寻。
[BOS]秦甸数千里，风流河转流。人间不能用，山上保风流。已对秦钱酒，唯擎楚客舟。双舟无处去，但见独留留。
[BOS]昨夜风微吹落梅，新声万骑报明台。此中欲折长杨柳，到处将文更战来。
[BOS]绿杨高万在，东照复全归。太守看山远，家家觉日稀。学泉封岁醉，过水著霄稀。欲问初归计，朝朝暮复归。
[BOS]蓬蒿春日长，殷勤雨露声。未央仙府好，有弄空山荣。坐见白昼日，闲因白云城。年年见官意，洒送出林名。
[BOS]故树春堂欲画阑，预辞新熟过梨花。金毛晴烧青衫近，青绿阴园白露寒。
</code></pre></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ rel=tag>深度学习</a></li><li class=tags__item><a class="tags__link btn" href=/tags/tensorflow/ rel=tag>Tensorflow</a></li></ul></div></footer></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/deep-learning/2020-07-16-%E9%80%9A%E8%BF%87tensorflow2%E4%BD%BF%E7%94%A8bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>通过Tensorflow2使用Bert预训练模型的两种方式</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/deep-learning/2020-07-29-batch_normalization%E4%B8%8Elayer_normalization%E7%9A%84%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>Batch Normalization与Layer Normalization的理解整理</p></a></div></nav><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c',repo:'writinglite.github.io',owner:'writinglite',admin:['writinglite'],id:'aaaaa',distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>