<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络 - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络"><meta property="og:description" content="大家好，今天是Tensorflow实战系列的第一次分享，本次分享共有3个主题：
 Tensorflow介绍 Tensorflow核心概念 使用Tensorflow实现手写数字识别任务  由于本次分享内容以实战为主不会涉及过多理论的讲解，但是分享过程中涉及到的理论知识也会做一个快速回顾。
Tensorflow Introduction Tensorflow 是由Google研发的开源软件库，它既是一个实现机器学习算法的接口，同时也是执行机器学习算法的框架，它对深度学习中常用的神经网络结构等算法进行了封装，因此开发人员可以快速的进行模型搭建。
1、Tensorflow 发展史  2011 年，Google Brain内部孵化出一个项目叫做DistBelief, 它是为深度神经网络构建的一个机器学习系统，是Tensorflow的前身。 2015年11月，Google正式发布了Tensorflow的白皮书并开源TensorFlow 0.1 版本。 2017年02月，Tensorflow正式发布了1.0.0版本，同时也标志着稳定版的诞生。 2019年10月，TensorFlow在经历七个多月(2019年3月1日-2019年10月1日)的2.0 Alpha 版本的更新迭代后发布 2.0 正式版。  通过上面的发展史我们可以看到，虽然经过了9年时间Tensorflow依然是目前最流行的深度学习框架之一。
2、Tensorflow VS Pytorch 上面说到Tensorflow是目前最流行的深度学习框架之一，那另一款可以和Tensorflow一较高下的深度学习框架就是-Pytorch了。Pytorch是由Facebook研发的一款开源的机器学习库，自16年发布以来发展非常迅猛。Tensorflow和Pytorch如何选择呢，我的看法是：都可以，虽然刚开始时Pytorch和Tensorflow还是差别较大的，比较Pytorch有动态图、类python的编程方式，Tensorflow则支持可视化，生产部署更加简单易用，但通过这几年的发展Pytorch和Tensorfow越来越像了，Tensorflow添加了动态图，而Pytorch也在工业部署上有了很大改善。因此在两都的选择上不必太过纠结。
import tensorflow as tf import matplotlib as mpl %matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd import os import sys print(sys.version) 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] for module in tf, np, mpl: print(module."><meta property="og:type" content="article"><meta property="og:url" content="/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%981-tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-06-24T20:43:00+00:00"><meta property="article:modified_time" content="2020-06-24T20:43:00+00:00"><meta itemprop=name content="Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络"><meta itemprop=description content="大家好，今天是Tensorflow实战系列的第一次分享，本次分享共有3个主题：
 Tensorflow介绍 Tensorflow核心概念 使用Tensorflow实现手写数字识别任务  由于本次分享内容以实战为主不会涉及过多理论的讲解，但是分享过程中涉及到的理论知识也会做一个快速回顾。
Tensorflow Introduction Tensorflow 是由Google研发的开源软件库，它既是一个实现机器学习算法的接口，同时也是执行机器学习算法的框架，它对深度学习中常用的神经网络结构等算法进行了封装，因此开发人员可以快速的进行模型搭建。
1、Tensorflow 发展史  2011 年，Google Brain内部孵化出一个项目叫做DistBelief, 它是为深度神经网络构建的一个机器学习系统，是Tensorflow的前身。 2015年11月，Google正式发布了Tensorflow的白皮书并开源TensorFlow 0.1 版本。 2017年02月，Tensorflow正式发布了1.0.0版本，同时也标志着稳定版的诞生。 2019年10月，TensorFlow在经历七个多月(2019年3月1日-2019年10月1日)的2.0 Alpha 版本的更新迭代后发布 2.0 正式版。  通过上面的发展史我们可以看到，虽然经过了9年时间Tensorflow依然是目前最流行的深度学习框架之一。
2、Tensorflow VS Pytorch 上面说到Tensorflow是目前最流行的深度学习框架之一，那另一款可以和Tensorflow一较高下的深度学习框架就是-Pytorch了。Pytorch是由Facebook研发的一款开源的机器学习库，自16年发布以来发展非常迅猛。Tensorflow和Pytorch如何选择呢，我的看法是：都可以，虽然刚开始时Pytorch和Tensorflow还是差别较大的，比较Pytorch有动态图、类python的编程方式，Tensorflow则支持可视化，生产部署更加简单易用，但通过这几年的发展Pytorch和Tensorfow越来越像了，Tensorflow添加了动态图，而Pytorch也在工业部署上有了很大改善。因此在两都的选择上不必太过纠结。
import tensorflow as tf import matplotlib as mpl %matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd import os import sys print(sys.version) 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] for module in tf, np, mpl: print(module."><meta itemprop=datePublished content="2020-06-24T20:43:00+00:00"><meta itemprop=dateModified content="2020-06-24T20:43:00+00:00"><meta itemprop=wordCount content="2191"><meta itemprop=keywords content="深度学习,Tensorflow,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络"><meta name=twitter:description content="大家好，今天是Tensorflow实战系列的第一次分享，本次分享共有3个主题：
 Tensorflow介绍 Tensorflow核心概念 使用Tensorflow实现手写数字识别任务  由于本次分享内容以实战为主不会涉及过多理论的讲解，但是分享过程中涉及到的理论知识也会做一个快速回顾。
Tensorflow Introduction Tensorflow 是由Google研发的开源软件库，它既是一个实现机器学习算法的接口，同时也是执行机器学习算法的框架，它对深度学习中常用的神经网络结构等算法进行了封装，因此开发人员可以快速的进行模型搭建。
1、Tensorflow 发展史  2011 年，Google Brain内部孵化出一个项目叫做DistBelief, 它是为深度神经网络构建的一个机器学习系统，是Tensorflow的前身。 2015年11月，Google正式发布了Tensorflow的白皮书并开源TensorFlow 0.1 版本。 2017年02月，Tensorflow正式发布了1.0.0版本，同时也标志着稳定版的诞生。 2019年10月，TensorFlow在经历七个多月(2019年3月1日-2019年10月1日)的2.0 Alpha 版本的更新迭代后发布 2.0 正式版。  通过上面的发展史我们可以看到，虽然经过了9年时间Tensorflow依然是目前最流行的深度学习框架之一。
2、Tensorflow VS Pytorch 上面说到Tensorflow是目前最流行的深度学习框架之一，那另一款可以和Tensorflow一较高下的深度学习框架就是-Pytorch了。Pytorch是由Facebook研发的一款开源的机器学习库，自16年发布以来发展非常迅猛。Tensorflow和Pytorch如何选择呢，我的看法是：都可以，虽然刚开始时Pytorch和Tensorflow还是差别较大的，比较Pytorch有动态图、类python的编程方式，Tensorflow则支持可视化，生产部署更加简单易用，但通过这几年的发展Pytorch和Tensorfow越来越像了，Tensorflow添加了动态图，而Pytorch也在工业部署上有了很大改善。因此在两都的选择上不必太过纠结。
import tensorflow as tf import matplotlib as mpl %matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd import os import sys print(sys.version) 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] for module in tf, np, mpl: print(module."><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-06-24T20:43:00Z>2020-06-24</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/tensorflow2.x-keras.html rel=category>Tensorflow2.x keras</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#tensorflow-introduction>Tensorflow Introduction</a><ul><li><a href=#1tensorflow-发展史>1、Tensorflow 发展史</a></li><li><a href=#2tensorflow-vs-pytorch>2、Tensorflow VS Pytorch</a></li></ul></li><li><a href=#tensorflow-核心概念>Tensorflow 核心概念</a><ul><li><a href=#1tensor-张量>1、Tensor (张量)</a></li><li><a href=#2compute-grapth-计算图-与autograd自动求导>2、Compute Grapth (计算图) 与Autograd(自动求导)</a></li></ul></li><li><a href=#模型和数据说明>模型和数据说明</a><ul><li><a href=#数据说明>数据说明</a></li><li><a href=#全连接神经网络>全连接神经网络</a></li></ul></li><li><a href=#tensorlfow2搭建一个全连接神经网络>Tensorlfow2搭建一个全连接神经网络</a><ul><li><a href=#0why-tensorflow2x>0、Why Tensorflow2.X</a></li><li><a href=#1定义模型结构>1、定义模型结构</a></li><li><a href=#2loss-function-损失函数>2、Loss Function (损失函数)</a></li><li><a href=#3optimaizer-优化方法>3、Optimaizer (优化方法)</a></li><li><a href=#4metrics-评价方法>4、Metrics (评价方法)</a></li></ul></li><li><a href=#模型训练>模型训练</a></li><li><a href=#模型保存>模型保存</a><ul><li><a href=#通过callbacks的方式进行保存>通过callbacks的方式进行保存</a></li><li><a href=#保存成savedmodel>保存成SavedModel</a></li><li><a href=#两种保存方式的使用场景>两种保存方式的使用场景</a></li></ul></li><li><a href=#模型加载>模型加载</a></li><li><a href=#完整的图片分类任务实例>完整的图片分类任务实例</a></li><li><a href=#tensorflow-keras---custom>Tensorflow Keras - Custom</a></li></ul></nav></div></div><div class="content post__content clearfix"><p>大家好，今天是Tensorflow实战系列的第一次分享，本次分享共有3个主题：</p><ul><li>Tensorflow介绍</li><li>Tensorflow核心概念</li><li>使用Tensorflow实现手写数字识别任务</li></ul><p>由于本次分享内容以实战为主不会涉及过多理论的讲解，但是分享过程中涉及到的理论知识也会做一个快速回顾。</p><h2 id=tensorflow-introduction>Tensorflow Introduction</h2><p>Tensorflow 是由Google研发的开源软件库，它既是一个实现机器学习算法的接口，同时也是执行机器学习算法的框架，它对深度学习中常用的神经网络结构等算法进行了封装，因此开发人员可以快速的进行模型搭建。</p><h3 id=1tensorflow-发展史>1、Tensorflow 发展史</h3><ul><li>2011 年，Google Brain内部孵化出一个项目叫做DistBelief, 它是为深度神经网络构建的一个机器学习系统，是Tensorflow的前身。</li><li>2015年11月，Google正式发布了Tensorflow的白皮书并开源TensorFlow 0.1 版本。</li><li>2017年02月，Tensorflow正式发布了1.0.0版本，同时也标志着稳定版的诞生。</li><li>2019年10月，TensorFlow在经历七个多月(2019年3月1日-2019年10月1日)的2.0 Alpha 版本的更新迭代后发布 2.0 正式版。</li></ul><p>通过上面的发展史我们可以看到，虽然经过了9年时间Tensorflow依然是目前最流行的深度学习框架之一。</p><p><img src=./_image/framework.png alt></p><h3 id=2tensorflow-vs-pytorch>2、Tensorflow VS Pytorch</h3><p>上面说到Tensorflow是目前最流行的深度学习框架之一，那另一款可以和Tensorflow一较高下的深度学习框架就是-Pytorch了。Pytorch是由Facebook研发的一款开源的机器学习库，自16年发布以来发展非常迅猛。Tensorflow和Pytorch如何选择呢，我的看法是：都可以，虽然刚开始时Pytorch和Tensorflow还是差别较大的，比较Pytorch有动态图、类python的编程方式，Tensorflow则支持可视化，生产部署更加简单易用，但通过这几年的发展Pytorch和Tensorfow越来越像了，Tensorflow添加了动态图，而Pytorch也在工业部署上有了很大改善。因此在两都的选择上不必太过纠结。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> tensorflow <span style=color:#f92672>as</span> tf
<span style=color:#f92672>import</span> matplotlib <span style=color:#f92672>as</span> mpl
<span style=color:#f92672>%</span>matplotlib inline
<span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
<span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>import</span> pandas <span style=color:#f92672>as</span> pd
<span style=color:#f92672>import</span> os
<span style=color:#f92672>import</span> sys
<span style=color:#66d9ef>print</span>(sys<span style=color:#f92672>.</span>version)
</code></pre></div><pre><code>3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>for</span> module <span style=color:#f92672>in</span> tf, np, mpl:
    <span style=color:#66d9ef>print</span>(module<span style=color:#f92672>.</span>__name__, module<span style=color:#f92672>.</span>__version__)
</code></pre></div><pre><code>tensorflow 2.1.0
numpy 1.18.1
matplotlib 3.2.1
</code></pre><h2 id=tensorflow-核心概念>Tensorflow 核心概念</h2><h3 id=1tensor-张量>1、Tensor (张量)</h3><p>在Tensorflow中计算的数据都是以Tensor的形式来表示的。Tensor 可以把它看成是一个多维数组，并且它和NumPy中的np.arrays也非常相似。它有3个重要的属性：</p><ul><li>value</li><li>shape</li><li>dtype</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>rank_3_tensor <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant([
  [[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>],
   [<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>]],
  [[<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>13</span>, <span style=color:#ae81ff>14</span>],
   [<span style=color:#ae81ff>15</span>, <span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>17</span>, <span style=color:#ae81ff>18</span>, <span style=color:#ae81ff>19</span>]],
  [[<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>21</span>, <span style=color:#ae81ff>22</span>, <span style=color:#ae81ff>23</span>, <span style=color:#ae81ff>24</span>],
   [<span style=color:#ae81ff>25</span>, <span style=color:#ae81ff>26</span>, <span style=color:#ae81ff>27</span>, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>29</span>]],])
                    
<span style=color:#66d9ef>print</span>(rank_3_tensor)
</code></pre></div><pre><code>tf.Tensor(
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]

 [[10 11 12 13 14]
  [15 16 17 18 19]]

 [[20 21 22 23 24]
  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)
</code></pre><p><img src=./_image/tensor.png alt=tensor></p><p>tensor的dypte和编程语言中的变量类型非常相似，只是因此模型计算过程中运算量非常大对精度和粒度的要求更高。</p><ul><li>tf.float16: 16-bit half-precision floating-point.</li><li>tf.float32: 32-bit single-precision floating-point.</li><li>tf.float64: 64-bit double-precision floating-point.</li><li>tf.int8: 8-bit signed integer.</li><li>tf.int16: 16-bit signed integer.</li><li>tf.int32: 32-bit signed integer.</li><li>tf.int64: 64-bit signed integer.</li><li>tf.bool: Boolean.</li><li>tf.string: String.</li></ul><p>更好可参考：https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/dtypes/DType</p><h3 id=2compute-grapth-计算图-与autograd自动求导>2、Compute Grapth (计算图) 与Autograd(自动求导)</h3><p>计算图是一种描述计算过程的语言。图的节点由事先定义的运算构成，图的各个节点之间由张量（tensor）来连接，Tensorflow的计算过程就是张量（tensor）在节点之间从前到后的流动传输过程。另外在图上计算变量的梯度也非常容易，在使用梯度下降求解模型参数时非常方便。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>a <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(<span style=color:#ae81ff>3.</span>)
b <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(<span style=color:#ae81ff>2.</span>)

<span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>GradientTape() <span style=color:#66d9ef>as</span> tape:
    c <span style=color:#f92672>=</span> a <span style=color:#f92672>+</span> b
    d <span style=color:#f92672>=</span> b <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
    e <span style=color:#f92672>=</span> c <span style=color:#f92672>*</span> d

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;e :&#39;</span>, e<span style=color:#f92672>.</span>numpy())

a_gradient, b_gradient <span style=color:#f92672>=</span> tape<span style=color:#f92672>.</span>gradient(e, [a, b])
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;b_gradient : &#39;</span>, b_gradient<span style=color:#f92672>.</span>numpy())
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;a_gradient : &#39;</span>, a_gradient<span style=color:#f92672>.</span>numpy())
</code></pre></div><pre><code>e : 15.0
b_gradient :  8.0
a_gradient :  3.0
</code></pre><p><img src=./_image/grapth_compute.png alt=计算图></p><p>以求e关于变量b的偏导数为例，从e到b的路径有两条，将每条路径中的值相乘再将种路径的值相加即可得到最终的偏导数结果。</p><p>即使不了解计算图等概念也不会影响我们使用Tensorflow来完成各式各样任务，但是它可以帮忙我们了解Tensorflow的运作机制，在遇到问题时可以更合理的问题问题原因。</p><h2 id=模型和数据说明>模型和数据说明</h2><p>在讲解后面的内容之前，先对之后会使用的模型和数据做一个回顾和说明。后面的例子中会搭建一个全连接的神经网络来实现对手写数字识别任务。</p><h3 id=数据说明>数据说明</h3><p>本次分享我们主要使用的数据是 MNIST， MNIST Dataset 是一个手写数字数据集，其包含 60,000 个示例训练集和 10,000 个示例测试集，它主要用于机器视觉领域的图像分类，每个样本是28*28的灰度图片，共0-9 10个类别。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>mnist <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>mnist
(x_train_all, y_train_all), (x_test, y_test) <span style=color:#f92672>=</span> mnist<span style=color:#f92672>.</span>load_data()

<span style=color:#75715e># 如果第一层layer的activation是relu的话，这里要做规一化</span>
x_train_all, x_test <span style=color:#f92672>=</span> x_train_all<span style=color:#f92672>/</span><span style=color:#ae81ff>255.0</span>, x_test<span style=color:#f92672>/</span><span style=color:#ae81ff>255.0</span>

<span style=color:#75715e># 在训练集中取前5000做为验证集</span>
x_valid, x_train <span style=color:#f92672>=</span> x_train_all[:<span style=color:#ae81ff>5000</span>], x_train_all[<span style=color:#ae81ff>5000</span>:]
y_valid, y_train <span style=color:#f92672>=</span> y_train_all[:<span style=color:#ae81ff>5000</span>], y_train_all[<span style=color:#ae81ff>5000</span>:]
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>print</span>(x_valid<span style=color:#f92672>.</span>shape, y_valid<span style=color:#f92672>.</span>shape)
<span style=color:#66d9ef>print</span>(x_train<span style=color:#f92672>.</span>shape, y_train<span style=color:#f92672>.</span>shape)
<span style=color:#66d9ef>print</span>(x_test<span style=color:#f92672>.</span>shape, y_test<span style=color:#f92672>.</span>shape)
</code></pre></div><pre><code>(5000, 28, 28) (5000,)
(55000, 28, 28) (55000,)
(10000, 28, 28) (10000,)
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plotImages</span>(images):
    fig, axes <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>1</span>, len(images), figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>5</span>))
    axes <span style=color:#f92672>=</span> axes<span style=color:#f92672>.</span>flatten()
    <span style=color:#66d9ef>for</span> img, ax <span style=color:#f92672>in</span> zip(images, axes):
        ax<span style=color:#f92672>.</span>imshow(img, interpolation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;nearest&#34;</span>, cmap<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;binary&#34;</span>)
        ax<span style=color:#f92672>.</span>axis(<span style=color:#e6db74>&#39;off&#39;</span>)
    plt<span style=color:#f92672>.</span>tight_layout()
    plt<span style=color:#f92672>.</span>show()

plotImages(x_train[<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>5</span>])
</code></pre></div><p><img src=./_image/output_17_0.png alt=png></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;label ：&#39;</span>, y_train[<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>5</span>])
</code></pre></div><pre><code>label ： [7 3 4 6 1]
</code></pre><h3 id=全连接神经网络>全连接神经网络</h3><p>全连接神经网络(fully connected neural network)，顾名思义，就是相邻两层之间任意两个节点之间都有连接。全连接神经网络是最为普通的一种模型（比如和CNN相比），由于是全连接，所以会有更多的权重值和连接，因此也意味着占用更多的内存和计算。</p><p><img src=./_image/fully_connect_model.png alt=全连接神经网络模型></p><p>全连接层的计算方式：
<img src=./_image/fully_connect_nn.png alt=全连接层></p><p>推荐演示工具：https://playground.tensorflow.org/</p><h2 id=tensorlfow2搭建一个全连接神经网络>Tensorlfow2搭建一个全连接神经网络</h2><h3 id=0why-tensorflow2x>0、Why Tensorflow2.X</h3><pre><code>Tensorflow1.X 版本的编程方式更像是在画一张计算图，通常构建一个模型的时候是先定义一张图，然后在图中添加计算结点，最终将这个张图拿去做计算。而2.X将计算图的构建过程隐藏在底层中了，这使得它的语法看起来更加友好也比较符合正常的编程思维。
</code></pre><p><img src=./_image/tensorflow2.png alt=tensorflow_keras></p><p>在TF2版本中，有两种高级API，分别是Estimator和tf.keras，Estimator早在TF1版本就已经出现而tf.keras是TF2中新增加的。后面的内容主要会以tf.keras为主。</p><h3 id=1定义模型结构>1、定义模型结构</h3><p><strong>Sequential API (连续)</strong>：Sequential API 通过model.add方法添加神经网络层，适合链式结构的神经网络。</p><p><strong>Functional API (函数式)</strong>：Functional API 通过指定inputs和outputs，将inputs到outputs的中间计算过程做为模型计算逻辑。它可以让计算逻辑更加灵活不局限于链式这样的简单结构。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_sequential_model</span>():
    model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Sequential(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sequential_model&#39;</span>)
    model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Flatten(input_shape<span style=color:#f92672>=</span>[<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>], name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;flatten&#39;</span>))
    model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>300</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;h1&#39;</span>))
    model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>200</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;h2&#39;</span>)) 
    model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>100</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;h3&#39;</span>))
    model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>10</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;softmax&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;outputs&#39;</span>))
    <span style=color:#66d9ef>return</span> model

sequential_model <span style=color:#f92672>=</span> generate_sequential_model()
sequential_model<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;sequential_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
h1 (Dense)                   (None, 300)               235500    
_________________________________________________________________
h2 (Dense)                   (None, 200)               60200     
_________________________________________________________________
h3 (Dense)                   (None, 100)               20100     
_________________________________________________________________
outputs (Dense)              (None, 10)                1010      
=================================================================
Total params: 316,810
Trainable params: 316,810
Non-trainable params: 0
_________________________________________________________________
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_functional_model</span>():
    inputs <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Input(shape<span style=color:#f92672>=</span>[<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>], name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;inputs&#39;</span>)
    flatten <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Flatten(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;flatten&#39;</span>)(inputs)
    h1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>300</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;h1&#39;</span>)(flatten)
    h2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>200</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;h2&#39;</span>)(h1)
    h3 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>100</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;h3&#39;</span>)(h2)
    outputs <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>10</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;softmax&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;outputs&#39;</span>)(h3)
    model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Model(inputs<span style=color:#f92672>=</span>inputs, outputs<span style=color:#f92672>=</span>outputs, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;functional_model&#34;</span>)
    <span style=color:#66d9ef>return</span> model
functioal_model <span style=color:#f92672>=</span> generate_functional_model()
functioal_model<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;functional_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
inputs (InputLayer)          [(None, 28, 28)]          0         
_________________________________________________________________
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
h1 (Dense)                   (None, 300)               235500    
_________________________________________________________________
h2 (Dense)                   (None, 200)               60200     
_________________________________________________________________
h3 (Dense)                   (None, 100)               20100     
_________________________________________________________________
outputs (Dense)              (None, 10)                1010      
=================================================================
Total params: 316,810
Trainable params: 316,810
Non-trainable params: 0
_________________________________________________________________
</code></pre><h3 id=2loss-function-损失函数>2、Loss Function (损失函数)</h3><p>交叉熵损失：tf.keras.losses.SparseCategoricalCrossentropy，常用的适用于分类任务的Loss。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>cce <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>losses<span style=color:#f92672>.</span>SparseCategoricalCrossentropy()
loss <span style=color:#f92672>=</span> cce(
  tf<span style=color:#f92672>.</span>convert_to_tensor([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>]),
  tf<span style=color:#f92672>.</span>convert_to_tensor([[<span style=color:#f92672>.</span><span style=color:#ae81ff>9</span>, <span style=color:#f92672>.</span><span style=color:#ae81ff>05</span>, <span style=color:#f92672>.</span><span style=color:#ae81ff>05</span>], [<span style=color:#f92672>.</span><span style=color:#ae81ff>5</span>, <span style=color:#f92672>.</span><span style=color:#ae81ff>89</span>, <span style=color:#f92672>.</span><span style=color:#ae81ff>6</span>], [<span style=color:#f92672>.</span><span style=color:#ae81ff>05</span>, <span style=color:#f92672>.</span><span style=color:#ae81ff>01</span>, <span style=color:#f92672>.</span><span style=color:#ae81ff>94</span>]]))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;Loss: &#39;</span>, loss<span style=color:#f92672>.</span>numpy())  <span style=color:#75715e># Loss: 0.3239</span>
</code></pre></div><pre><code>Loss:  0.32396814
</code></pre><h3 id=3optimaizer-优化方法>3、Optimaizer (优化方法)</h3><pre><code>在确定了损失函数之后，我们就可以选择一个优化算法来让损失函数最小化。以SGD举例：
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>sgd_optimizer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>optimizers<span style=color:#f92672>.</span>SGD(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)

a <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(<span style=color:#ae81ff>3.</span>)
b <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>Variable(<span style=color:#ae81ff>2.</span>)

<span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>GradientTape() <span style=color:#66d9ef>as</span> tape:
    c <span style=color:#f92672>=</span> a <span style=color:#f92672>+</span> b
    d <span style=color:#f92672>=</span> b <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
    e <span style=color:#f92672>=</span> c <span style=color:#f92672>*</span> d

a_gradient, b_gradient <span style=color:#f92672>=</span> tape<span style=color:#f92672>.</span>gradient(e, [a, b])
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;b_gradient : &#39;</span>, b_gradient<span style=color:#f92672>.</span>numpy())
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;a_gradient : &#39;</span>, a_gradient<span style=color:#f92672>.</span>numpy())

sgd_optimizer<span style=color:#f92672>.</span>apply_gradients([(a_gradient, a), (b_gradient, b)])
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;a update: &#39;</span>, a<span style=color:#f92672>.</span>numpy())
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;b update: &#39;</span>, b<span style=color:#f92672>.</span>numpy())
</code></pre></div><pre><code>b_gradient :  8.0
a_gradient :  3.0
a update:  2.97
b update:  1.92
</code></pre><h3 id=4metrics-评价方法>4、Metrics (评价方法)</h3><p>评价函数和<strong>损失函数</strong>相似，只不过评价函数的结果不会用于训练过程中。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>sca <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>metrics<span style=color:#f92672>.</span>SparseCategoricalAccuracy() 
_ <span style=color:#f92672>=</span> sca<span style=color:#f92672>.</span>update_state([<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>], [[<span style=color:#ae81ff>0.02</span>, <span style=color:#ae81ff>0.9</span>, <span style=color:#ae81ff>0.08</span>], [<span style=color:#ae81ff>0.05</span>, <span style=color:#ae81ff>0.95</span>, <span style=color:#ae81ff>0</span>]]) 
<span style=color:#66d9ef>print</span>(sca<span style=color:#f92672>.</span>result()<span style=color:#f92672>.</span>numpy() )
</code></pre></div><pre><code>0.5
</code></pre><h2 id=模型训练>模型训练</h2><p>上面提到的模型结构、损失函数、优化方法是构成模型必不可少的3个要素。在集齐这些要素之后就要可以拿数据训练模型了。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model <span style=color:#f92672>=</span> functioal_model
model<span style=color:#f92672>.</span>compile(loss<span style=color:#f92672>=</span>cce, optimizer<span style=color:#f92672>=</span>sgd_optimizer, metrics<span style=color:#f92672>=</span>[sca])
history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(x_train, y_train, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, validation_data<span style=color:#f92672>=</span>(x_valid, y_valid))
</code></pre></div><pre><code>Train on 55000 samples, validate on 5000 samples
Epoch 1/2
55000/55000 [==============================] - 5s 94us/sample - loss: 0.6173 - sparse_categorical_accuracy: 0.8341 - val_loss: 0.2859 - val_sparse_categorical_accuracy: 0.9198
Epoch 2/2
55000/55000 [==============================] - 5s 87us/sample - loss: 0.2660 - sparse_categorical_accuracy: 0.9233 - val_loss: 0.2109 - val_sparse_categorical_accuracy: 0.9404
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 在测试集上的效果</span>
model<span style=color:#f92672>.</span>evaluate(x_test, y_test)
</code></pre></div><pre><code>10000/10000 [==============================] - 0s 50us/sample - loss: 0.2149 - sparse_categorical_accuracy: 0.9377





[0.2148841947108507, 0.9377]
</code></pre><h2 id=模型保存>模型保存</h2><h3 id=通过callbacks的方式进行保存>通过callbacks的方式进行保存</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>output_model_file <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;./mnist_model_callback.h5&#34;</span>
callbacks <span style=color:#f92672>=</span> [
    tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>callbacks<span style=color:#f92672>.</span>ModelCheckpoint(output_model_file, save_best_only<span style=color:#f92672>=</span>True, save_weights_only<span style=color:#f92672>=</span>False)
]
history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(x_train, y_train, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, validation_data<span style=color:#f92672>=</span>(x_valid, y_valid), callbacks<span style=color:#f92672>=</span>callbacks)
</code></pre></div><pre><code>Train on 55000 samples, validate on 5000 samples
Epoch 1/2
55000/55000 [==============================] - 5s 90us/sample - loss: 0.2079 - sparse_categorical_accuracy: 0.9392 - val_loss: 0.1790 - val_sparse_categorical_accuracy: 0.9520
Epoch 2/2
55000/55000 [==============================] - 5s 89us/sample - loss: 0.1709 - sparse_categorical_accuracy: 0.9506 - val_loss: 0.1563 - val_sparse_categorical_accuracy: 0.9576
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;训练后的模型参数：&#39;</span>, model<span style=color:#f92672>.</span>variables[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>numpy()[<span style=color:#ae81ff>0</span>, :<span style=color:#ae81ff>5</span>])
</code></pre></div><pre><code>训练后的模型参数： [ 0.01215426 -0.05889301  0.04321195  0.05594952 -0.00950153]
</code></pre><h3 id=保存成savedmodel>保存成SavedModel</h3><p>在使用TensorFlow Serving时，会用到这种格式的模型，模型目录结构如下所示：</p><ul><li>assets是一个可选目录，用于存放预测时的辅助文档信息；</li><li>variables保存的变量信息；</li><li>saved_model.pb或saved_model.pbtxt存放MetaGraphDef，存储训练预测模型的程序逻辑</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>saved_model_file <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;mnist_model_saved_model&#34;</span>
model<span style=color:#f92672>.</span>save(saved_model_file)
</code></pre></div><pre><code>WARNING:tensorflow:From /home/hwyang/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
INFO:tensorflow:Assets written to: mnist_model_saved_model/assets
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 模型保存的输入输入格式</span>
<span style=color:#960050;background-color:#1e0010>!</span>saved_model_cli show <span style=color:#f92672>--</span>dir <span style=color:#f92672>./</span>mnist_model_saved_model <span style=color:#f92672>--</span>tag_set serve <span style=color:#f92672>--</span>signature_def serving_default
</code></pre></div><pre><code>2020-06-24 11:13:37.498139: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-06-24 11:13:37.498235: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-06-24 11:13:37.498296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
The given SavedModel SignatureDef contains the following input(s):
  inputs['inputs'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 28, 28)
      name: serving_default_inputs:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['outputs'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 10)
      name: StatefulPartitionedCall:0
Method name is: tensorflow/serving/predict
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 调用模型</span>
<span style=color:#960050;background-color:#1e0010>!</span>saved_model_cli run <span style=color:#f92672>--</span>dir <span style=color:#f92672>./</span>mnist_model_saved_model\
    <span style=color:#f92672>--</span>tag_set serve <span style=color:#f92672>--</span>signature_def serving_default \
    <span style=color:#f92672>--</span>input_exprs <span style=color:#e6db74>&#39;inputs=np.ones((2, 28, 28))&#39;</span>
</code></pre></div><pre><code>2020-06-24 11:13:39.641330: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-06-24 11:13:39.641450: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-06-24 11:13:39.641460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-06-24 11:13:40.425721: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-06-24 11:13:40.425767: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2020-06-24 11:13:40.425785: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (work-computer): /proc/driver/nvidia/version does not exist
2020-06-24 11:13:40.426013: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-06-24 11:13:40.433525: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1799995000 Hz
2020-06-24 11:13:40.435146: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x402f9a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-06-24 11:13:40.435196: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /home/hwyang/.local/lib/python3.6/site-packages/tensorflow_core/python/tools/saved_model_cli.py:420: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
Result for output key outputs:
[[6.3519548e-07 4.2883544e-11 3.6132155e-04 3.7582448e-01 7.1212423e-15
  1.1317922e-03 1.6414268e-12 1.1638527e-12 6.2268174e-01 2.5810540e-10]
 [6.3519548e-07 4.2883544e-11 3.6132155e-04 3.7582448e-01 7.1212423e-15
  1.1317922e-03 1.6414268e-12 1.1638527e-12 6.2268174e-01 2.5810540e-10]]
</code></pre><h3 id=两种保存方式的使用场景>两种保存方式的使用场景</h3><p>1、在需要间断的训练模型时使用callbacks的方式更加合适，例如模型运行中被意外中断了就可以通过callbacks过程中保存的模型快速恢复之前的训练状态。</p><p>2、而当我们要部署模型时使用SavedModel可以将模型快速的部署tensorflow serving中。</p><h2 id=模型加载>模型加载</h2><p>1、只加载参数</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model_only_weiths <span style=color:#f92672>=</span> generate_functional_model()
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;加载前的参数值：&#39;</span>, model_only_weiths<span style=color:#f92672>.</span>variables[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>numpy()[<span style=color:#ae81ff>0</span>, :<span style=color:#ae81ff>5</span>])
model_only_weiths<span style=color:#f92672>.</span>load_weights(output_model_file)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;加载后的参数值：&#39;</span>, model_only_weiths<span style=color:#f92672>.</span>variables[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>numpy()[<span style=color:#ae81ff>0</span>, :<span style=color:#ae81ff>5</span>])
</code></pre></div><pre><code>加载前的参数值： [-0.0438241   0.07230127 -0.00713674 -0.06280634  0.01845549]
加载后的参数值： [ 0.01215426 -0.05889301  0.04321195  0.05594952 -0.00950153]
</code></pre><p>2、加载模型结构和参数</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>model_weithts_and_model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>load_model(output_model_file)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;加载后的参数值：&#39;</span>, model_weithts_and_model<span style=color:#f92672>.</span>variables[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>numpy()[<span style=color:#ae81ff>0</span>, :<span style=color:#ae81ff>5</span>])
</code></pre></div><pre><code>加载后的参数值： [ 0.01215426 -0.05889301  0.04321195  0.05594952 -0.00950153]
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 打印模型结构</span>
model_weithts_and_model<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;functional_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
inputs (InputLayer)          [(None, 28, 28)]          0         
_________________________________________________________________
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
h1 (Dense)                   (None, 300)               235500    
_________________________________________________________________
h2 (Dense)                   (None, 200)               60200     
_________________________________________________________________
h3 (Dense)                   (None, 100)               20100     
_________________________________________________________________
outputs (Dense)              (None, 10)                1010      
=================================================================
Total params: 316,810
Trainable params: 316,810
Non-trainable params: 0
_________________________________________________________________
</code></pre><h2 id=完整的图片分类任务实例>完整的图片分类任务实例</h2><p>1、加载数据</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># tf.keras.datasets.fashion_mnist</span>
mnist <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>mnist
(x_train_all, y_train_all), (x_test, y_test) <span style=color:#f92672>=</span> mnist<span style=color:#f92672>.</span>load_data()
<span style=color:#66d9ef>print</span>(type(x_train_all), type(y_train_all))
<span style=color:#75715e># 如果第一层layer的activation是relu的话，这里要做规一化</span>
x_train_all, x_test <span style=color:#f92672>=</span> x_train_all<span style=color:#f92672>/</span><span style=color:#ae81ff>255.0</span>, x_test<span style=color:#f92672>/</span><span style=color:#ae81ff>255.0</span>

x_valid, x_train <span style=color:#f92672>=</span> x_train_all[:<span style=color:#ae81ff>5000</span>], x_train_all[<span style=color:#ae81ff>5000</span>:]
y_valid, y_train <span style=color:#f92672>=</span> y_train_all[:<span style=color:#ae81ff>5000</span>], y_train_all[<span style=color:#ae81ff>5000</span>:]
</code></pre></div><pre><code>&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;
</code></pre><p>2、定义模型，forward时的计算（神经网络结构）</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>mnist_model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Sequential()
mnist_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Flatten(input_shape<span style=color:#f92672>=</span>[<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>]))
<span style=color:#75715e># 下面的activation 如果是relu的话，那x一定要做规一化即除上255，如果是sigmoid的话就不会有问题</span>
mnist_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>300</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>)) 
mnist_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>200</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>))
mnist_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>100</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>))
mnist_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>10</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;softmax&#34;</span>))
</code></pre></div><p>3、定义损失函数，并选择优化器</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>mnist_model<span style=color:#f92672>.</span>compile(loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sparse_categorical_crossentropy&#34;</span>, 
             optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sgd&#34;</span>, 
             metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;accuracy&#34;</span>])
</code></pre></div><p>4、迭代的对数据进行模型训练</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>history <span style=color:#f92672>=</span> mnist_model<span style=color:#f92672>.</span>fit(x_train, y_train, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, validation_data<span style=color:#f92672>=</span>(x_valid, y_valid))
</code></pre></div><pre><code>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 5s 93us/sample - loss: 0.6206 - accuracy: 0.8366 - val_loss: 0.2858 - val_accuracy: 0.9184
Epoch 2/10
55000/55000 [==============================] - 5s 97us/sample - loss: 0.2665 - accuracy: 0.9234 - val_loss: 0.2190 - val_accuracy: 0.9372
Epoch 3/10
55000/55000 [==============================] - 5s 93us/sample - loss: 0.2078 - accuracy: 0.9398 - val_loss: 0.1702 - val_accuracy: 0.9504
Epoch 4/10
55000/55000 [==============================] - 5s 93us/sample - loss: 0.1704 - accuracy: 0.9508 - val_loss: 0.1489 - val_accuracy: 0.9592
Epoch 5/10
55000/55000 [==============================] - 5s 97us/sample - loss: 0.1436 - accuracy: 0.9584 - val_loss: 0.1306 - val_accuracy: 0.9626
Epoch 6/10
55000/55000 [==============================] - 5s 97us/sample - loss: 0.1242 - accuracy: 0.9647 - val_loss: 0.1179 - val_accuracy: 0.9686
Epoch 7/10
55000/55000 [==============================] - 5s 87us/sample - loss: 0.1082 - accuracy: 0.9687 - val_loss: 0.1086 - val_accuracy: 0.9696
Epoch 8/10
55000/55000 [==============================] - 5s 90us/sample - loss: 0.0958 - accuracy: 0.9727 - val_loss: 0.1006 - val_accuracy: 0.9708
Epoch 9/10
55000/55000 [==============================] - 5s 99us/sample - loss: 0.0860 - accuracy: 0.9751 - val_loss: 0.0964 - val_accuracy: 0.9730
Epoch 10/10
55000/55000 [==============================] - 5s 96us/sample - loss: 0.0768 - accuracy: 0.9784 - val_loss: 0.0865 - val_accuracy: 0.9764
</code></pre><p>5、在测试集上对模型进行评估</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_learning_curves</span>(history):
    pd<span style=color:#f92672>.</span>DataFrame(history<span style=color:#f92672>.</span>history)<span style=color:#f92672>.</span>plot(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>5</span>))
    plt<span style=color:#f92672>.</span>grid(True)
    plt<span style=color:#f92672>.</span>gca()<span style=color:#f92672>.</span>set_ylim(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)
    plt<span style=color:#f92672>.</span>show()
plot_learning_curves(history)
</code></pre></div><p><img src=./_image/output_63_0.png alt=png></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>loss, acc <span style=color:#f92672>=</span> mnist_model<span style=color:#f92672>.</span>evaluate(x_test, y_test)
<span style=color:#66d9ef>print</span>(loss, acc)
</code></pre></div><pre><code>10000/10000 [==============================] - 1s 54us/sample - loss: 0.0923 - accuracy: 0.9716
0.09230228984989226 0.9716
</code></pre><h2 id=tensorflow-keras---custom>Tensorflow Keras - Custom</h2><p>在之前构建模型时使用的都是Tensorflow已经封装好的方法，下面通过实例来展示如何封装自己的计算方法。</p><p>1、Custom Layer (自定义层)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CustomizedDenseLayer</span>(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Layer):
    <span style=color:#66d9ef>def</span> __init__(self, units, activation<span style=color:#f92672>=</span>None, <span style=color:#f92672>**</span>kwargs):
        self<span style=color:#f92672>.</span>units <span style=color:#f92672>=</span> units
        self<span style=color:#f92672>.</span>activation <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Activation(activation)
        super(CustomizedDenseLayer, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
    
    <span style=color:#75715e>#</span>
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build</span>(self, input_shape):
        <span style=color:#75715e># 构建所需要的参数</span>
        self<span style=color:#f92672>.</span>kernel <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_weight(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;kernel&#34;</span>,
                                      shape<span style=color:#f92672>=</span>(input_shape[<span style=color:#ae81ff>1</span>], self<span style=color:#f92672>.</span>units),
                                      initializer<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;uniform&#34;</span>,
                                      dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32,
                                      trainable<span style=color:#f92672>=</span>True)
        self<span style=color:#f92672>.</span>bias <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_weight(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bias&#34;</span>, shape<span style=color:#f92672>=</span>(self<span style=color:#f92672>.</span>units, ),
                                   initializer <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;zeros&#34;</span>,
                                   dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32,
                                   trainable<span style=color:#f92672>=</span>True)
        super(CustomizedDenseLayer, self)<span style=color:#f92672>.</span>build(input_shape)
    
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call</span>(self, x):
        <span style=color:#75715e># 正向计算</span>
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>activation(x <span style=color:#960050;background-color:#1e0010>@</span> self<span style=color:#f92672>.</span>kernel <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>bias)
</code></pre></div><p>2、Custom Model (自定义模型)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CustomizedModel</span>(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Model):
    <span style=color:#66d9ef>def</span> __init__(self, <span style=color:#f92672>**</span>kwargs):
        super(CustomizedModel, self)<span style=color:#f92672>.</span>__init__(<span style=color:#f92672>**</span>kwargs)
        self<span style=color:#f92672>.</span>flatten <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Flatten()
        self<span style=color:#f92672>.</span>h1 <span style=color:#f92672>=</span> CustomizedDenseLayer(units<span style=color:#f92672>=</span><span style=color:#ae81ff>300</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;custom_h1&#39;</span>)
        self<span style=color:#f92672>.</span>h2 <span style=color:#f92672>=</span> CustomizedDenseLayer(units<span style=color:#f92672>=</span><span style=color:#ae81ff>200</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;custom_h2&#39;</span>)
        self<span style=color:#f92672>.</span>h3 <span style=color:#f92672>=</span> CustomizedDenseLayer(units<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;custom_h3&#39;</span>)
        self<span style=color:#f92672>.</span>output_layer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>10</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;softmax&#34;</span>)
        
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call</span>(self, input):
        ret <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>flatten(input)
        ret <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>h1(ret)
        ret <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>h2(ret)
        ret <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>h3(ret)
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>output_layer(ret)
    
custom_model <span style=color:#f92672>=</span> CustomizedModel()
custom_model<span style=color:#f92672>.</span>build(input_shape<span style=color:#f92672>=</span>(None, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>))
custom_model<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;customized_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          multiple                  0         
_________________________________________________________________
custom_h1 (CustomizedDenseLa multiple                  235500    
_________________________________________________________________
custom_h2 (CustomizedDenseLa multiple                  60200     
_________________________________________________________________
custom_h3 (CustomizedDenseLa multiple                  20100     
_________________________________________________________________
dense_4 (Dense)              multiple                  1010      
=================================================================
Total params: 316,810
Trainable params: 316,810
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>3、Custom Train (自定义训练过程)</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>custom_fit</span>(model, x_train, y_train, x_valid, y_valid, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>):
    steps_per_epoch <span style=color:#f92672>=</span> len(x_train) <span style=color:#f92672>//</span> batch_size
    
    optimizer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>optimizers<span style=color:#f92672>.</span>SGD()
    train_metric <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>metrics<span style=color:#f92672>.</span>SparseCategoricalAccuracy()
    valid_metric <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>metrics<span style=color:#f92672>.</span>SparseCategoricalAccuracy()
    loss_obj <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>losses<span style=color:#f92672>.</span>SparseCategoricalCrossentropy()

    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>random_batch</span>(x, y, batch_size<span style=color:#f92672>=</span>batch_size):
        idx <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, len(x_train), size<span style=color:#f92672>=</span>batch_size)
        <span style=color:#66d9ef>return</span> x[idx], y[idx]

    <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;steps_per_epoch is: &#34;</span> , steps_per_epoch)
    
    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
        train_metric<span style=color:#f92672>.</span>reset_states()
        valid_metric<span style=color:#f92672>.</span>reset_states()
        <span style=color:#66d9ef>for</span> step <span style=color:#f92672>in</span> range(steps_per_epoch):
            x_batch, y_batch <span style=color:#f92672>=</span> random_batch(x_train, y_train, batch_size)
            <span style=color:#66d9ef>with</span> tf<span style=color:#f92672>.</span>GradientTape() <span style=color:#66d9ef>as</span> tape:
                y_pred <span style=color:#f92672>=</span> model(x_batch)
                loss <span style=color:#f92672>=</span> loss_obj(y_batch, y_pred)
            grads <span style=color:#f92672>=</span> tape<span style=color:#f92672>.</span>gradient(loss, model<span style=color:#f92672>.</span>variables)
            optimizer<span style=color:#f92672>.</span>apply_gradients(zip(grads, model<span style=color:#f92672>.</span>variables))
            train_metric(y_batch, y_pred)
            <span style=color:#66d9ef>if</span> step <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
                <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\r</span><span style=color:#e6db74>Epoch&#34;</span>, epoch,<span style=color:#e6db74>&#34; train loss: &#34;</span>,  loss<span style=color:#f92672>.</span>numpy(), <span style=color:#e6db74>&#34; train mse: &#34;</span>, train_metric<span style=color:#f92672>.</span>result()<span style=color:#f92672>.</span>numpy())
        y_valid_pred <span style=color:#f92672>=</span> model(x_valid)
        valid_metric(y_valid, y_valid_pred)
        valid_loss <span style=color:#f92672>=</span> loss_obj(y_valid, y_valid_pred)
        <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>&#34;</span>, <span style=color:#e6db74>&#34;valid mse: &#34;</span>, valid_metric<span style=color:#f92672>.</span>result()<span style=color:#f92672>.</span>numpy())
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>custom_fit(model<span style=color:#f92672>=</span>custom_model, x_train<span style=color:#f92672>=</span>x_train, y_train<span style=color:#f92672>=</span>y_train, x_valid<span style=color:#f92672>=</span>x_valid, y_valid<span style=color:#f92672>=</span>y_valid, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</code></pre></div><pre><code>steps_per_epoch is:  859
WARNING:tensorflow:Layer customized_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Epoch 0  train loss:  2.3038578  train mse:  0.1875
Epoch 0  train loss:  2.280334  train mse:  0.21395421
Epoch 0  train loss:  2.2446647  train mse:  0.31584266
Epoch 0  train loss:  2.1839218  train mse:  0.3867317
Epoch 0  train loss:  2.092816  train mse:  0.42627805
Epoch 0  train loss:  1.8289787  train mse:  0.4548715
Epoch 0  train loss:  1.4188151  train mse:  0.48367304
Epoch 0  train loss:  1.1096661  train mse:  0.51466656
Epoch 0  train loss:  0.912598  train mse:  0.5451389
     valid mse:  0.8054
Epoch 1  train loss:  0.86221695  train mse:  0.8125
Epoch 1  train loss:  0.7114984  train mse:  0.80522895
Epoch 1  train loss:  0.5244288  train mse:  0.8143657
Epoch 1  train loss:  0.4718449  train mse:  0.82340115
Epoch 1  train loss:  0.44886717  train mse:  0.8302681
Epoch 1  train loss:  0.49274105  train mse:  0.8369199
Epoch 1  train loss:  0.3548026  train mse:  0.8425801
Epoch 1  train loss:  0.5724619  train mse:  0.8459567
Epoch 1  train loss:  0.34557572  train mse:  0.8497581
     valid mse:  0.8914
</code></pre><p><a href=https://playground.tensorflow.org/>https://playground.tensorflow.org/</a></p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ rel=tag>深度学习</a></li><li class=tags__item><a class="tags__link btn" href=/tags/tensorflow/ rel=tag>Tensorflow</a></li></ul></div></footer></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/deep-learning/2020-05-09-transformers%E7%9A%84%E4%B8%80%E4%BA%9B%E8%BF%B7%E6%80%9D.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>Transformers的一些迷思</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/deep-learning/2020-07-16-%E9%80%9A%E8%BF%87tensorflow2%E4%BD%BF%E7%94%A8bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>通过Tensorflow2使用Bert预训练模型的两种方式</p></a></div></nav><section class=comments><div id=disqus_thread></div><script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//writinglite.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c ',repo:'writinglite.com',owner:'writinglite',admin:['writinglite'],id:location.pathname,distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>