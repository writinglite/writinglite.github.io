<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>卷积神经网络实战_图片分类 - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="卷积神经网络实战_图片分类"><meta property="og:description" content="理论部分 CNN解决的问题 在CNN出现之前，图像对于人工智能来说是一个难题，有2个原因：
 图像需要处理的数据量太大，导致成本很高，效率很低。 图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高。  另一个角度（用全连接神经网络处理大尺寸图像的缺点）：
 其次参数过多效率低下，训练困难，同时大量的参数也很快会导致网络过拟合 图像展开为向量会丢失空间信息；  卷积层 这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。
在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：
池化层 池化层简单说就是下采样，他可以大大降低数据的维度，也可以缓解卷积层对位置的过度敏感性。其过程如下：
上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。
之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。
总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。
实战部分 import matplotlib.pyplot as plt import numpy as np import os import PIL import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import Sequential # 卷积层 x = tf.constant(range(9), dtype=tf.float32) x = tf.reshape(x, shape=(1, 3, 3, 1)) print('x :', tf.reshape(x, shape=(3, 3))) conv_layer = tf.keras.layers.Conv2D(filters=1, kernel_size=(2, 2), strides=(1, 1), kernel_initializer=tf."><meta property="og:type" content="article"><meta property="og:url" content="/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%984-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-10-13T20:43:00+00:00"><meta property="article:modified_time" content="2020-10-13T20:43:00+00:00"><meta itemprop=name content="卷积神经网络实战_图片分类"><meta itemprop=description content="理论部分 CNN解决的问题 在CNN出现之前，图像对于人工智能来说是一个难题，有2个原因：
 图像需要处理的数据量太大，导致成本很高，效率很低。 图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高。  另一个角度（用全连接神经网络处理大尺寸图像的缺点）：
 其次参数过多效率低下，训练困难，同时大量的参数也很快会导致网络过拟合 图像展开为向量会丢失空间信息；  卷积层 这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。
在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：
池化层 池化层简单说就是下采样，他可以大大降低数据的维度，也可以缓解卷积层对位置的过度敏感性。其过程如下：
上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。
之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。
总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。
实战部分 import matplotlib.pyplot as plt import numpy as np import os import PIL import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import Sequential # 卷积层 x = tf.constant(range(9), dtype=tf.float32) x = tf.reshape(x, shape=(1, 3, 3, 1)) print('x :', tf.reshape(x, shape=(3, 3))) conv_layer = tf.keras.layers.Conv2D(filters=1, kernel_size=(2, 2), strides=(1, 1), kernel_initializer=tf."><meta itemprop=datePublished content="2020-10-13T20:43:00+00:00"><meta itemprop=dateModified content="2020-10-13T20:43:00+00:00"><meta itemprop=wordCount content="7046"><meta itemprop=keywords content="深度学习,Tensorflow,"><meta name=twitter:card content="summary"><meta name=twitter:title content="卷积神经网络实战_图片分类"><meta name=twitter:description content="理论部分 CNN解决的问题 在CNN出现之前，图像对于人工智能来说是一个难题，有2个原因：
 图像需要处理的数据量太大，导致成本很高，效率很低。 图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高。  另一个角度（用全连接神经网络处理大尺寸图像的缺点）：
 其次参数过多效率低下，训练困难，同时大量的参数也很快会导致网络过拟合 图像展开为向量会丢失空间信息；  卷积层 这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。
在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：
池化层 池化层简单说就是下采样，他可以大大降低数据的维度，也可以缓解卷积层对位置的过度敏感性。其过程如下：
上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。
之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。
总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。
实战部分 import matplotlib.pyplot as plt import numpy as np import os import PIL import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import Sequential # 卷积层 x = tf.constant(range(9), dtype=tf.float32) x = tf.reshape(x, shape=(1, 3, 3, 1)) print('x :', tf.reshape(x, shape=(3, 3))) conv_layer = tf.keras.layers.Conv2D(filters=1, kernel_size=(2, 2), strides=(1, 1), kernel_initializer=tf."><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>卷积神经网络实战_图片分类</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-10-13T20:43:00Z>2020-10-13</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/tensorflow2.x-keras.html rel=category>Tensorflow2.x keras</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#理论部分>理论部分</a><ul><li><a href=#cnn解决的问题>CNN解决的问题</a></li><li><a href=#卷积层>卷积层</a></li><li><a href=#池化层>池化层</a></li></ul></li><li><a href=#实战部分>实战部分</a><ul><li><a href=#lenet>LeNet</a></li><li><a href=#alexnet>AlexNet</a></li><li><a href=#vgg>VGG</a></li><li><a href=#googlenet>GoogleNet</a></li><li><a href=#resnet>ResNet</a></li><li><a href=#图片分类>图片分类</a></li></ul></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=理论部分>理论部分</h2><h3 id=cnn解决的问题>CNN解决的问题</h3><p>在CNN出现之前，图像对于人工智能来说是一个难题，有2个原因：</p><ol><li>图像需要处理的数据量太大，导致成本很高，效率很低。</li><li>图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高。</li></ol><p>另一个角度（用全连接神经网络处理大尺寸图像的缺点）：</p><ol><li>其次参数过多效率低下，训练困难，同时大量的参数也很快会导致网络过拟合</li><li>图像展开为向量会丢失空间信息；</li></ol><h3 id=卷积层>卷积层</h3><p><img src=_image/juanji.gif alt></p><p>这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。</p><p>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：</p><p><img src=_image/150926.jpg alt></p><h3 id=池化层>池化层</h3><p>池化层简单说就是下采样，他可以大大降低数据的维度，也可以缓解卷积层对位置的过度敏感性。其过程如下：</p><p><img src=_image/chihua.gif alt></p><p>上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。</p><p>之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。</p><p>总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</p><h2 id=实战部分>实战部分</h2><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt
<span style=color:#f92672>import</span> numpy <span style=color:#f92672>as</span> np
<span style=color:#f92672>import</span> os
<span style=color:#f92672>import</span> PIL
<span style=color:#f92672>import</span> tensorflow <span style=color:#f92672>as</span> tf

<span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
<span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> layers
<span style=color:#f92672>from</span> tensorflow.keras.models <span style=color:#f92672>import</span> Sequential
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 卷积层</span>
x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant(range(<span style=color:#ae81ff>9</span>), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>reshape(x, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;x :&#39;</span>, tf<span style=color:#f92672>.</span>reshape(x, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)))
conv_layer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), kernel_initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>initializers<span style=color:#f92672>.</span>Ones())
conv_ret <span style=color:#f92672>=</span> conv_layer(x)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;shape :&#39;</span>, conv_ret<span style=color:#f92672>.</span>shape)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;conv ret :&#39;</span>, tf<span style=color:#f92672>.</span>reshape(conv_ret, shape<span style=color:#f92672>=</span>(conv_ret<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>], conv_ret<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>])))

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;kernel size :&#39;</span>, conv_layer<span style=color:#f92672>.</span>weights[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape)
</code></pre></div><pre><code>x : tf.Tensor(
[[0. 1. 2.]
 [3. 4. 5.]
 [6. 7. 8.]], shape=(3, 3), dtype=float32)
shape : (1, 2, 2, 1)
conv ret : tf.Tensor(
[[ 8. 12.]
 [20. 24.]], shape=(2, 2), dtype=float32)
kernel size : (2, 2, 1, 1)
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 多输入层-卷积层</span>
x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant(range(<span style=color:#ae81ff>18</span>), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>reshape(x, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>2</span>))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;x(channel-1) :&#39;</span>, tf<span style=color:#f92672>.</span>reshape(x[:, :, :, <span style=color:#ae81ff>0</span>], shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;x(channel-2) :&#39;</span>, tf<span style=color:#f92672>.</span>reshape(x[:, :, :, <span style=color:#ae81ff>1</span>], shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)))

conv_layer <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(<span style=color:#ae81ff>1</span>, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), kernel_initializer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>initializers<span style=color:#f92672>.</span>Ones())
conv_ret <span style=color:#f92672>=</span> conv_layer(x)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;shape :&#39;</span>, conv_ret<span style=color:#f92672>.</span>shape)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;conv ret :&#39;</span>, tf<span style=color:#f92672>.</span>reshape(conv_ret, shape<span style=color:#f92672>=</span>(conv_ret<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>], conv_ret<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>])))

<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;kernel size :&#39;</span>, conv_layer<span style=color:#f92672>.</span>weights[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape)
</code></pre></div><pre><code>x(channel-1) : tf.Tensor(
[[ 0.  2.  4.]
 [ 6.  8. 10.]
 [12. 14. 16.]], shape=(3, 3), dtype=float32)
x(channel-2) : tf.Tensor(
[[ 1.  3.  5.]
 [ 7.  9. 11.]
 [13. 15. 17.]], shape=(3, 3), dtype=float32)
shape : (1, 2, 2, 1)
conv ret : tf.Tensor(
[[ 36.  52.]
 [ 84. 100.]], shape=(2, 2), dtype=float32)
kernel size : (2, 2, 2, 1)
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 池化层</span>
x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>constant(range(<span style=color:#ae81ff>9</span>), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>reshape(x, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>))
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;x :&#39;</span>,tf<span style=color:#f92672>.</span>reshape(x, shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>)))

pool_ret <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>MaxPool2D(pool_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))(x)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;pooling ret: &#39;</span>, tf<span style=color:#f92672>.</span>reshape(pool_ret, shape<span style=color:#f92672>=</span>(conv_ret<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>], conv_ret<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>])))
</code></pre></div><pre><code>x : tf.Tensor(
[[0. 1. 2.]
 [3. 4. 5.]
 [6. 7. 8.]], shape=(3, 3), dtype=float32)
pooling ret:  tf.Tensor(
[[4. 5.]
 [7. 8.]], shape=(2, 2), dtype=float32)
</code></pre><h3 id=lenet>LeNet</h3><p>LeNet 诞生于 1994 年，是最早的卷积神经网络之一，LeNet分为卷积层块和全连接层块两个部分。</p><p>卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用 5×5 的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为 2×2 ，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p><p>卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p><p><img src=_image/LeNet.png alt=leNet></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>leNet_model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Sequential()
leNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>InputLayer(input_shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>1</span>)))

leNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>5</span>), activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>))
leNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>AveragePooling2D(pool_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>))) <span style=color:#75715e># strides default is None, If None, it will default to `pool_size`.  </span>

leNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>5</span>), activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>))
leNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>AveragePooling2D(pool_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)))

leNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Flatten())
leNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(units<span style=color:#f92672>=</span><span style=color:#ae81ff>120</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>))
leNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(units<span style=color:#f92672>=</span><span style=color:#ae81ff>84</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>))
leNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(units<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, activation <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;softmax&#39;</span>))
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>leNet_model<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_5 (Conv2D)            (None, 28, 28, 6)         156       
_________________________________________________________________
average_pooling2d (AveragePo (None, 14, 14, 6)         0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 10, 10, 16)        2416      
_________________________________________________________________
average_pooling2d_1 (Average (None, 5, 5, 16)          0         
_________________________________________________________________
flatten (Flatten)            (None, 400)               0         
_________________________________________________________________
dense (Dense)                (None, 120)               48120     
_________________________________________________________________
dense_1 (Dense)              (None, 84)                10164     
_________________________________________________________________
dense_2 (Dense)              (None, 10)                850       
=================================================================
Total params: 61,706
Trainable params: 61,706
Non-trainable params: 0
_________________________________________________________________
</code></pre><h3 id=alexnet>AlexNet</h3><p>2012年，AlexNet横空出世。这个模型的名字来源于论文第一作者的姓名Alex Krizhevsky [1]。AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。</p><p>AlexNet与LeNet的设计理念非常相似，但也有显著的区别。</p><ol><li><p><strong>更深更宽</strong>，与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层，AlexNet使用的卷积通道数也大于LeNet中的卷积通道数十倍。</p></li><li><p><strong>AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数</strong>。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，从而令模型无法得到有效训练。</p></li><li><p><strong>训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合</strong>。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout</p></li><li><p>AlexNet引入了大量的<strong>图像增广</strong>，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</p></li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>alexNet_model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Sequential()
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>InputLayer(input_shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>1</span>)))

<span style=color:#75715e># 使用较大的11 x 11窗口来捕获物体。同时使用步幅4来较大幅度减小输出高和宽。这里使用的输出通道数比LeNet中的也要大很多</span>
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>96</span>, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>11</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>4</span>), activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>))
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>MaxPool2D(pool_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)))

<span style=color:#75715e># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span>
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ZeroPadding2D(padding<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)))
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>5</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>))
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>MaxPool2D(pool_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)))

<span style=color:#75715e># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span>
<span style=color:#75715e># 前两个卷积层后不使用池化层来减小输入的高和宽</span>
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ZeroPadding2D(padding<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)))
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>384</span>, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>))

alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ZeroPadding2D(padding<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)))
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>384</span>, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>))

alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ZeroPadding2D(padding<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)))
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>))

<span style=color:#75715e># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span>
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>4096</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>))
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dropout(rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>))
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>4096</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>))
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dropout(rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>))

<span style=color:#75715e># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span>
alexNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>10</span>))

alexNet_model<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_7 (Conv2D)            (None, 54, 54, 96)        11712     
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 26, 26, 96)        0         
_________________________________________________________________
zero_padding2d (ZeroPadding2 (None, 30, 30, 96)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 26, 26, 256)       614656    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 12, 12, 256)       0         
_________________________________________________________________
zero_padding2d_1 (ZeroPaddin (None, 14, 14, 256)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 12, 12, 384)       885120    
_________________________________________________________________
zero_padding2d_2 (ZeroPaddin (None, 14, 14, 384)       0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 12, 12, 384)       1327488   
_________________________________________________________________
zero_padding2d_3 (ZeroPaddin (None, 14, 14, 384)       0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 12, 12, 256)       884992    
_________________________________________________________________
dense_3 (Dense)              (None, 12, 12, 4096)      1052672   
_________________________________________________________________
dropout (Dropout)            (None, 12, 12, 4096)      0         
_________________________________________________________________
dense_4 (Dense)              (None, 12, 12, 4096)      16781312  
_________________________________________________________________
dropout_1 (Dropout)          (None, 12, 12, 4096)      0         
_________________________________________________________________
dense_5 (Dense)              (None, 12, 12, 10)        40970     
=================================================================
Total params: 21,598,922
Trainable params: 21,598,922
Non-trainable params: 0
_________________________________________________________________
</code></pre><h3 id=vgg>VGG</h3><p>AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。我们将在本章的后续几节里介绍几种不同的深度网络设计思路。</p><p>本节介绍VGG，它的名字来源于论文作者所在的实验室Visual Geometry Group [1]。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。</p><p>VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为 3×3 的卷积层后接上一个步幅为2、窗口形状为 2×2 的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用vgg_block函数来实现这个基础的VGG块，它可以指定卷积层的数量num_convs和输出通道数num_channels。</p><p>不同结构的vgg：
<img src=_image/vgg.jpeg alt></p><p><strong>有用设计</strong>：两个3´3的卷积层串联相当于1个5´5的卷积层，即一个像素会跟周围5´5的像素产生关联，可以说感受野大小为5´5。而3个3´3的卷积层串联的效果则相当于1个7´7的卷积层。除此之外，3个串联的3´3的卷积层，拥有比1个7´7的卷积层更少的参数量，只有后者的。最重要的是，3个3´3的卷积层拥有比1个7´7的卷积层更多的非线性变换（前者可以使用三次ReLU激活函数，而后者只有一次），使得CNN对特征的学习能力更强。</p><p><img src=_image/3x3-5.jpeg alt></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>VGGBlock</span>(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Layer):
  <span style=color:#66d9ef>def</span> __init__(self, num_convs, num_channels):
    super(VGGBlock, self)<span style=color:#f92672>.</span>__init__()
    self<span style=color:#f92672>.</span>convs <span style=color:#f92672>=</span> []
    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_convs):
      self<span style=color:#f92672>.</span>convs<span style=color:#f92672>.</span>append(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ZeroPadding2D(padding<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)))
      self<span style=color:#f92672>.</span>convs<span style=color:#f92672>.</span>append(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span>num_channels, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>), activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>))
    self<span style=color:#f92672>.</span>maxpool <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>MaxPool2D(pool_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>))
  
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call</span>(self, inputs):
    <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>convs:
      inputs <span style=color:#f92672>=</span> layer(inputs)
    outputs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>maxpool(inputs)
    <span style=color:#66d9ef>return</span> outputs

vgg_block <span style=color:#f92672>=</span> VGGBlock(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>64</span>)

test_inputs <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>uniform(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>), dtype<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>float32)
vgg_block<span style=color:#f92672>.</span>call(test_inputs)<span style=color:#f92672>.</span>shape
</code></pre></div><pre><code>TensorShape([2, 16, 16, 64])
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_vggNet</span>(conv_arch):
    vggNet_model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Sequential()
    vggNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>InputLayer(input_shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>1</span>)))
    <span style=color:#75715e># 卷积层部分</span>
    <span style=color:#66d9ef>for</span> (num_convs, num_channels) <span style=color:#f92672>in</span> conv_arch:
        vggNet_model<span style=color:#f92672>.</span>add(VGGBlock(num_convs, num_channels))
    vggNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Flatten())
    <span style=color:#75715e># 全连接层部分</span>
    vggNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>4096</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>))
    vggNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dropout(rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>))
    vggNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>4096</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>))
    vggNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dropout(rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>))
    vggNet_model<span style=color:#f92672>.</span>add(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(units<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>))
    <span style=color:#66d9ef>return</span> vggNet_model

conv_arch <span style=color:#f92672>=</span> ((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>64</span>), (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>128</span>), (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>256</span>), (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>512</span>), (<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>512</span>))
vggNet <span style=color:#f92672>=</span> generate_vggNet(conv_arch)
vggNet<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;sequential_5&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
vgg_block_16 (VGGBlock)      (None, 112, 112, 64)      640       
_________________________________________________________________
vgg_block_17 (VGGBlock)      (None, 56, 56, 128)       73856     
_________________________________________________________________
vgg_block_18 (VGGBlock)      (None, 28, 28, 256)       885248    
_________________________________________________________________
vgg_block_19 (VGGBlock)      (None, 14, 14, 512)       3539968   
_________________________________________________________________
vgg_block_20 (VGGBlock)      (None, 7, 7, 512)         4719616   
_________________________________________________________________
flatten_1 (Flatten)          (None, 25088)             0         
_________________________________________________________________
dense_12 (Dense)             (None, 4096)              102764544 
_________________________________________________________________
dropout_6 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_13 (Dense)             (None, 4096)              16781312  
_________________________________________________________________
dropout_7 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_14 (Dense)             (None, 10)                40970     
=================================================================
Total params: 128,806,154
Trainable params: 128,806,154
Non-trainable params: 0
_________________________________________________________________
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>vgg16_app <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>applications<span style=color:#f92672>.</span>VGG16(weights<span style=color:#f92672>=</span>None)
vgg16_app<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;vgg16&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_9 (InputLayer)         [(None, 224, 224, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544 
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
predictions (Dense)          (None, 1000)              4097000   
=================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
_________________________________________________________________
</code></pre><h3 id=googlenet>GoogleNet</h3><p>Google Inception Net首次出现在ILSVRC 2014的比赛中（和VGGNet同年），就以较大优势取得了第一名。那届比赛中的Inception Net通常被称为Inception V1，它最大的特点是控制了计算量和参数量的同时，获得了非常好的分类性能——top-5错误率6.67%，只有AlexNet的一半不到。</p><p>Inception Module的基本结构，其中有4个分支：</p><ol><li>第一个分支对输入进行1´1的卷积，这其实也是NIN中提出的一个重要结构。1´1的卷积是一个非常优秀的结构，它可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道升维和降维。可以看到Inception Module的4个分支都用到了1´1卷积，来进行低成本（计算量比3´3小很多）的跨通道的特征变换。</li><li>第二个分支先使用了1´1卷积，然后连接3´3卷积，相当于进行了两次特征变换。</li><li>第三个分支类似，先是1´1的卷积，然后连接5´5卷积。</li><li>最后一个分支则是3´3最大池化后直接使用1´1卷积。
<img src=_image/inception.png alt=inception></li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Inception</span>(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Layer):
  <span style=color:#66d9ef>def</span> __init__(self):
    super(Inception, self)<span style=color:#f92672>.</span>__init__()

    self<span style=color:#f92672>.</span>b1_conv_1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)

    self<span style=color:#f92672>.</span>b2_conv_1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)
    self<span style=color:#f92672>.</span>b2_padding <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ZeroPadding2D(padding<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
    self<span style=color:#f92672>.</span>b2_conv_3 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)

    <span style=color:#75715e># padding(1) + conv(3*3) = conv(3*3, padding=&#39;same&#39;)</span>

    self<span style=color:#f92672>.</span>b3_conv_1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)
    self<span style=color:#f92672>.</span>b3_padding <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ZeroPadding2D(padding<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>))
    self<span style=color:#f92672>.</span>b3_conv_5 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)

    self<span style=color:#f92672>.</span>b4_padding <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ZeroPadding2D(padding<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
    self<span style=color:#f92672>.</span>b4_pool_3 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>MaxPool2D(pool_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, strides<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
    self<span style=color:#f92672>.</span>b4_conv_1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(filters<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)

    self<span style=color:#f92672>.</span>concatenate <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Concatenate()
  
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call</span>(self, inputs):
    <span style=color:#75715e># 线路1，单1 x 1卷积层</span>
    branch_1 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>b1_conv_1(inputs)
    <span style=color:#75715e># 线路2，1 x 1卷积层后接3 x 3卷积层</span>
    branch_2 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>b2_conv_3(self<span style=color:#f92672>.</span>b2_padding(self<span style=color:#f92672>.</span>b2_conv_1(inputs)))
    <span style=color:#75715e># 线路3，1 x 1卷积层后接5 x 5卷积层</span>
    branch_3 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>b3_conv_5(self<span style=color:#f92672>.</span>b3_padding(self<span style=color:#f92672>.</span>b3_conv_1(inputs)))
    <span style=color:#75715e># 线路4，3 x 3最大池化层后接1 x 1卷积层</span>
    branch_4 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>b4_conv_1(self<span style=color:#f92672>.</span>b4_padding(self<span style=color:#f92672>.</span>b4_pool_3(inputs)))
    <span style=color:#75715e># channel_axis = -1</span>
    outputs <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>concatenate([branch_1, branch_2, branch_3, branch_4], axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>) 
    <span style=color:#66d9ef>return</span> outputs
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>inception <span style=color:#f92672>=</span> Inception()
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>test_inputs <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>uniform(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>3</span>))
inception(test_inputs)<span style=color:#f92672>.</span>shape
</code></pre></div><pre><code>TensorShape([2, 10, 10, 256])
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>tmp <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>applications<span style=color:#f92672>.</span>InceptionV3(weights<span style=color:#f92672>=</span>None)
tmp<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;inception_v3&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           [(None, 299, 299, 3) 0                                            
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 149, 149, 32) 864         input_11[0][0]                   
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 149, 149, 32) 96          conv2d_53[0][0]                  
__________________________________________________________________________________________________
activation (Activation)         (None, 149, 149, 32) 0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 147, 147, 32) 9216        activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 147, 147, 32) 96          conv2d_54[0][0]                  
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 147, 147, 32) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 147, 147, 64) 18432       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 64) 192         conv2d_55[0][0]                  
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 64) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_24 (MaxPooling2D) (None, 73, 73, 64)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 73, 73, 80)   5120        max_pooling2d_24[0][0]           
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 73, 73, 80)   240         conv2d_56[0][0]                  
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 73, 73, 80)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 71, 71, 192)  138240      activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 71, 71, 192)  576         conv2d_57[0][0]                  
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 71, 71, 192)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
max_pooling2d_25 (MaxPooling2D) (None, 35, 35, 192)  0           activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 35, 35, 64)   12288       max_pooling2d_25[0][0]           
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_61[0][0]                  
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 35, 35, 48)   9216        max_pooling2d_25[0][0]           
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 35, 35, 96)   55296       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 48)   144         conv2d_59[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 96)   288         conv2d_62[0][0]                  
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 48)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 96)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_25[0][0]           
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 35, 35, 64)   12288       max_pooling2d_25[0][0]           
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 35, 35, 64)   76800       activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 35, 35, 96)   82944       activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 35, 35, 64)   192         conv2d_58[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 64)   192         conv2d_60[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_63[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 32)   96          conv2d_64[0][0]                  
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 35, 35, 64)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 64)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 32)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_5[0][0]               
                                                                 activation_7[0][0]               
                                                                 activation_10[0][0]              
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_68[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 35, 35, 96)   55296       activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 48)   144         conv2d_66[0][0]                  
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 96)   288         conv2d_69[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 48)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 96)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 35, 35, 64)   76800       activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 35, 35, 96)   82944       activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 64)   192         conv2d_65[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 64)   192         conv2d_67[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_70[0][0]                  
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 64)   192         conv2d_71[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 64)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 64)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 64)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_12[0][0]              
                                                                 activation_14[0][0]              
                                                                 activation_17[0][0]              
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_75[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 35, 35, 96)   55296       activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 48)   144         conv2d_73[0][0]                  
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 96)   288         conv2d_76[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 48)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 96)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 35, 35, 64)   76800       activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 35, 35, 96)   82944       activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_72[0][0]                  
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 64)   192         conv2d_74[0][0]                  
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_77[0][0]                  
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 64)   192         conv2d_78[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 64)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 64)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_19[0][0]              
                                                                 activation_21[0][0]              
                                                                 activation_24[0][0]              
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 35, 35, 64)   192         conv2d_80[0][0]                  
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 35, 35, 64)   0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 35, 35, 96)   55296       activation_27[0][0]              
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 96)   288         conv2d_81[0][0]                  
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 96)   0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 17, 17, 96)   82944       activation_28[0][0]              
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 17, 17, 384)  1152        conv2d_79[0][0]                  
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 17, 17, 96)   288         conv2d_82[0][0]                  
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 17, 17, 384)  0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 17, 17, 96)   0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
max_pooling2d_26 (MaxPooling2D) (None, 17, 17, 288)  0           mixed2[0][0]                     
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_26[0][0]              
                                                                 activation_29[0][0]              
                                                                 max_pooling2d_26[0][0]           
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 128)  384         conv2d_87[0][0]                  
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 128)  0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 17, 17, 128)  114688      activation_34[0][0]              
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_88[0][0]                  
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 128)  384         conv2d_84[0][0]                  
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_89[0][0]                  
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 128)  0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 17, 17, 128)  114688      activation_31[0][0]              
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_85[0][0]                  
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_90[0][0]                  
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 17, 17, 192)  172032      activation_32[0][0]              
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 17, 17, 192)  172032      activation_37[0][0]              
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 192)  576         conv2d_83[0][0]                  
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 192)  576         conv2d_86[0][0]                  
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 192)  576         conv2d_91[0][0]                  
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_92[0][0]                  
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 192)  0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 192)  0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 192)  0           batch_normalization_38[0][0]     
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_30[0][0]              
                                                                 activation_33[0][0]              
                                                                 activation_38[0][0]              
                                                                 activation_39[0][0]              
__________________________________________________________________________________________________
conv2d_97 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 160)  480         conv2d_97[0][0]                  
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 160)  0           batch_normalization_44[0][0]     
__________________________________________________________________________________________________
conv2d_98 (Conv2D)              (None, 17, 17, 160)  179200      activation_44[0][0]              
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_98[0][0]                  
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_99 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 160)  480         conv2d_94[0][0]                  
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_99[0][0]                  
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 160)  0           batch_normalization_41[0][0]     
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     
__________________________________________________________________________________________________
conv2d_95 (Conv2D)              (None, 17, 17, 160)  179200      activation_41[0][0]              
__________________________________________________________________________________________________
conv2d_100 (Conv2D)             (None, 17, 17, 160)  179200      activation_46[0][0]              
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_95[0][0]                  
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_100[0][0]                 
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_96 (Conv2D)              (None, 17, 17, 192)  215040      activation_42[0][0]              
__________________________________________________________________________________________________
conv2d_101 (Conv2D)             (None, 17, 17, 192)  215040      activation_47[0][0]              
__________________________________________________________________________________________________
conv2d_102 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_93[0][0]                  
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 192)  576         conv2d_96[0][0]                  
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 192)  576         conv2d_101[0][0]                 
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_102[0][0]                 
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 192)  0           batch_normalization_43[0][0]     
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 192)  0           batch_normalization_48[0][0]     
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_40[0][0]              
                                                                 activation_43[0][0]              
                                                                 activation_48[0][0]              
                                                                 activation_49[0][0]              
__________________________________________________________________________________________________
conv2d_107 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 160)  480         conv2d_107[0][0]                 
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 160)  0           batch_normalization_54[0][0]     
__________________________________________________________________________________________________
conv2d_108 (Conv2D)             (None, 17, 17, 160)  179200      activation_54[0][0]              
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_108[0][0]                 
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     
__________________________________________________________________________________________________
conv2d_104 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_109 (Conv2D)             (None, 17, 17, 160)  179200      activation_55[0][0]              
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 160)  480         conv2d_104[0][0]                 
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_109[0][0]                 
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 160)  0           batch_normalization_51[0][0]     
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     
__________________________________________________________________________________________________
conv2d_105 (Conv2D)             (None, 17, 17, 160)  179200      activation_51[0][0]              
__________________________________________________________________________________________________
conv2d_110 (Conv2D)             (None, 17, 17, 160)  179200      activation_56[0][0]              
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_105[0][0]                 
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_110[0][0]                 
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_103 (Conv2D)             (None, 17, 17, 192)  147456      mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_106 (Conv2D)             (None, 17, 17, 192)  215040      activation_52[0][0]              
__________________________________________________________________________________________________
conv2d_111 (Conv2D)             (None, 17, 17, 192)  215040      activation_57[0][0]              
__________________________________________________________________________________________________
conv2d_112 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_103[0][0]                 
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 192)  576         conv2d_106[0][0]                 
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 192)  576         conv2d_111[0][0]                 
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_112[0][0]                 
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 192)  0           batch_normalization_53[0][0]     
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 192)  0           batch_normalization_58[0][0]     
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_50[0][0]              
                                                                 activation_53[0][0]              
                                                                 activation_58[0][0]              
                                                                 activation_59[0][0]              
__________________________________________________________________________________________________
conv2d_117 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_117[0][0]                 
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     
__________________________________________________________________________________________________
conv2d_118 (Conv2D)             (None, 17, 17, 192)  258048      activation_64[0][0]              
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_118[0][0]                 
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     
__________________________________________________________________________________________________
conv2d_114 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_119 (Conv2D)             (None, 17, 17, 192)  258048      activation_65[0][0]              
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_114[0][0]                 
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_119[0][0]                 
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     
__________________________________________________________________________________________________
conv2d_115 (Conv2D)             (None, 17, 17, 192)  258048      activation_61[0][0]              
__________________________________________________________________________________________________
conv2d_120 (Conv2D)             (None, 17, 17, 192)  258048      activation_66[0][0]              
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_115[0][0]                 
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_120[0][0]                 
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_113 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_116 (Conv2D)             (None, 17, 17, 192)  258048      activation_62[0][0]              
__________________________________________________________________________________________________
conv2d_121 (Conv2D)             (None, 17, 17, 192)  258048      activation_67[0][0]              
__________________________________________________________________________________________________
conv2d_122 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_113[0][0]                 
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_116[0][0]                 
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_121[0][0]                 
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_122[0][0]                 
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_60[0][0]              
                                                                 activation_63[0][0]              
                                                                 activation_68[0][0]              
                                                                 activation_69[0][0]              
__________________________________________________________________________________________________
conv2d_125 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 17, 17, 192)  576         conv2d_125[0][0]                 
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 17, 17, 192)  0           batch_normalization_72[0][0]     
__________________________________________________________________________________________________
conv2d_126 (Conv2D)             (None, 17, 17, 192)  258048      activation_72[0][0]              
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_126[0][0]                 
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     
__________________________________________________________________________________________________
conv2d_123 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     
__________________________________________________________________________________________________
conv2d_127 (Conv2D)             (None, 17, 17, 192)  258048      activation_73[0][0]              
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_123[0][0]                 
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_127[0][0]                 
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     
__________________________________________________________________________________________________
conv2d_124 (Conv2D)             (None, 8, 8, 320)    552960      activation_70[0][0]              
__________________________________________________________________________________________________
conv2d_128 (Conv2D)             (None, 8, 8, 192)    331776      activation_74[0][0]              
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 8, 8, 320)    960         conv2d_124[0][0]                 
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 8, 8, 192)    576         conv2d_128[0][0]                 
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 8, 8, 320)    0           batch_normalization_71[0][0]     
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 8, 8, 192)    0           batch_normalization_75[0][0]     
__________________________________________________________________________________________________
max_pooling2d_27 (MaxPooling2D) (None, 8, 8, 768)    0           mixed7[0][0]                     
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_71[0][0]              
                                                                 activation_75[0][0]              
                                                                 max_pooling2d_27[0][0]           
__________________________________________________________________________________________________
conv2d_133 (Conv2D)             (None, 8, 8, 448)    573440      mixed8[0][0]                     
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 448)    1344        conv2d_133[0][0]                 
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 448)    0           batch_normalization_80[0][0]     
__________________________________________________________________________________________________
conv2d_130 (Conv2D)             (None, 8, 8, 384)    491520      mixed8[0][0]                     
__________________________________________________________________________________________________
conv2d_134 (Conv2D)             (None, 8, 8, 384)    1548288     activation_80[0][0]              
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 384)    1152        conv2d_130[0][0]                 
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 384)    1152        conv2d_134[0][0]                 
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 384)    0           batch_normalization_77[0][0]     
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 384)    0           batch_normalization_81[0][0]     
__________________________________________________________________________________________________
conv2d_131 (Conv2D)             (None, 8, 8, 384)    442368      activation_77[0][0]              
__________________________________________________________________________________________________
conv2d_132 (Conv2D)             (None, 8, 8, 384)    442368      activation_77[0][0]              
__________________________________________________________________________________________________
conv2d_135 (Conv2D)             (None, 8, 8, 384)    442368      activation_81[0][0]              
__________________________________________________________________________________________________
conv2d_136 (Conv2D)             (None, 8, 8, 384)    442368      activation_81[0][0]              
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     
__________________________________________________________________________________________________
conv2d_129 (Conv2D)             (None, 8, 8, 320)    409600      mixed8[0][0]                     
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_131[0][0]                 
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_132[0][0]                 
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_135[0][0]                 
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_136[0][0]                 
__________________________________________________________________________________________________
conv2d_137 (Conv2D)             (None, 8, 8, 192)    245760      average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 320)    960         conv2d_129[0][0]                 
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 192)    576         conv2d_137[0][0]                 
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 320)    0           batch_normalization_76[0][0]     
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_78[0][0]              
                                                                 activation_79[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_82[0][0]              
                                                                 activation_83[0][0]              
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 192)    0           batch_normalization_84[0][0]     
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_76[0][0]              
                                                                 mixed9_0[0][0]                   
                                                                 concatenate_2[0][0]              
                                                                 activation_84[0][0]              
__________________________________________________________________________________________________
conv2d_142 (Conv2D)             (None, 8, 8, 448)    917504      mixed9[0][0]                     
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 448)    1344        conv2d_142[0][0]                 
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 448)    0           batch_normalization_89[0][0]     
__________________________________________________________________________________________________
conv2d_139 (Conv2D)             (None, 8, 8, 384)    786432      mixed9[0][0]                     
__________________________________________________________________________________________________
conv2d_143 (Conv2D)             (None, 8, 8, 384)    1548288     activation_89[0][0]              
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 384)    1152        conv2d_139[0][0]                 
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 384)    1152        conv2d_143[0][0]                 
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 384)    0           batch_normalization_86[0][0]     
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 384)    0           batch_normalization_90[0][0]     
__________________________________________________________________________________________________
conv2d_140 (Conv2D)             (None, 8, 8, 384)    442368      activation_86[0][0]              
__________________________________________________________________________________________________
conv2d_141 (Conv2D)             (None, 8, 8, 384)    442368      activation_86[0][0]              
__________________________________________________________________________________________________
conv2d_144 (Conv2D)             (None, 8, 8, 384)    442368      activation_90[0][0]              
__________________________________________________________________________________________________
conv2d_145 (Conv2D)             (None, 8, 8, 384)    442368      activation_90[0][0]              
__________________________________________________________________________________________________
average_pooling2d_10 (AveragePo (None, 8, 8, 2048)   0           mixed9[0][0]                     
__________________________________________________________________________________________________
conv2d_138 (Conv2D)             (None, 8, 8, 320)    655360      mixed9[0][0]                     
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_140[0][0]                 
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_141[0][0]                 
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_144[0][0]                 
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_145[0][0]                 
__________________________________________________________________________________________________
conv2d_146 (Conv2D)             (None, 8, 8, 192)    393216      average_pooling2d_10[0][0]       
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 320)    960         conv2d_138[0][0]                 
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 192)    576         conv2d_146[0][0]                 
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 320)    0           batch_normalization_85[0][0]     
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_87[0][0]              
                                                                 activation_88[0][0]              
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 8, 8, 768)    0           activation_91[0][0]              
                                                                 activation_92[0][0]              
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 192)    0           batch_normalization_93[0][0]     
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_85[0][0]              
                                                                 mixed9_1[0][0]                   
                                                                 concatenate_3[0][0]              
                                                                 activation_93[0][0]              
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    
__________________________________________________________________________________________________
predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   
==================================================================================================
Total params: 23,851,784
Trainable params: 23,817,352
Non-trainable params: 34,432
__________________________________________________________________________________________________
</code></pre><h3 id=resnet>ResNet</h3><p>ResNet（Residual Neural Network）由微软研究院的Kaiming He等4名华人提出，通过使用Residual Unit成功训练152层深的神经网络，在ILSVRC 2015比赛中获得了冠军，取得3.57%的top-5错误率，同时参数量却比VGGNet低，效果非常突出。</p><p>假定某段神经网络的输入是x，期望输出是，如果我们直接把输入x传到输出作为初始结果，那么此时我们需要学习的目标就是。如图14所示，这就是一个ResNet的残差学习单元（Residual Unit），ResNet相当于将学习目标改变了，不再是学习一个完整的输出，只是输出和输入的差别，即残差。</p><p>可以看到普通直连的卷积神经网络和ResNet的最大区别在于，ResNet有很多旁路的支线将输入直接连到后面的层，使得后面的层可以直接学习残差，这种结构也被称为shortcut或skip connections。</p><p><img src=_image/residual.png alt></p><p><strong>Inception V2</strong> 学习了VGGNet，用两个3´3的卷积代替5´5的大卷积（用以降低参数量并减轻过拟合），还提出了著名的Batch Normalization（以下简称BN）方法。</p><p><strong>Inception V3</strong> 网络则主要的改造：引入了Factorization into small convolutions的思想，将一个较大的二维卷积拆成两个较小的一维卷积，比如将7´7卷积拆成1´7卷积和7´1卷积，或者将3´3卷积拆成1´3卷积和3´1卷积，如图12所示。一方面节约了大量参数，加速运算并减轻了过拟合（比将7´7卷积拆成1´7卷积和7´1卷积，比拆成3个3´3卷积更节约参数）</p><p>总结：
在AlexNet之后，我们可以将卷积神经网络的发展分为两类，一类是网络结构上的改进调整（图18中的左侧分支），另一类是网络深度的增加（图18中的右侧分支）。
<img src=_image/lenet-inception_restnet.png alt></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Residual</span>(tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Layer):
  <span style=color:#66d9ef>def</span> __init__(self, num_channels, use_1x1conv<span style=color:#f92672>=</span>False, strides<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, <span style=color:#f92672>**</span>kwargs):
    super(Residual, self)<span style=color:#f92672>.</span>__init__()
    self<span style=color:#f92672>.</span>zeropadding1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ZeroPadding2D(padding<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
    self<span style=color:#f92672>.</span>conv1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(num_channels, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>), strides<span style=color:#f92672>=</span>strides)
    self<span style=color:#f92672>.</span>zeropadding2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ZeroPadding2D(padding<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
    self<span style=color:#f92672>.</span>conv2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(num_channels, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>))

    <span style=color:#66d9ef>if</span> use_1x1conv:
      self<span style=color:#f92672>.</span>conv3 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Conv2D(num_channels, kernel_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), strides<span style=color:#f92672>=</span>strides)
    <span style=color:#66d9ef>else</span>:
      self<span style=color:#f92672>.</span>conv3 <span style=color:#f92672>=</span> None
    self<span style=color:#f92672>.</span>bn1 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>BatchNormalization()
    self<span style=color:#f92672>.</span>bn2 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>BatchNormalization()
  
  <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call</span>(self, X):
    Y <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ReLU()(self<span style=color:#f92672>.</span>bn1(self<span style=color:#f92672>.</span>conv1(self<span style=color:#f92672>.</span>zeropadding1(X))))
    Y <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>bn2(self<span style=color:#f92672>.</span>conv2(self<span style=color:#f92672>.</span>zeropadding2(Y)))
    <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>conv3:
      X <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv3(X)
    <span style=color:#66d9ef>return</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>ReLU()(Y <span style=color:#f92672>+</span> X)
residual <span style=color:#f92672>=</span> Residual(<span style=color:#ae81ff>64</span>, use_1x1conv<span style=color:#f92672>=</span>True)
test_inputs <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>uniform(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>))
residual(test_inputs)<span style=color:#f92672>.</span>shape
</code></pre></div><pre><code>TensorShape([2, 32, 32, 64])
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>resNet_app <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>applications<span style=color:#f92672>.</span>ResNet50(weights<span style=color:#f92672>=</span>None)
resNet_app<span style=color:#f92672>.</span>summary()
</code></pre></div><pre><code>Model: &quot;resnet50&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_12 (InputLayer)           [(None, 224, 224, 3) 0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_12[0][0]                   
__________________________________________________________________________________________________
conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 
__________________________________________________________________________________________________
pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 
__________________________________________________________________________________________________
conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          
                                                                 conv2_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           
__________________________________________________________________________________________________
conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           
                                                                 conv2_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           
__________________________________________________________________________________________________
conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           
                                                                 conv2_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          
                                                                 conv3_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           
__________________________________________________________________________________________________
conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           
                                                                 conv3_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           
__________________________________________________________________________________________________
conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           
                                                                 conv3_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           
__________________________________________________________________________________________________
conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           
                                                                 conv3_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          
                                                                 conv4_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           
__________________________________________________________________________________________________
conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           
                                                                 conv4_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           
__________________________________________________________________________________________________
conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           
                                                                 conv4_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           
__________________________________________________________________________________________________
conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           
                                                                 conv4_block4_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           
__________________________________________________________________________________________________
conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           
                                                                 conv4_block5_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           
__________________________________________________________________________________________________
conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        
__________________________________________________________________________________________________
conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        
__________________________________________________________________________________________________
conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           
                                                                 conv4_block6_3_bn[0][0]          
__________________________________________________________________________________________________
conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           
__________________________________________________________________________________________________
conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          
                                                                 conv5_block1_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           
__________________________________________________________________________________________________
conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           
                                                                 conv5_block2_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           
__________________________________________________________________________________________________
conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        
__________________________________________________________________________________________________
conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        
__________________________________________________________________________________________________
conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           
                                                                 conv5_block3_3_bn[0][0]          
__________________________________________________________________________________________________
conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           
__________________________________________________________________________________________________
predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   
==================================================================================================
Total params: 25,636,712
Trainable params: 25,583,592
Non-trainable params: 53,120
__________________________________________________________________________________________________
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>plot_model(resNet_app)
</code></pre></div><p><img src=_image/output_23_0.png alt=png></p><p>CNN技巧：</p><ol><li>“1Padding+3x3卷积” 与 “2Padding+5x5卷积”可以保持特征图长宽不变。</li><li>1x1卷积在不需要padding的情况下就可以保持特征图长宽不变，通常用来降低channel数量。</li><li>3x3卷积很有效（并能减少参数），2个3x3卷积=5x5卷积, 3个3x3卷积=7x7卷积（这里的等于指感受野）。</li><li>一个较大的二维卷积拆成两个较小的一维卷积，比如将7´7卷积拆成1´7卷积和7´1卷积。一方面节约了大量参数，加速运算并减轻了过拟合。</li><li>卷积在做特征提取时，通常不会改变特征图的长宽，则是通用pooling层来减小特征图长宽。</li><li>整个卷积网络变化趋势：特征图长宽逐渐减少（提高感受野），通道数量逐渐增加（提取更多特征信息）。</li></ol><h3 id=图片分类>图片分类</h3><h4 id=数据下载>数据下载</h4><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># !wget https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip</span>
<span style=color:#75715e># !unzip cats_and_dogs_filtered.zip</span>
</code></pre></div><h4 id=数据读取>数据读取</h4><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>PATH <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(<span style=color:#e6db74>&#39;./&#39;</span>, <span style=color:#e6db74>&#39;cats_and_dogs_filtered&#39;</span>)

train_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(PATH, <span style=color:#e6db74>&#39;train&#39;</span>)
validation_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(PATH, <span style=color:#e6db74>&#39;validation&#39;</span>)

BATCH_SIZE <span style=color:#f92672>=</span> <span style=color:#ae81ff>32</span>
IMG_SIZE <span style=color:#f92672>=</span> (<span style=color:#ae81ff>160</span>, <span style=color:#ae81ff>160</span>)

train_dataset <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>image_dataset_from_directory(train_dir,
                        shuffle<span style=color:#f92672>=</span>True,
                        batch_size<span style=color:#f92672>=</span>BATCH_SIZE,
                        image_size<span style=color:#f92672>=</span>IMG_SIZE)

validation_dataset <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>image_dataset_from_directory(validation_dir,
                          shuffle<span style=color:#f92672>=</span>True,
                          batch_size<span style=color:#f92672>=</span>BATCH_SIZE,
                          image_size<span style=color:#f92672>=</span>IMG_SIZE)

class_names <span style=color:#f92672>=</span> train_dataset<span style=color:#f92672>.</span>class_names
num_classes <span style=color:#f92672>=</span> len(class_names)
<span style=color:#66d9ef>print</span>(class_names)
</code></pre></div><pre><code>Found 2000 files belonging to 2 classes.
Found 1000 files belonging to 2 classes.
['cats', 'dogs']
</code></pre><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#f92672>as</span> plt

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>show_data</span>(dataset):
  plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>))
  <span style=color:#66d9ef>for</span> images, labels <span style=color:#f92672>in</span> dataset<span style=color:#f92672>.</span>take(<span style=color:#ae81ff>1</span>):
    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>9</span>):
      ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplot(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
      plt<span style=color:#f92672>.</span>imshow(images[i]<span style=color:#f92672>.</span>numpy()<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#34;uint8&#34;</span>))
      plt<span style=color:#f92672>.</span>title(class_names[labels[i]])
      plt<span style=color:#f92672>.</span>axis(<span style=color:#e6db74>&#34;off&#34;</span>)

show_data(train_dataset)
</code></pre></div><p><img src=_image/output_29_0.png alt=png></p><h4 id=构建模型并训练>构建模型并训练</h4><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_model</span>():
  model <span style=color:#f92672>=</span> Sequential([
    layers<span style=color:#f92672>.</span>experimental<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>Rescaling(<span style=color:#ae81ff>1.</span><span style=color:#f92672>/</span><span style=color:#ae81ff>255</span>, input_shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>160</span>, <span style=color:#ae81ff>160</span>, <span style=color:#ae81ff>3</span>)),
    layers<span style=color:#f92672>.</span>Conv2D(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;same&#39;</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
    layers<span style=color:#f92672>.</span>MaxPooling2D(),
    layers<span style=color:#f92672>.</span>Conv2D(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;same&#39;</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
    layers<span style=color:#f92672>.</span>MaxPooling2D(),
    layers<span style=color:#f92672>.</span>Conv2D(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;same&#39;</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
    layers<span style=color:#f92672>.</span>MaxPooling2D(),
    layers<span style=color:#f92672>.</span>Flatten(),
    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>128</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
    layers<span style=color:#f92672>.</span>Dense(num_classes)
  ])
  <span style=color:#66d9ef>return</span> model

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_model</span>(model, train_dataset, validation_dataset, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>,):
  model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;adam&#39;</span>,
          loss<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>losses<span style=color:#f92672>.</span>SparseCategoricalCrossentropy(from_logits<span style=color:#f92672>=</span>True),
          metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>])
  history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(
    train_dataset,
    validation_data<span style=color:#f92672>=</span>validation_dataset,
    epochs<span style=color:#f92672>=</span>epochs
  )
  <span style=color:#66d9ef>return</span> history
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_history</span>(history, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>):
  acc <span style=color:#f92672>=</span> history<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;accuracy&#39;</span>]
  val_acc <span style=color:#f92672>=</span> history<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;val_accuracy&#39;</span>]

  loss<span style=color:#f92672>=</span>history<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;loss&#39;</span>]
  val_loss<span style=color:#f92672>=</span>history<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#39;val_loss&#39;</span>]

  epochs_range <span style=color:#f92672>=</span> range(epochs)

  plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>8</span>))
  plt<span style=color:#f92672>.</span>subplot(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>)
  plt<span style=color:#f92672>.</span>plot(epochs_range, acc, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Training Accuracy&#39;</span>)
  plt<span style=color:#f92672>.</span>plot(epochs_range, val_acc, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Validation Accuracy&#39;</span>)
  plt<span style=color:#f92672>.</span>legend(loc<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;lower right&#39;</span>)
  plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Training and Validation Accuracy&#39;</span>)

  plt<span style=color:#f92672>.</span>subplot(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)
  plt<span style=color:#f92672>.</span>plot(epochs_range, loss, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Training Loss&#39;</span>)
  plt<span style=color:#f92672>.</span>plot(epochs_range, val_loss, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Validation Loss&#39;</span>)
  plt<span style=color:#f92672>.</span>legend(loc<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;upper right&#39;</span>)
  plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Training and Validation Loss&#39;</span>)
  plt<span style=color:#f92672>.</span>show()
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>sample_model <span style=color:#f92672>=</span> generate_model()
history <span style=color:#f92672>=</span> train_model(sample_model, train_dataset, validation_dataset, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
plot_history(history)
</code></pre></div><pre><code>Epoch 1/10
63/63 [==============================] - 69s 1s/step - loss: 0.7187 - accuracy: 0.5605 - val_loss: 0.6372 - val_accuracy: 0.6250
Epoch 2/10
63/63 [==============================] - 32s 516ms/step - loss: 0.6184 - accuracy: 0.6575 - val_loss: 0.6171 - val_accuracy: 0.6930
Epoch 3/10
63/63 [==============================] - 36s 564ms/step - loss: 0.5688 - accuracy: 0.7185 - val_loss: 0.6320 - val_accuracy: 0.6750
Epoch 4/10
63/63 [==============================] - 36s 566ms/step - loss: 0.5011 - accuracy: 0.7600 - val_loss: 0.6163 - val_accuracy: 0.6820
Epoch 5/10
63/63 [==============================] - 37s 580ms/step - loss: 0.4381 - accuracy: 0.7865 - val_loss: 0.6432 - val_accuracy: 0.7060
Epoch 6/10
63/63 [==============================] - 38s 607ms/step - loss: 0.3843 - accuracy: 0.8220 - val_loss: 0.6558 - val_accuracy: 0.7250
Epoch 7/10
63/63 [==============================] - 37s 583ms/step - loss: 0.2740 - accuracy: 0.8825 - val_loss: 0.7358 - val_accuracy: 0.7180
Epoch 8/10
63/63 [==============================] - 38s 604ms/step - loss: 0.1906 - accuracy: 0.9180 - val_loss: 0.8276 - val_accuracy: 0.7210
Epoch 9/10
63/63 [==============================] - 40s 629ms/step - loss: 0.1391 - accuracy: 0.9480 - val_loss: 0.9111 - val_accuracy: 0.7340
Epoch 10/10
63/63 [==============================] - 41s 648ms/step - loss: 0.0952 - accuracy: 0.9665 - val_loss: 1.0884 - val_accuracy: 0.7120
</code></pre><p><img src=_image/output_33_1.png alt=png></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_model_with_data_augmentation_and_dropout</span>():
  data_augmentation <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential(
    [
      layers<span style=color:#f92672>.</span>experimental<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>RandomFlip(<span style=color:#e6db74>&#34;horizontal&#34;</span>, input_shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>160</span>, <span style=color:#ae81ff>160</span>,<span style=color:#ae81ff>3</span>)),
      layers<span style=color:#f92672>.</span>experimental<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>RandomRotation(<span style=color:#ae81ff>0.1</span>),
      layers<span style=color:#f92672>.</span>experimental<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>RandomZoom(<span style=color:#ae81ff>0.1</span>),
    ]
  )

  model <span style=color:#f92672>=</span> Sequential([
    data_augmentation,
    layers<span style=color:#f92672>.</span>experimental<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>Rescaling(<span style=color:#ae81ff>1.</span><span style=color:#f92672>/</span><span style=color:#ae81ff>255</span>),
    layers<span style=color:#f92672>.</span>Conv2D(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;same&#39;</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
    layers<span style=color:#f92672>.</span>MaxPooling2D(),
    layers<span style=color:#f92672>.</span>Conv2D(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;same&#39;</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
    layers<span style=color:#f92672>.</span>MaxPooling2D(),
    layers<span style=color:#f92672>.</span>Conv2D(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;same&#39;</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
    layers<span style=color:#f92672>.</span>MaxPooling2D(),
    layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.5</span>),
    layers<span style=color:#f92672>.</span>Flatten(),
    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>128</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
    layers<span style=color:#f92672>.</span>Dense(num_classes)
  ])
  <span style=color:#66d9ef>return</span> model
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>with_data_augmentation_and_dropout_model <span style=color:#f92672>=</span> generate_model_with_data_augmentation_and_dropout()
history <span style=color:#f92672>=</span> train_model(with_data_augmentation_and_dropout_model, train_dataset, validation_dataset, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>15</span>)
plot_history(history, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>15</span>)
</code></pre></div><pre><code>Epoch 1/15
63/63 [==============================] - 35s 563ms/step - loss: 0.7666 - accuracy: 0.5060 - val_loss: 0.6933 - val_accuracy: 0.5000
Epoch 2/15
63/63 [==============================] - 36s 570ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6924 - val_accuracy: 0.5900
Epoch 3/15
63/63 [==============================] - 35s 552ms/step - loss: 0.6944 - accuracy: 0.5090 - val_loss: 0.6920 - val_accuracy: 0.5860
Epoch 4/15
63/63 [==============================] - 35s 555ms/step - loss: 0.6894 - accuracy: 0.5420 - val_loss: 0.6890 - val_accuracy: 0.5810
Epoch 5/15
63/63 [==============================] - 34s 546ms/step - loss: 0.6824 - accuracy: 0.5685 - val_loss: 0.6842 - val_accuracy: 0.5640
Epoch 6/15
63/63 [==============================] - 35s 550ms/step - loss: 0.6776 - accuracy: 0.5735 - val_loss: 0.6683 - val_accuracy: 0.5840
Epoch 7/15
63/63 [==============================] - 35s 559ms/step - loss: 0.6499 - accuracy: 0.6380 - val_loss: 0.6459 - val_accuracy: 0.6110
Epoch 8/15
63/63 [==============================] - 35s 560ms/step - loss: 0.6231 - accuracy: 0.6710 - val_loss: 0.6170 - val_accuracy: 0.6690
Epoch 9/15
63/63 [==============================] - 36s 567ms/step - loss: 0.6075 - accuracy: 0.6870 - val_loss: 0.5888 - val_accuracy: 0.7010
Epoch 10/15
63/63 [==============================] - 35s 558ms/step - loss: 0.5903 - accuracy: 0.6930 - val_loss: 0.5931 - val_accuracy: 0.6900
Epoch 11/15
63/63 [==============================] - 36s 573ms/step - loss: 0.5764 - accuracy: 0.7070 - val_loss: 0.5924 - val_accuracy: 0.6880
Epoch 12/15
63/63 [==============================] - 35s 562ms/step - loss: 0.5679 - accuracy: 0.7160 - val_loss: 0.5671 - val_accuracy: 0.7040
Epoch 13/15
63/63 [==============================] - 35s 559ms/step - loss: 0.5727 - accuracy: 0.7105 - val_loss: 0.5832 - val_accuracy: 0.6940
Epoch 14/15
63/63 [==============================] - 36s 571ms/step - loss: 0.5539 - accuracy: 0.7225 - val_loss: 0.5918 - val_accuracy: 0.7000
Epoch 15/15
63/63 [==============================] - 36s 565ms/step - loss: 0.5497 - accuracy: 0.7160 - val_loss: 0.5694 - val_accuracy: 0.6990
</code></pre><p><img src=_image/output_35_1.png alt=png></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_model_with_vgg</span>():
  vgg16 <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>applications<span style=color:#f92672>.</span>VGG16(include_top<span style=color:#f92672>=</span>False, weights<span style=color:#f92672>=</span>None, input_shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>160</span>, <span style=color:#ae81ff>160</span>, <span style=color:#ae81ff>3</span>))
  vgg16<span style=color:#f92672>.</span>load_weights(<span style=color:#e6db74>&#39;vgg16_weights_tf_dim_ordering_tf_kernels.h5&#39;</span>, by_name<span style=color:#f92672>=</span>True)
  vgg16<span style=color:#f92672>.</span>trainable<span style=color:#f92672>=</span>False
  data_augmentation <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Sequential([
    tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>experimental<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>RandomFlip(<span style=color:#e6db74>&#39;horizontal&#39;</span>),
    tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>experimental<span style=color:#f92672>.</span>preprocessing<span style=color:#f92672>.</span>RandomRotation(<span style=color:#ae81ff>0.2</span>),
  ])

  preprocess_input <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>applications<span style=color:#f92672>.</span>vgg16<span style=color:#f92672>.</span>preprocess_input

  inputs <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Input(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>3</span>))
  x <span style=color:#f92672>=</span> data_augmentation(inputs)
  x <span style=color:#f92672>=</span> preprocess_input(x)
  x <span style=color:#f92672>=</span> vgg16(x, training<span style=color:#f92672>=</span>True)
  x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>GlobalAveragePooling2D()(x)
  x <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.2</span>)(x)
  outputs <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(num_classes)(x)
  model <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>Model(inputs, outputs)

  <span style=color:#66d9ef>return</span> model
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>with_vgg_model <span style=color:#f92672>=</span> generate_model_with_vgg()
history <span style=color:#f92672>=</span> train_model(with_vgg_model, train_dataset, validation_dataset, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
plot_history(history, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</code></pre></div><pre><code>WARNING:tensorflow:Model was constructed with shape (None, 160, 160, 3) for input Tensor(&quot;input_9:0&quot;, shape=(None, 160, 160, 3), dtype=float32), but it was called on an input with incompatible shape (None, 224, 224, 3).
Epoch 1/10
WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(&quot;input_10:0&quot;, shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 160, 160, 3).
WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(&quot;input_10:0&quot;, shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 160, 160, 3).
63/63 [==============================] - ETA: 0s - loss: 2.2022 - accuracy: 0.7145WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(&quot;input_10:0&quot;, shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 160, 160, 3).
63/63 [==============================] - 335s 5s/step - loss: 2.2022 - accuracy: 0.7145 - val_loss: 0.6007 - val_accuracy: 0.8890
Epoch 2/10
63/63 [==============================] - 321s 5s/step - loss: 0.8997 - accuracy: 0.8455 - val_loss: 0.3511 - val_accuracy: 0.9260
Epoch 3/10
63/63 [==============================] - 317s 5s/step - loss: 0.6832 - accuracy: 0.8770 - val_loss: 0.2706 - val_accuracy: 0.9440
Epoch 4/10
63/63 [==============================] - 379s 6s/step - loss: 0.5846 - accuracy: 0.8890 - val_loss: 0.2178 - val_accuracy: 0.9620
Epoch 5/10
63/63 [==============================] - 335s 5s/step - loss: 0.5474 - accuracy: 0.9040 - val_loss: 0.2843 - val_accuracy: 0.9450
Epoch 6/10
63/63 [==============================] - 353s 6s/step - loss: 0.4610 - accuracy: 0.9095 - val_loss: 0.1814 - val_accuracy: 0.9570
Epoch 7/10
63/63 [==============================] - 399s 6s/step - loss: 0.4025 - accuracy: 0.9105 - val_loss: 0.1666 - val_accuracy: 0.9640
Epoch 8/10
63/63 [==============================] - 369s 6s/step - loss: 0.3679 - accuracy: 0.9250 - val_loss: 0.1574 - val_accuracy: 0.9610
Epoch 9/10
63/63 [==============================] - 344s 5s/step - loss: 0.3023 - accuracy: 0.9200 - val_loss: 0.1745 - val_accuracy: 0.9650
Epoch 10/10
63/63 [==============================] - 336s 5s/step - loss: 0.3916 - accuracy: 0.9170 - val_loss: 0.1473 - val_accuracy: 0.9650
</code></pre><p><img src=_image/output_37_1.png alt=png></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>evalation</span>(model, dataset):
  image_batch, label_batch <span style=color:#f92672>=</span> dataset<span style=color:#f92672>.</span>as_numpy_iterator()<span style=color:#f92672>.</span>next()
  predictions <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(image_batch)
  predictions <span style=color:#f92672>=</span> tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>argmax(predictions, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
  <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;Predictions:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>, predictions<span style=color:#f92672>.</span>numpy())
  <span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#39;Labels:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>, label_batch)

  plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>))
  <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>9</span>):
    ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplot(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
    plt<span style=color:#f92672>.</span>imshow(image_batch[i]<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#34;uint8&#34;</span>))
    plt<span style=color:#f92672>.</span>title(class_names[predictions[i]])
    plt<span style=color:#f92672>.</span>axis(<span style=color:#e6db74>&#34;off&#34;</span>)
</code></pre></div><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>evalation(with_vgg_model, validation_dataset)
</code></pre></div><pre><code>Predictions:
 [1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0]
Labels:
 [1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0]
</code></pre><p><img src=_image/output_39_1.png alt=png></p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ rel=tag>深度学习</a></li><li class=tags__item><a class="tags__link btn" href=/tags/tensorflow/ rel=tag>Tensorflow</a></li></ul></div></footer></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/deep-learning/2020-07-29-batch_normalization%E4%B8%8Elayer_normalization%E7%9A%84%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>Batch Normalization与Layer Normalization的理解整理</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/deep-learning/2020-10-16-deeplearning%E4%B8%ADcrf%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>DeepLearning中CRF计算原理</p></a></div></nav><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c',repo:'writinglite.github.io',owner:'writinglite',admin:['writinglite'],id:'aaaaa',distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>