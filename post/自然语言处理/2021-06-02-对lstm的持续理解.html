<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>对LSTM的持续理解 - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="对LSTM的持续理解"><meta property="og:description" content="深度学习的流行起源于图像领域，CNN算是最早被大家所熟悉的网络结构。但是对于带有时间特性或序列性质的数据任务上，CNN式的网络结构并不是完美契合的。因此后来有人提出了与上述任务更加契合的网络结构-RNN。
RNN实际上是一类网络结构的统称，具有以下特点的网络都可以算法是RNN。

即： \( y^t, h^t = f(x^t, h^{t-1}) \) 因此对$f$的不同处理产生了不同的RNN算法，如LSTM、GRU。
与CNN相同，也可以对RNN进行Deep：

RNN面临的问题 回到单层的RNN，设计一个简单的$f$函数 \( \begin{align} h_t &= tanh(W^xx_t &#43; W^hh_{t-1}) \\ y_t &= tanh(W^oh_t) \end{align} \) 上述方法有一个致命的缺点——梯度消失（梯度爆炸）。那如何解决这个问题呢，其中一种方案就是LSTM。
LSTM的提出 LSTM的计算方式如下： \( \begin{align} & i_t = sigm(W^{xi}x_t &#43; W^{hi}h_{t-1}) \\ & f_t = sigm(W^{xf}x_t &#43; W^{hf}h_{t-1}) \\ & o_t = sigm(W^{xo}x_t &#43; W^{ho}h_{t-1}) \\ & \tilde{c_t} = tanh(W^{xc}x_t &#43; W^{hc}h_{t-1}) \\ & c_t = f_t \bigodot c_{t-1} &#43; i_t \bigodot \tilde{c_t} \\ & h_t = o_t \bigodot tanh(c_t) \\ & y_t = \sigma(W^o h_t) \end{align} \)"><meta property="og:type" content="article"><meta property="og:url" content="/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-06-02-%E5%AF%B9lstm%E7%9A%84%E6%8C%81%E7%BB%AD%E7%90%86%E8%A7%A3.html"><meta property="article:section" content="post"><meta itemprop=name content="对LSTM的持续理解"><meta itemprop=description content="深度学习的流行起源于图像领域，CNN算是最早被大家所熟悉的网络结构。但是对于带有时间特性或序列性质的数据任务上，CNN式的网络结构并不是完美契合的。因此后来有人提出了与上述任务更加契合的网络结构-RNN。
RNN实际上是一类网络结构的统称，具有以下特点的网络都可以算法是RNN。

即： \( y^t, h^t = f(x^t, h^{t-1}) \) 因此对$f$的不同处理产生了不同的RNN算法，如LSTM、GRU。
与CNN相同，也可以对RNN进行Deep：

RNN面临的问题 回到单层的RNN，设计一个简单的$f$函数 \( \begin{align} h_t &= tanh(W^xx_t &#43; W^hh_{t-1}) \\ y_t &= tanh(W^oh_t) \end{align} \) 上述方法有一个致命的缺点——梯度消失（梯度爆炸）。那如何解决这个问题呢，其中一种方案就是LSTM。
LSTM的提出 LSTM的计算方式如下： \( \begin{align} & i_t = sigm(W^{xi}x_t &#43; W^{hi}h_{t-1}) \\ & f_t = sigm(W^{xf}x_t &#43; W^{hf}h_{t-1}) \\ & o_t = sigm(W^{xo}x_t &#43; W^{ho}h_{t-1}) \\ & \tilde{c_t} = tanh(W^{xc}x_t &#43; W^{hc}h_{t-1}) \\ & c_t = f_t \bigodot c_{t-1} &#43; i_t \bigodot \tilde{c_t} \\ & h_t = o_t \bigodot tanh(c_t) \\ & y_t = \sigma(W^o h_t) \end{align} \)"><meta itemprop=wordCount content="130"><meta itemprop=keywords content="自然语言处理,"><meta name=twitter:card content="summary"><meta name=twitter:title content="对LSTM的持续理解"><meta name=twitter:description content="深度学习的流行起源于图像领域，CNN算是最早被大家所熟悉的网络结构。但是对于带有时间特性或序列性质的数据任务上，CNN式的网络结构并不是完美契合的。因此后来有人提出了与上述任务更加契合的网络结构-RNN。
RNN实际上是一类网络结构的统称，具有以下特点的网络都可以算法是RNN。

即： \( y^t, h^t = f(x^t, h^{t-1}) \) 因此对$f$的不同处理产生了不同的RNN算法，如LSTM、GRU。
与CNN相同，也可以对RNN进行Deep：

RNN面临的问题 回到单层的RNN，设计一个简单的$f$函数 \( \begin{align} h_t &= tanh(W^xx_t &#43; W^hh_{t-1}) \\ y_t &= tanh(W^oh_t) \end{align} \) 上述方法有一个致命的缺点——梯度消失（梯度爆炸）。那如何解决这个问题呢，其中一种方案就是LSTM。
LSTM的提出 LSTM的计算方式如下： \( \begin{align} & i_t = sigm(W^{xi}x_t &#43; W^{hi}h_{t-1}) \\ & f_t = sigm(W^{xf}x_t &#43; W^{hf}h_{t-1}) \\ & o_t = sigm(W^{xo}x_t &#43; W^{ho}h_{t-1}) \\ & \tilde{c_t} = tanh(W^{xc}x_t &#43; W^{hc}h_{t-1}) \\ & c_t = f_t \bigodot c_{t-1} &#43; i_t \bigodot \tilde{c_t} \\ & h_t = o_t \bigodot tanh(c_t) \\ & y_t = \sigma(W^o h_t) \end{align} \)"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/archives/><span class=menu__text>archives</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>对LSTM的持续理解</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/nlp.html rel=category>NLP</a></span></div></div></header><div class="content post__content clearfix"><p>深度学习的流行起源于图像领域，CNN算是最早被大家所熟悉的网络结构。但是对于带有时间特性或序列性质的数据任务上，CNN式的网络结构并不是完美契合的。因此后来有人提出了与上述任务更加契合的网络结构-RNN。</p><p>RNN实际上是一类网络结构的统称，具有以下特点的网络都可以算法是RNN。</p><p><figure><img src=2021-06-02-%E5%AF%B9LSTM%E7%9A%84%E6%8C%81%E7%BB%AD%E7%90%86%E8%A7%A3.assets/RNN.png alt=img></figure></p><p>即：
<span class=math>\(
y^t, h^t = f(x^t, h^{t-1})
\)</span>
因此对$f$的不同处理产生了不同的RNN算法，如LSTM、GRU。</p><p>与CNN相同，也可以对RNN进行Deep：</p><p><figure><img src=2021-06-02-%E5%AF%B9LSTM%E7%9A%84%E6%8C%81%E7%BB%AD%E7%90%86%E8%A7%A3.assets/Deep_RNN.png alt=img></figure></p><h3 id=rnn面临的问题>RNN面临的问题</h3><p>回到单层的RNN，设计一个简单的$f$函数
<span class=math>\(
\begin{align}
h_t &= tanh(W^xx_t + W^hh_{t-1}) \\
y_t &= tanh(W^oh_t)
\end{align}
\)</span>
上述方法有一个致命的缺点——梯度消失（梯度爆炸）。那如何解决这个问题呢，其中一种方案就是LSTM。</p><h3 id=lstm的提出>LSTM的提出</h3><p>LSTM的计算方式如下：
<span class=math>\(
\begin{align}
& i_t = sigm(W^{xi}x_t + W^{hi}h_{t-1}) \\
& f_t = sigm(W^{xf}x_t + W^{hf}h_{t-1}) \\
& o_t = sigm(W^{xo}x_t + W^{ho}h_{t-1}) \\
& \tilde{c_t} = tanh(W^{xc}x_t + W^{hc}h_{t-1}) \\
& c_t = f_t \bigodot c_{t-1} + i_t \bigodot \tilde{c_t} \\
& h_t = o_t \bigodot tanh(c_t) \\
& y_t = \sigma(W^o h_t)
\end{align}
\)</span></p><h4 id=lstm如何解决梯度消失问题>LSTM如何解决梯度消失问题</h4><h4 id=如何理解-hidden-stateh-和-cell-statec>如何理解 hidden state(h) 和 cell state(c)</h4><h5 id=说法1>说法1</h5><p>hidden state是cell state经过一个神经元和一道“输出门”后得到的，因此hidden state里包含的记忆，实际上是cell state衰减之后的内容。另外，cell state在一个衰减较少的通道里沿时间轴传递，对时间跨度较大的信息的保持能力比hidden state要强很多。</p><p>因此，实际上hidden state里存储的，主要是“近期记忆”；cell state里存储的，主要是“远期记忆”。cell state的存在，使得LSTM得以对长依赖进行很好地刻画。</p><h5 id=说法2>说法2</h5><p>h(t) 是依赖于 c(t) 的非线性变换。</p><p>c(t) 是使用 f(t) 和 i(t) 计算的。</p><p>最后 f (t) 和 i(t) 取决于 h(t−1)。</p><p>因此，在我看来，c 充当内存，而 h 充当它的副本，可以传递它以在下一个时间步进一步处理输入。 也许这可能是 GRU 将两者合并的动机。</p><hr><p><a href=https://www.zhihu.com/question/68456751/answer/1097053422>https://www.zhihu.com/question/68456751/answer/1097053422</a></p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/ rel=tag>自然语言处理</a></li></ul></div></footer></article></main><nav class="pager flex"><div class="pager__item pager__item--next"><a class=pager__link href=/post/other/2016-04-08-hexo%E4%B8%8Egithubpages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>hexo与githubpages搭建个人博客</p></a></div></nav><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c',repo:'writinglite.github.io',owner:'writinglite',admin:['writinglite'],id:decodeURI(location.pathname.split("/").pop()).replace(".html","").substring(0,49),distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script><script type=text/x-mathjax-config>
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script></body></html>