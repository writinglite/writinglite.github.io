<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>集成学习简单介绍 - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="集成学习简单介绍"><meta property="og:description" content="背景 目前在kaggle中，GBDT和RF都是非常流行的算法，在一些比赛中，排名先前的算法都有这两种算法的身影。这两种算法都属于Ensemble算法，因此，本文将介绍介绍下Ensemble的思想，以及实现Ensemble的两种方法。
什么是集成学习 Ensemble通过构建多个学习器来完成学习任务，直觉上来说，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但并不是说，随机进行组合就能得到好的效果，各个子模型应该“好而不同”，即个体学习器要有一定的“准确性”又要有“互补性”。
根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即
 个体学习器间存在强依赖关系、必须串行生成的序列化算法。 个体学习器间不存在依赖关系、可同时生成的并行化算法。 前者的代表是Bossting，后者的代表是Bagging。从偏差-方差的角度看Bossting主要关注降低偏差，因此Bossting能基于泛化性能相当弱的学习器构建出很强的学习器，比如一棵只有5层的决策树。而Bagging主要关注降低方差，因此它在不剪枝层次较深的决策树、神经网络等易样本干扰的学习器上效用更加明显。  要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。
Bostting Bostting是一族可将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这个T个基学习器进行加权结合。
Bagging 如上所述，一个可行的集成学习算法，要满足两个要点，一是“多样性”二是“准确性”。一个可行的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据集中训练出一个学习器，这样就满足了“多样性”。然而，采样的每个子集都完全不同，则每个学习器只用到了一个小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器，为了解决这个问题可以采用自助采样法。
 《机器学习》 周志华 https://www.zhihu.com/question/29036379"><meta property="og:type" content="article"><meta property="og:url" content="writinglite.com/post/machine-learning/2017-07-15-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"><meta property="article:section" content="post"><meta itemprop=name content="集成学习简单介绍"><meta itemprop=description content="背景 目前在kaggle中，GBDT和RF都是非常流行的算法，在一些比赛中，排名先前的算法都有这两种算法的身影。这两种算法都属于Ensemble算法，因此，本文将介绍介绍下Ensemble的思想，以及实现Ensemble的两种方法。
什么是集成学习 Ensemble通过构建多个学习器来完成学习任务，直觉上来说，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但并不是说，随机进行组合就能得到好的效果，各个子模型应该“好而不同”，即个体学习器要有一定的“准确性”又要有“互补性”。
根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即
 个体学习器间存在强依赖关系、必须串行生成的序列化算法。 个体学习器间不存在依赖关系、可同时生成的并行化算法。 前者的代表是Bossting，后者的代表是Bagging。从偏差-方差的角度看Bossting主要关注降低偏差，因此Bossting能基于泛化性能相当弱的学习器构建出很强的学习器，比如一棵只有5层的决策树。而Bagging主要关注降低方差，因此它在不剪枝层次较深的决策树、神经网络等易样本干扰的学习器上效用更加明显。  要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。
Bostting Bostting是一族可将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这个T个基学习器进行加权结合。
Bagging 如上所述，一个可行的集成学习算法，要满足两个要点，一是“多样性”二是“准确性”。一个可行的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据集中训练出一个学习器，这样就满足了“多样性”。然而，采样的每个子集都完全不同，则每个学习器只用到了一个小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器，为了解决这个问题可以采用自助采样法。
 《机器学习》 周志华 https://www.zhihu.com/question/29036379"><meta itemprop=wordCount content="16"><meta itemprop=keywords content="机器学习,"><meta name=twitter:card content="summary"><meta name=twitter:title content="集成学习简单介绍"><meta name=twitter:description content="背景 目前在kaggle中，GBDT和RF都是非常流行的算法，在一些比赛中，排名先前的算法都有这两种算法的身影。这两种算法都属于Ensemble算法，因此，本文将介绍介绍下Ensemble的思想，以及实现Ensemble的两种方法。
什么是集成学习 Ensemble通过构建多个学习器来完成学习任务，直觉上来说，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但并不是说，随机进行组合就能得到好的效果，各个子模型应该“好而不同”，即个体学习器要有一定的“准确性”又要有“互补性”。
根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即
 个体学习器间存在强依赖关系、必须串行生成的序列化算法。 个体学习器间不存在依赖关系、可同时生成的并行化算法。 前者的代表是Bossting，后者的代表是Bagging。从偏差-方差的角度看Bossting主要关注降低偏差，因此Bossting能基于泛化性能相当弱的学习器构建出很强的学习器，比如一棵只有5层的决策树。而Bagging主要关注降低方差，因此它在不剪枝层次较深的决策树、神经网络等易样本干扰的学习器上效用更加明显。  要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。
Bostting Bostting是一族可将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这个T个基学习器进行加权结合。
Bagging 如上所述，一个可行的集成学习算法，要满足两个要点，一是“多样性”二是“准确性”。一个可行的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据集中训练出一个学习器，这样就满足了“多样性”。然而，采样的每个子集都完全不同，则每个学习器只用到了一个小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器，为了解决这个问题可以采用自助采样法。
 《机器学习》 周志华 https://www.zhihu.com/question/29036379"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/writinglite.com/css/style.css><link rel=stylesheet href=/writinglite.com/css/custom.css><link rel="shortcut icon" href=/writinglite.com/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/writinglite.com title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>集成学习简单介绍</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#背景>背景</a></li><li><a href=#什么是集成学习>什么是集成学习</a></li><li><a href=#bostting>Bostting</a></li><li><a href=#bagging>Bagging</a></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=背景>背景</h2><p>目前在kaggle中，GBDT和RF都是非常流行的算法，在一些比赛中，排名先前的算法都有这两种算法的身影。这两种算法都属于Ensemble算法，因此，本文将介绍介绍下Ensemble的思想，以及实现Ensemble的两种方法。</p><h2 id=什么是集成学习>什么是集成学习</h2><p>Ensemble通过构建多个学习器来完成学习任务，直觉上来说，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但并不是说，随机进行组合就能得到好的效果，各个子模型应该“好而不同”，即个体学习器要有一定的“准确性”又要有“互补性”。</p><p>根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即</p><ol><li>个体学习器间存在强依赖关系、必须串行生成的序列化算法。</li><li>个体学习器间不存在依赖关系、可同时生成的并行化算法。
前者的代表是Bossting，后者的代表是Bagging。从偏差-方差的角度看Bossting主要关注降低偏差，因此Bossting能基于泛化性能相当弱的学习器构建出很强的学习器，比如一棵只有5层的决策树。而Bagging主要关注降低方差，因此它在不剪枝层次较深的决策树、神经网络等易样本干扰的学习器上效用更加明显。</li></ol><p><strong>要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。</strong></p><h2 id=bostting>Bostting</h2><p>Bostting是一族可将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这个T个基学习器进行加权结合。</p><h2 id=bagging>Bagging</h2><p>如上所述，一个可行的集成学习算法，要满足两个要点，一是“多样性”二是“准确性”。一个可行的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据集中训练出一个学习器，这样就满足了“多样性”。然而，采样的每个子集都完全不同，则每个学习器只用到了一个小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器，为了解决这个问题可以采用自助采样法。</p><hr><p>《机器学习》 周志华
<a href=https://www.zhihu.com/question/29036379>https://www.zhihu.com/question/29036379</a></p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/writinglite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ rel=tag>机器学习</a></li></ul></div></footer></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=writinglite.com/post/other/2017-05-09-%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/ rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>静态博客搭建</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=writinglite.com/post/machine-learning/2016-11-25-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/ rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>隐马尔可夫模型</p></a></div></nav></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/writinglite.com/js/menu.js></script><script src=/writinglite.com/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>