<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>生成模型与高斯判别分析 - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="生成模型与高斯判别分析"><meta property="og:description" content="生成模型与判别模型 判别模型 判别模型对P(y|x)或者说对y建模，直接学习得到y。 常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。
生成模型 生成模型对P(x|y)或者说对x建模，通过计算 $$ \begin{aligned} \arg\max\limits_{y}p(y\vert x) &= \arg\max\limits_y\frac{p(x\vert y)p(y)}{p(x)} \
&= \arg\max\limits_y p(x\vert y)p(y) \end{aligned} $$ 得到y。 因此给定x进行比较时，P(x)为固定值，所以P(x)可省略。 常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等。
 高斯分布 若随机变量 X, X服从一个位置参数为 $\mu$ 、尺度参数为$\sigma$ 的概率分布，记为： $$ \displaystyle X \sim N(\mu ,\sigma ^{2}) $$ 则其概率密度函数为 $$ \displaystyle f(x)={1 \over \sigma {\sqrt {2\pi }}},e^{-{(x-\mu )^{2} \over 2\sigma ^{2}}} $$
多元高斯分布 $$ X ∼ N(\mu,\Sigma) $$
$$ \begin{aligned} E[X] &= \mu \
Cov(X) &= \Sigma \end{aligned} $$ 概率密度函数为 $$ \begin{equation} p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \end{equation} $$ 其中$|\Sigma|$是$\Sigma$的行列式，$\Sigma$是协方差矩阵，而且是对称半正定的。"><meta property="og:type" content="article"><meta property="og:url" content="/post/machine-learning/2017-03-12-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-03-12T11:09:00+00:00"><meta property="article:modified_time" content="2017-03-12T11:09:00+00:00"><meta itemprop=name content="生成模型与高斯判别分析"><meta itemprop=description content="生成模型与判别模型 判别模型 判别模型对P(y|x)或者说对y建模，直接学习得到y。 常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。
生成模型 生成模型对P(x|y)或者说对x建模，通过计算 $$ \begin{aligned} \arg\max\limits_{y}p(y\vert x) &= \arg\max\limits_y\frac{p(x\vert y)p(y)}{p(x)} \
&= \arg\max\limits_y p(x\vert y)p(y) \end{aligned} $$ 得到y。 因此给定x进行比较时，P(x)为固定值，所以P(x)可省略。 常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等。
 高斯分布 若随机变量 X, X服从一个位置参数为 $\mu$ 、尺度参数为$\sigma$ 的概率分布，记为： $$ \displaystyle X \sim N(\mu ,\sigma ^{2}) $$ 则其概率密度函数为 $$ \displaystyle f(x)={1 \over \sigma {\sqrt {2\pi }}},e^{-{(x-\mu )^{2} \over 2\sigma ^{2}}} $$
多元高斯分布 $$ X ∼ N(\mu,\Sigma) $$
$$ \begin{aligned} E[X] &= \mu \
Cov(X) &= \Sigma \end{aligned} $$ 概率密度函数为 $$ \begin{equation} p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \end{equation} $$ 其中$|\Sigma|$是$\Sigma$的行列式，$\Sigma$是协方差矩阵，而且是对称半正定的。"><meta itemprop=datePublished content="2017-03-12T11:09:00+00:00"><meta itemprop=dateModified content="2017-03-12T11:09:00+00:00"><meta itemprop=wordCount content="181"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="生成模型与高斯判别分析"><meta name=twitter:description content="生成模型与判别模型 判别模型 判别模型对P(y|x)或者说对y建模，直接学习得到y。 常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。
生成模型 生成模型对P(x|y)或者说对x建模，通过计算 $$ \begin{aligned} \arg\max\limits_{y}p(y\vert x) &= \arg\max\limits_y\frac{p(x\vert y)p(y)}{p(x)} \
&= \arg\max\limits_y p(x\vert y)p(y) \end{aligned} $$ 得到y。 因此给定x进行比较时，P(x)为固定值，所以P(x)可省略。 常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等。
 高斯分布 若随机变量 X, X服从一个位置参数为 $\mu$ 、尺度参数为$\sigma$ 的概率分布，记为： $$ \displaystyle X \sim N(\mu ,\sigma ^{2}) $$ 则其概率密度函数为 $$ \displaystyle f(x)={1 \over \sigma {\sqrt {2\pi }}},e^{-{(x-\mu )^{2} \over 2\sigma ^{2}}} $$
多元高斯分布 $$ X ∼ N(\mu,\Sigma) $$
$$ \begin{aligned} E[X] &= \mu \
Cov(X) &= \Sigma \end{aligned} $$ 概率密度函数为 $$ \begin{equation} p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \end{equation} $$ 其中$|\Sigma|$是$\Sigma$的行列式，$\Sigma$是协方差矩阵，而且是对称半正定的。"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>生成模型与高斯判别分析</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-03-12T11:09:00Z>2017-03-12</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/machine-learning.html rel=category>machine learning</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#生成模型与判别模型>生成模型与判别模型</a><ul><li><a href=#判别模型>判别模型</a></li><li><a href=#生成模型>生成模型</a></li></ul></li><li><a href=#高斯分布>高斯分布</a></li><li><a href=#多元高斯分布>多元高斯分布</a></li><li><a href=#joint似然与condition似然>Joint似然与Condition似然</a></li><li><a href=#高斯判别模型>高斯判别模型</a></li><li><a href=#logistic回归与高斯判别模型>Logistic回归与高斯判别模型</a></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=生成模型与判别模型>生成模型与判别模型</h2><h3 id=判别模型>判别模型</h3><p>判别模型对P(y|x)或者说对y建模，直接学习得到y。
常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。</p><h3 id=生成模型>生成模型</h3><p>生成模型对P(x|y)或者说对x建模，通过计算
$$
\begin{aligned}
\arg\max\limits_{y}p(y\vert x) &= \arg\max\limits_y\frac{p(x\vert y)p(y)}{p(x)} \<br>&= \arg\max\limits_y p(x\vert y)p(y)
\end{aligned}
$$
得到y。
因此给定x进行比较时，P(x)为固定值，所以P(x)可省略。
常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等。</p><hr><h2 id=高斯分布>高斯分布</h2><p>若随机变量 X, X服从一个位置参数为 $\mu$ 、尺度参数为$\sigma$ 的概率分布，记为：
$$
\displaystyle X \sim N(\mu ,\sigma ^{2})
$$
则其概率密度函数为
$$
\displaystyle f(x)={1 \over \sigma {\sqrt {2\pi }}},e^{-{(x-\mu )^{2} \over 2\sigma ^{2}}}
$$</p><h2 id=多元高斯分布>多元高斯分布</h2><p>$$
X ∼ N(\mu,\Sigma)
$$</p><p>$$
\begin{aligned}
E[X] &= \mu \<br>Cov(X) &= \Sigma
\end{aligned}
$$
概率密度函数为
$$
\begin{equation}
p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}
\end{equation}
$$
其中$|\Sigma|$是$\Sigma$的行列式，$\Sigma$是协方差矩阵，而且是对称半正定的。</p><hr><h2 id=joint似然与condition似然>Joint似然与Condition似然</h2><p>定义Condition似然
$$
L(\theta) = \prod_{i=1}^{m}P(y^{(i)}|x^{(i)})
$$</p><p>定义Joint似然
$$
L(\theta) = \prod_{i=1}^{m}P(x^{(i)},y^{(i)})
$$</p><hr><h2 id=高斯判别模型>高斯判别模型</h2><p>高斯判别分析模型假设P(X|Y)服从多元高斯分布,并且y本身是服从贝努力分布。</p><p>$$
\begin{aligned}
y &\sim Bernoulli(\phi) \<br>x\big\vert y &\sim \cal{N}(\mu_0,\Sigma) \<br>x\big\vert y &\sim \cal{N}(\mu_1,\Sigma)
\end{aligned}
$$</p><blockquote><p>意这里的参数有两个$\mu$，表示在不同的结果模型下，特征均值不同，但我们假设协方差$\Sigma$相同。反映在图上就是不同模型中心位置不同，但形状相同。这样就可以用直线来进行分隔判别。</p></blockquote><p>展开
$$
\begin{aligned}
p(y) &= \phi^y(1-\phi)^{1-y} \<br>p(x\vert y = 0) &= \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)} \<br>p(x\vert y = 1) &= \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)}
\end{aligned}
$$</p><p>joint对数似然函数为
$$
\begin{aligned}
\ell(\phi,\mu_0,\mu_1,\Sigma) &= \log\prod\limits_{i=1}^mp(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma) \<br>&= \log\prod\limits_{i=1}^mp(x^{(i)}\vert y^{(i)};\phi,\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)
\end{aligned}
$$</p><p>极大化joint对数似然函，经过推导可得：
$$
\begin{aligned}
\phi &= \frac{1}{m}I{y^{(i)}=1} \<br>\mu_0 &= \frac{\sum\limits_{i=1}^m1{y^{(i)}=0}x^{(i)}}{\sum\limits_{i=1}^m1{y^{(i)}=0}} \<br>\mu_1 &= \frac{\sum\limits_{i=1}^m1{y^{(i)}=1}x^{(i)}}{\sum\limits_{i=1}^m1{y^{(i)}=1}} \<br>\Sigma &= \frac{1}{m}\sum\limits_{i-1}^m(x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^T
\end{aligned}
$$</p><p>$\phi$是训练样本中结果y=1占有的比例。</p><p>$\mu_0$是y=0的样本中特征均值。</p><p>$\mu_0$是y=1的样本中特征均值。</p><p>$\Sigma$是样本特征方差均值。</p><h2 id=logistic回归与高斯判别模型>Logistic回归与高斯判别模型</h2><p>当x|y符合高斯分布，那么P(y=1|x)就是logsitic回归。但是反过来如果P(y=1|x)是logsitic回归，x|y不一定是高斯分布。例如当x|y符合泊松分布，那么P(y=1|x)也会是logsitic回归。</p><p>相比Logistic，高斯判别模型有更强的假设，即假设x|y符合高斯分布。也因此高斯判别模型需要更少的数据。</p><p>如果x|y=0,x|y=1,符合指数分布簇，P(y=1|x)都会是logsitic回归。</p><p>所以高斯判别模型的优点是所需要的数据少，但有很强的假设鲁棒性不高。
反之Logistic的优点鲁棒性高，但所需要的数据相对多。</p></div></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/machine-learning/2017-03-12-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>朴素贝叶斯</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/java/2017-05-13-guice%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>Guice 使用方法</p></a></div></nav><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c',repo:'writinglite.com',owner:'writinglite',admin:['writinglite'],id:location.pathname,distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>