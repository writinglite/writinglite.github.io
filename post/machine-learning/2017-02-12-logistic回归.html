<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Logistic回归 - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Logistic回归"><meta property="og:description" content="背景 logistic回归是统计学习中经典的分类方法，在深度学习也有很多应用。本文主要介绍logistic回归，然后将其推广到多分类问题-softmax回归。
什么是logistic 虽然名称中有回归，其实logistic回归模型是一个经典二分类模型。logistic回归在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数。
logstic回归模型的特点，一个事件的几率是指该事件发生的概率与不发生的概率的比值，如果事件发生的概率是p，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率为 $$ logit(p) = \log \frac{p}{1-p} $$ 对于logistic回归而言这个对数几率就是wx，也就是说线性函数的值越接近正无穷，概率值越接近1；线性函数的值越接近负无穷，概率值越接近0，这样的模型就是logistic回归模型。一句话概括的话，logistic回归模型实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。
$$ h(\theta) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}} $$ where $$ g(z) = \frac{1}{1+e^{-z}} $$
 $$ P(p=1|x;\theta) = h_\theta(x) \
P(p=0|x;\theta) = 1 - h_\theta(x) $$
因此 $$ P(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y} $$
所以似然函数为 $$ \begin{align} L(\theta) & = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) \\
& =\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\
& =\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \end{align} $$
对数似然函数 $$ \begin{align} l(\theta) & = \log L(\theta) \\
& = \sum_{i=1}^my^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)})) \end{align} $$"><meta property="og:type" content="article"><meta property="og:url" content="/post/machine-learning/2017-02-12-logistic%E5%9B%9E%E5%BD%92.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-07-19T21:41:00+00:00"><meta property="article:modified_time" content="2017-07-19T21:41:00+00:00"><meta itemprop=name content="Logistic回归"><meta itemprop=description content="背景 logistic回归是统计学习中经典的分类方法，在深度学习也有很多应用。本文主要介绍logistic回归，然后将其推广到多分类问题-softmax回归。
什么是logistic 虽然名称中有回归，其实logistic回归模型是一个经典二分类模型。logistic回归在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数。
logstic回归模型的特点，一个事件的几率是指该事件发生的概率与不发生的概率的比值，如果事件发生的概率是p，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率为 $$ logit(p) = \log \frac{p}{1-p} $$ 对于logistic回归而言这个对数几率就是wx，也就是说线性函数的值越接近正无穷，概率值越接近1；线性函数的值越接近负无穷，概率值越接近0，这样的模型就是logistic回归模型。一句话概括的话，logistic回归模型实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。
$$ h(\theta) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}} $$ where $$ g(z) = \frac{1}{1+e^{-z}} $$
 $$ P(p=1|x;\theta) = h_\theta(x) \
P(p=0|x;\theta) = 1 - h_\theta(x) $$
因此 $$ P(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y} $$
所以似然函数为 $$ \begin{align} L(\theta) & = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) \\
& =\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\
& =\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \end{align} $$
对数似然函数 $$ \begin{align} l(\theta) & = \log L(\theta) \\
& = \sum_{i=1}^my^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)})) \end{align} $$"><meta itemprop=datePublished content="2017-07-19T21:41:00+00:00"><meta itemprop=dateModified content="2017-07-19T21:41:00+00:00"><meta itemprop=wordCount content="226"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Logistic回归"><meta name=twitter:description content="背景 logistic回归是统计学习中经典的分类方法，在深度学习也有很多应用。本文主要介绍logistic回归，然后将其推广到多分类问题-softmax回归。
什么是logistic 虽然名称中有回归，其实logistic回归模型是一个经典二分类模型。logistic回归在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数。
logstic回归模型的特点，一个事件的几率是指该事件发生的概率与不发生的概率的比值，如果事件发生的概率是p，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率为 $$ logit(p) = \log \frac{p}{1-p} $$ 对于logistic回归而言这个对数几率就是wx，也就是说线性函数的值越接近正无穷，概率值越接近1；线性函数的值越接近负无穷，概率值越接近0，这样的模型就是logistic回归模型。一句话概括的话，logistic回归模型实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。
$$ h(\theta) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}} $$ where $$ g(z) = \frac{1}{1+e^{-z}} $$
 $$ P(p=1|x;\theta) = h_\theta(x) \
P(p=0|x;\theta) = 1 - h_\theta(x) $$
因此 $$ P(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y} $$
所以似然函数为 $$ \begin{align} L(\theta) & = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) \\
& =\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\
& =\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \end{align} $$
对数似然函数 $$ \begin{align} l(\theta) & = \log L(\theta) \\
& = \sum_{i=1}^my^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)})) \end{align} $$"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Logistic回归</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-07-19T21:41:00Z>2017-07-19</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/machine-learning.html rel=category>machine learning</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#背景>背景</a></li><li><a href=#什么是logistic>什么是logistic</a></li><li><a href=#softmax>softmax</a></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=背景>背景</h2><p>logistic回归是统计学习中经典的分类方法，在深度学习也有很多应用。本文主要介绍logistic回归，然后将其推广到多分类问题-softmax回归。</p><h2 id=什么是logistic>什么是logistic</h2><p>虽然名称中有回归，其实logistic回归模型是一个经典二分类模型。logistic回归在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数。</p><p>logstic回归模型的特点，一个事件的几率是指该事件发生的概率与不发生的概率的比值，如果事件发生的概率是p，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率为
$$
logit(p) = \log \frac{p}{1-p}
$$
对于logistic回归而言这个对数几率就是wx，也就是说线性函数的值越接近正无穷，概率值越接近1；线性函数的值越接近负无穷，概率值越接近0，这样的模型就是logistic回归模型。一句话概括的话，logistic回归模型实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。</p><p>$$
h(\theta) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}
$$
where
$$
g(z) = \frac{1}{1+e^{-z}}
$$</p><hr><p>$$
P(p=1|x;\theta) = h_\theta(x) \<br>P(p=0|x;\theta) = 1 - h_\theta(x)
$$</p><p>因此
$$
P(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}
$$</p><p>所以似然函数为
$$
\begin{align}
L(\theta) & = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) \\<br>& =\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\<br>& =\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}
\end{align}
$$</p><p>对数似然函数
$$
\begin{align}
l(\theta) & = \log L(\theta) \\<br>& = \sum_{i=1}^my^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)}))
\end{align}
$$</p><p>$l(\theta)$梯度：
$$
\begin{align}
\frac{\partial}{\partial\theta_j}l(\theta) & = \left(y\frac{1}{g(\theta^T)}-(1-y)\frac{1}{1-g(\theta^T)}\right)\frac{\partial}{\partial\theta_j}g(\theta^Tx) \\<br>& = \left(y\frac{1}{g(\theta^T)}-(1-y)\frac{1}{1-g(\theta^T)}\right)g(\theta^Tx)g(1-\theta^Tx)\frac{\partial}{\partial\theta_j}\theta^Tx \\<br>& = (y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j \\<br>& = (y-h_\theta(x))x_j
\end{align}
$$</p><p>对了求最大似然，使用梯度上升算法：
$$
\theta_j := \theta_j+ \alpha(y^i-h_\theta(x^i))x_j^i
$$</p><h2 id=softmax>softmax</h2><p>对于类别数目为k的分类问题，softmax为
$$
h_{\theta}(x_{(i)}) = \begin{matrix}
p(y_{(i)}=1|x_{(i)};\theta) \<br>p(y_{(i)}=2|x_{(i)};\theta) \<br>\vdots \<br>p(y_{(i)}=k|x_{(i)};\theta)
\end{matrix}
=\frac{1}{\sum_{j=1}^ke^{\theta_j^Tx^{(i)}}}
\begin{matrix}
e^{\theta_1^Tx^{(i)}}\<br>e^{\theta_2^Tx^{(i)}}\<br>\vdots \<br>e^{\theta_3^Tx^{(i)}}\<br>\end{matrix}
$$
$\theta_1,\theta_2,\dots,\theta_k$都是模型参数,其中 $\frac{1}{\sum_{j=1}^ke^{\theta_j^Tx^{(i)}}}$是对概率分布做归一化。</p><p>softmax的似然函数为
$$
\begin{align}
L(\theta) & = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) \\<br>& =\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\<br>& =\prod_{i=1}^m\prod_{j=1}^k(\frac{e^{\theta_j^Tx^{(i)}}}{\sum_{j=1}^ke^{\theta_j^Tx^{(i)}}})^{I{y^{(i)}=j}}
\end{align}
$$
其中$I{y^{(i)}=j}$是指示函数，当表达式成立值为1否则为0。因此可以得到log似然函数</p><p>$$
\begin{align}
l(\theta) & = \log L(\theta) \\<br>& = \sum_{i=1}^m \sum_{j=1}^k I{y^{(i)}=j} \log p(y_{(i)}=j|x_{(i)};\theta)
\end{align}
$$
可以看到，Softmax代价函数与logistic 代价函数在形式上非常类似，只是在Softmax损失函数中对类标记的 k 个可能值进行了累加。</p><p>Softmax回归模型参数化的特点
oftmax 回归有一个不寻常的特点：它有一个“冗余”的参数集。直观上来解释，如果要进行k个分类，只需要k-1个参数即可，因为要做归一化，可以用1减其余的概率即可得到最后一个的概率，其实logistic回归就是这样做的。</p><p>Softmax回归与Logistic 回归的关系
将k设为2
$$
h_{\theta}(x_{(i)}) =\frac{1}{e^{\theta_1^Tx^{(i)}} + e^{\theta_2^Tx^{(i)}}}
\begin{matrix}
e^{\theta_1^Tx^{(i)}}\<br>e^{\theta_2^Tx^{(i)}}\<br>\end{matrix}
$$
利用softmax回归参数冗余的特点，从两个参数向量中都减去向量$\theta_1$，得到:</p><p>Softmax 回归 vs. k 个二元分类器
果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？
这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。）
如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。
现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？
在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。</p><hr><p><a href=http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92>http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92</a>
《统计学习方法》
《机器学习》周志华</p></div></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/machine-learning/2017-07-15-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>集成学习简单介绍</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/machine-learning/2017-07-19-%E5%86%B3%E7%AD%96%E6%A0%91.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>决策树</p></a></div></nav><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c',repo:'writinglite.github.io',owner:'writinglite',admin:['writinglite'],id:decodeURI(location.pathname.split("/").pop()).substring(0,49),distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>