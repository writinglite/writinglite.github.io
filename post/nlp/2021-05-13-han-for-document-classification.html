<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>HAN for Document Classification - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="HAN for Document Classification"><meta property="og:description" content="论文链接：Hierarchical Attention Networks for Document Classification

HAN (Hierarchical Attention Networks for Document Classification) 是一个针对文本分类任务的层次化 attention 模型。它有个显著的特点:通过&#34;词-句子-文章&#34;的层次化结构来表示一篇文本。
HAN 模型的灵感来源于人在阅读 document 的时候，不同的词和句子对人理解 document 信息有不同的影响。因为，词和句子的重要性是和上下文息息相关的，即使是相同的词和句子，在不同的上下文中重要性也不一样。人在阅读一篇文章时，对 document 不同的内容是有着不同的注意度的。
attention 的目的是要把一个句子中，对句子的含义影响最大的词语找出来。
论文通过将 $h_{it}$ 输入到一个 dense 网络中得到的结果$u_{it}$ 作为$h_{it}$的隐含表示。$h_{it}$可以是双向RNN的结果，或者bert中经过encoder后的结果。 \( u_{it}=\tanh(W_wh_{it}+b_w) \) 为了衡量单词的重要性，这里用$u_{it}$ 和一个随机初始化的上下文向量$uw$ 的相似度来表示，然后经过 softmax 操作获得了一个归一化的 attention 权重矩阵 $\alpha{it}$，代表句子$i$中第 $t$个词的权重。 \( \alpha_{it}=\frac{\exp(u^T_{it}u_w)}{\sum_t(\exp(u^T_{it}u_w))} \) 得到了 attention 权重矩阵之后，句子向量 $s_i$ 可以看作这些词向量的加权求和。这里的上下文向量$u_w$ 是在训练网络的过程中学习获得的。我们可以把 $u_w$当作一种询问的高级表示，比如&#34;哪些词含有比较重要的信息? \( s_i=\sum_t\alpha_{it}h_{it} \)
使用这个句子向量就可以做句子level的任务，例如文本分类等。
 主要查考：https://zhuanlan.zhihu.com/p/54165155"><meta property="og:type" content="article"><meta property="og:url" content="/post/nlp/2021-05-13-han-for-document-classification.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2021-05-12T16:18:00+00:00"><meta property="article:modified_time" content="2021-05-12T16:18:00+00:00"><meta itemprop=name content="HAN for Document Classification"><meta itemprop=description content="论文链接：Hierarchical Attention Networks for Document Classification

HAN (Hierarchical Attention Networks for Document Classification) 是一个针对文本分类任务的层次化 attention 模型。它有个显著的特点:通过&#34;词-句子-文章&#34;的层次化结构来表示一篇文本。
HAN 模型的灵感来源于人在阅读 document 的时候，不同的词和句子对人理解 document 信息有不同的影响。因为，词和句子的重要性是和上下文息息相关的，即使是相同的词和句子，在不同的上下文中重要性也不一样。人在阅读一篇文章时，对 document 不同的内容是有着不同的注意度的。
attention 的目的是要把一个句子中，对句子的含义影响最大的词语找出来。
论文通过将 $h_{it}$ 输入到一个 dense 网络中得到的结果$u_{it}$ 作为$h_{it}$的隐含表示。$h_{it}$可以是双向RNN的结果，或者bert中经过encoder后的结果。 \( u_{it}=\tanh(W_wh_{it}+b_w) \) 为了衡量单词的重要性，这里用$u_{it}$ 和一个随机初始化的上下文向量$uw$ 的相似度来表示，然后经过 softmax 操作获得了一个归一化的 attention 权重矩阵 $\alpha{it}$，代表句子$i$中第 $t$个词的权重。 \( \alpha_{it}=\frac{\exp(u^T_{it}u_w)}{\sum_t(\exp(u^T_{it}u_w))} \) 得到了 attention 权重矩阵之后，句子向量 $s_i$ 可以看作这些词向量的加权求和。这里的上下文向量$u_w$ 是在训练网络的过程中学习获得的。我们可以把 $u_w$当作一种询问的高级表示，比如&#34;哪些词含有比较重要的信息? \( s_i=\sum_t\alpha_{it}h_{it} \)
使用这个句子向量就可以做句子level的任务，例如文本分类等。
 主要查考：https://zhuanlan.zhihu.com/p/54165155"><meta itemprop=datePublished content="2021-05-12T16:18:00+00:00"><meta itemprop=dateModified content="2021-05-12T16:18:00+00:00"><meta itemprop=wordCount content="59"><meta itemprop=keywords content="自然语言处理,Deep Learning,"><meta name=twitter:card content="summary"><meta name=twitter:title content="HAN for Document Classification"><meta name=twitter:description content="论文链接：Hierarchical Attention Networks for Document Classification

HAN (Hierarchical Attention Networks for Document Classification) 是一个针对文本分类任务的层次化 attention 模型。它有个显著的特点:通过&#34;词-句子-文章&#34;的层次化结构来表示一篇文本。
HAN 模型的灵感来源于人在阅读 document 的时候，不同的词和句子对人理解 document 信息有不同的影响。因为，词和句子的重要性是和上下文息息相关的，即使是相同的词和句子，在不同的上下文中重要性也不一样。人在阅读一篇文章时，对 document 不同的内容是有着不同的注意度的。
attention 的目的是要把一个句子中，对句子的含义影响最大的词语找出来。
论文通过将 $h_{it}$ 输入到一个 dense 网络中得到的结果$u_{it}$ 作为$h_{it}$的隐含表示。$h_{it}$可以是双向RNN的结果，或者bert中经过encoder后的结果。 \( u_{it}=\tanh(W_wh_{it}+b_w) \) 为了衡量单词的重要性，这里用$u_{it}$ 和一个随机初始化的上下文向量$uw$ 的相似度来表示，然后经过 softmax 操作获得了一个归一化的 attention 权重矩阵 $\alpha{it}$，代表句子$i$中第 $t$个词的权重。 \( \alpha_{it}=\frac{\exp(u^T_{it}u_w)}{\sum_t(\exp(u^T_{it}u_w))} \) 得到了 attention 权重矩阵之后，句子向量 $s_i$ 可以看作这些词向量的加权求和。这里的上下文向量$u_w$ 是在训练网络的过程中学习获得的。我们可以把 $u_w$当作一种询问的高级表示，比如&#34;哪些词含有比较重要的信息? \( s_i=\sum_t\alpha_{it}h_{it} \)
使用这个句子向量就可以做句子level的任务，例如文本分类等。
 主要查考：https://zhuanlan.zhihu.com/p/54165155"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/archives/><span class=menu__text>archives</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>HAN for Document Classification</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2021-05-12T16:18:00Z>2021-05-12</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/nlp.html rel=category>NLP</a></span></div></div></header><div class="content post__content clearfix"><p>论文链接：<a href="https://link.zhihu.com/?target=https%3A//www.cs.cmu.edu/%7Ediyiy/docs/naacl16.pdf">Hierarchical Attention Networks for Document Classification</a></p><p><figure><img src=2021-05-13-HAN%20for%20Document%20Classification.assets/image-20201211111122727.png alt=image-20201211111122727></figure></p><p><strong>HAN (Hierarchical Attention Networks for Document Classification)</strong> 是一个针对文本分类任务的层次化 attention 模型。它有个显著的特点:通过"词-句子-文章"的层次化结构来表示一篇文本。</p><p><strong>HAN</strong> 模型的灵感来源于人在阅读 document 的时候，不同的词和句子对人理解 document 信息有不同的影响。因为，词和句子的重要性是和上下文息息相关的，即使是相同的词和句子，在不同的上下文中重要性也不一样。人在阅读一篇文章时，对 document 不同的内容是有着不同的注意度的。</p><p>attention 的目的是要把一个句子中，对句子的含义影响最大的词语找出来。</p><p>论文通过将 $h_{it}$ 输入到一个 dense 网络中得到的结果$u_{it}$ 作为$h_{it}$的隐含表示。<strong>$h_{it}$可以是双向RNN的结果，或者bert中经过encoder后的结果。</strong>
<span class=math>\(
u_{it}=\tanh(W_wh_{it}+b_w)
\)</span>
为了衡量单词的重要性，这里用$u_{it}$ 和一个随机初始化的上下文向量$u<em>w$ 的相似度来表示，然后经过 softmax 操作获得了一个归一化的 attention 权重矩阵 $\alpha</em>{it}$，代表句子$i$中第 $t$个词的权重。
<span class=math>\(
\alpha_{it}=\frac{\exp(u^T_{it}u_w)}{\sum_t(\exp(u^T_{it}u_w))}
\)</span>
得到了 attention 权重矩阵之后，句子向量 $s_i$ 可以看作这些词向量的加权求和。这里的上下文向量$u_w$ 是在训练网络的过程中学习获得的。我们可以把 $u_w$当作一种询问的高级表示，比如"哪些词含有比较重要的信息?
<span class=math>\(
s_i=\sum_t\alpha_{it}h_{it}
\)</span></p><p>使用这个句子向量就可以做句子level的任务，例如文本分类等。</p><hr><p>主要查考：<a href=https://zhuanlan.zhihu.com/p/54165155>https://zhuanlan.zhihu.com/p/54165155</a></p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/ rel=tag>自然语言处理</a></li><li class=tags__item><a class="tags__link btn" href=/tags/deep-learning/ rel=tag>Deep Learning</a></li></ul></div></footer></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/other/2021-05-06-%E4%BD%BF%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%88%B0github-pages.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>使用GitHub Actions自动部署hugo到GitHub Pages</p></a></div></nav><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c',repo:'writinglite.github.io',owner:'writinglite',admin:['writinglite'],id:decodeURI(location.pathname.split("/").pop()).replace(".html","").substring(0,49),distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>