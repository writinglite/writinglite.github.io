<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>最小二乘回归 - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="最小二乘回归"><meta property="og:description" content="最小二乘线性回归与梯度下降算法 线性回归 预测函数： $$ h_\theta(x)=\theta_0 + \theta_1x_1 + \theta_2x_2 $$
将$x_0 = 1$
$$ h_\theta(x) = \sum_{i=0}^n\theta_ix_i $$
其中 $\theta$这参数，n表示特征数量
 成本函数： $$ J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 $$
其中m表示样本数量，$(x^{(i)},y^{(i)})$表示第i个样本对儿，1/2是为了方便计算
 我们目标： $$ \min_\theta J(\theta) $$
最小二乘成本函数的概率学解释（并不是唯一的解释） 假设该线性模型符合高斯分布 $$ p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\sqrt{(y^{(i)}-\theta^Tx^{(i)^2})}{2\sigma^2}\right) $$
似然函数为： $$ L(\theta) = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) $$ 因此，线性回归的似然函数： $$ \begin{align} L(\theta) & = \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \
& = \prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{((y^{(i)}) - \theta^Tx^{(i)})^2}{2\sigma^2}\right) \end{align} $$
对数似然度为 $$ \begin{align} l(\theta) & = \log L(\theta)\"><meta property="og:type" content="article"><meta property="og:url" content="/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017-01-12-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%9B%9E%E5%BD%92.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-01-12T00:00:00+00:00"><meta property="article:modified_time" content="2017-01-12T00:00:00+00:00"><meta itemprop=name content="最小二乘回归"><meta itemprop=description content="最小二乘线性回归与梯度下降算法 线性回归 预测函数： $$ h_\theta(x)=\theta_0 + \theta_1x_1 + \theta_2x_2 $$
将$x_0 = 1$
$$ h_\theta(x) = \sum_{i=0}^n\theta_ix_i $$
其中 $\theta$这参数，n表示特征数量
 成本函数： $$ J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 $$
其中m表示样本数量，$(x^{(i)},y^{(i)})$表示第i个样本对儿，1/2是为了方便计算
 我们目标： $$ \min_\theta J(\theta) $$
最小二乘成本函数的概率学解释（并不是唯一的解释） 假设该线性模型符合高斯分布 $$ p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\sqrt{(y^{(i)}-\theta^Tx^{(i)^2})}{2\sigma^2}\right) $$
似然函数为： $$ L(\theta) = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) $$ 因此，线性回归的似然函数： $$ \begin{align} L(\theta) & = \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \
& = \prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{((y^{(i)}) - \theta^Tx^{(i)})^2}{2\sigma^2}\right) \end{align} $$
对数似然度为 $$ \begin{align} l(\theta) & = \log L(\theta)\"><meta itemprop=datePublished content="2017-01-12T00:00:00+00:00"><meta itemprop=dateModified content="2017-01-12T00:00:00+00:00"><meta itemprop=wordCount content="310"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="最小二乘回归"><meta name=twitter:description content="最小二乘线性回归与梯度下降算法 线性回归 预测函数： $$ h_\theta(x)=\theta_0 + \theta_1x_1 + \theta_2x_2 $$
将$x_0 = 1$
$$ h_\theta(x) = \sum_{i=0}^n\theta_ix_i $$
其中 $\theta$这参数，n表示特征数量
 成本函数： $$ J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 $$
其中m表示样本数量，$(x^{(i)},y^{(i)})$表示第i个样本对儿，1/2是为了方便计算
 我们目标： $$ \min_\theta J(\theta) $$
最小二乘成本函数的概率学解释（并不是唯一的解释） 假设该线性模型符合高斯分布 $$ p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\sqrt{(y^{(i)}-\theta^Tx^{(i)^2})}{2\sigma^2}\right) $$
似然函数为： $$ L(\theta) = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) $$ 因此，线性回归的似然函数： $$ \begin{align} L(\theta) & = \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \
& = \prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{((y^{(i)}) - \theta^Tx^{(i)})^2}{2\sigma^2}\right) \end{align} $$
对数似然度为 $$ \begin{align} l(\theta) & = \log L(\theta)\"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/archives/><span class=menu__text>archives</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>最小二乘回归</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-01-12T00:00:00Z>2017-01-12</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/machine-learning.html rel=category>machine learning</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#线性回归>线性回归</a></li><li><a href=#最小二乘成本函数的概率学解释并不是唯一的解释>最小二乘成本函数的概率学解释（并不是唯一的解释）</a></li><li><a href=#梯度下降>梯度下降</a></li><li><a href=#批梯度下降算法代码>批梯度下降算法代码</a></li></ul></nav></div></div><div class="content post__content clearfix"><h1 id=最小二乘线性回归与梯度下降算法>最小二乘线性回归与梯度下降算法</h1><h2 id=线性回归>线性回归</h2><p>预测函数：
$$
h_\theta(x)=\theta_0 + \theta_1x_1 + \theta_2x_2
$$</p><p>将$x_0 = 1$</p><p>$$
h_\theta(x) = \sum_{i=0}^n\theta_ix_i
$$</p><p>其中 $\theta$这参数，n表示特征数量</p><hr><p>成本函数：
$$
J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2
$$</p><p>其中m表示样本数量，$(x^{(i)},y^{(i)})$表示第i个样本对儿，1/2是为了方便计算</p><hr><p>我们目标：
$$
\min_\theta J(\theta)
$$</p><h2 id=最小二乘成本函数的概率学解释并不是唯一的解释>最小二乘成本函数的概率学解释（并不是唯一的解释）</h2><p>假设该线性模型符合高斯分布
$$
p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\sqrt{(y^{(i)}-\theta^Tx^{(i)^2})}{2\sigma^2}\right)
$$</p><p>似然函数为：
$$
L(\theta) = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ)
$$
因此，线性回归的似然函数：
$$
\begin{align}
L(\theta) & = \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \<br>& = \prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{((y^{(i)}) - \theta^Tx^{(i)})^2}{2\sigma^2}\right)
\end{align}
$$</p><p>对数似然度为
$$
\begin{align}
l(\theta) & = \log L(\theta)\<br>& = \log\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{((y^{(i)}) - \theta^Tx^{(i)})^2}{2\sigma^2}\right)\<br>& = \sum_{i=1}^m\log\frac{1}{{2\sigma^2}}\exp\left(-\frac{((y^{(i)}) - \theta^Tx^{(i)})^2}{2\sigma^2}\right)\<br>& = m\log\frac{1}{2\pi\sigma}-\frac{1}{\sigma^2}\cdot\frac{1}{2}\sum_{i=1}^m(y^{(i)} - \theta^Tx^{(i)})^2
\end{align}
$$
最大化$l(\theta)$相当于最小化，也就是成本函数
$$
\frac{1}{2}\sum_{i=1}^m(y^{(i)} - \theta^Tx^{(i)})^2
$$</p><h2 id=梯度下降>梯度下降</h2><p>步骤</p><ol><li>初始化$\theta$</li><li>改变$\theta$使$J(\theta)$降低</li><li>终止</li></ol><p>每一步使：
$$
\theta_i:=\theta_i-\alpha\frac{\partial}{\partial\theta_i}J(\theta)
$$</p><p>$\theta_i$表示第个参数，:=表示赋值符号，$\alpha$表示步长也叫学习速率，$\frac{\partial}{\partial\theta_i}$是对$\theta_i$求偏导数</p><p>对于单个样本：</p><p>$$
\begin{align}
\frac{\partial}{\partial\theta_i}J(\theta) & = \frac{\partial}{\partial\theta_i}\frac{1}{2}(h_\theta(x) - y)^2 \<br>& = 2\cdot\frac{1}{2}(h_\theta(x) - y) \cdot \frac{\partial}{\partial\theta_i}(h_\theta(x)-y) \<br>& = (h_\theta(x) - y)\cdot\frac{\partial}{\partial\theta_i}(\theta_0x_0+\theta_1x_1+&mldr; + \theta_nx_n-y) \
& = (h_\theta(x) - y)\cdot x_i
\end{align}
$$</p><p>(1)-(2) 根据链式求导法则，(3)-(4)$(\theta_0x_0+\theta_1x_1+&mldr; + \theta_nx_n-y)$只有$\theta_i$起作用
因此</p><p>$$
\theta_i:=\theta_i-\alpha(h_\theta(x) - y)\cdot x_i
$$</p><hr><p>对于多个样本：</p><p>$$
\theta_i:=\theta_i-\sum_{j=1}^m\alpha(h_\theta(x^{(j)}) - y^{(j)})\cdot x_i^{(j)}
$$</p><h2 id=批梯度下降算法代码>批梯度下降算法代码</h2><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=color:#66d9ef>public</span> <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>main</span><span style=color:#f92672>(</span>String<span style=color:#f92672>[]</span> args<span style=color:#f92672>)</span> <span style=color:#f92672>{</span>
    <span style=color:#75715e>//训练集合输入值
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>double</span> x<span style=color:#f92672>[][]</span> <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
            <span style=color:#f92672>{</span>1<span style=color:#f92672>,</span> 4<span style=color:#f92672>},</span>
            <span style=color:#f92672>{</span>2<span style=color:#f92672>,</span> 5<span style=color:#f92672>},</span>
            <span style=color:#f92672>{</span>5<span style=color:#f92672>,</span> 1<span style=color:#f92672>},</span>
            <span style=color:#f92672>{</span>4<span style=color:#f92672>,</span> 2<span style=color:#f92672>}};</span>
    <span style=color:#75715e>//训练集合期望输出值
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>double</span> y<span style=color:#f92672>[]</span> <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>
            19<span style=color:#f92672>,</span>
            26<span style=color:#f92672>,</span>
            19<span style=color:#f92672>,</span>
            20<span style=color:#f92672>};</span>
    <span style=color:#75715e>//参数
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>double</span> theta<span style=color:#f92672>[]</span> <span style=color:#f92672>=</span> <span style=color:#f92672>{</span>0<span style=color:#f92672>,</span> 0<span style=color:#f92672>};</span>
    <span style=color:#75715e>//步长
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>double</span> learningRate <span style=color:#f92672>=</span> 0<span style=color:#f92672>.</span><span style=color:#a6e22e>01</span><span style=color:#f92672>;</span>
    <span style=color:#75715e>//样本集合长度
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>int</span> m <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span><span style=color:#a6e22e>length</span><span style=color:#f92672>;</span>
    <span style=color:#75715e>//迭代次数
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>int</span> iteration <span style=color:#f92672>=</span> 100<span style=color:#f92672>;</span>
    <span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>int</span> index <span style=color:#f92672>=</span> 0<span style=color:#f92672>;</span> index <span style=color:#f92672>&lt;</span> iteration<span style=color:#f92672>;</span> index<span style=color:#f92672>++)</span> <span style=color:#f92672>{</span>
        <span style=color:#66d9ef>double</span> err_sum <span style=color:#f92672>=</span> 0<span style=color:#f92672>;</span>
        <span style=color:#66d9ef>double</span><span style=color:#f92672>[]</span> tmp <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> <span style=color:#66d9ef>double</span><span style=color:#f92672>[</span>theta<span style=color:#f92672>.</span><span style=color:#a6e22e>length</span><span style=color:#f92672>];</span>
        System<span style=color:#f92672>.</span><span style=color:#a6e22e>arraycopy</span><span style=color:#f92672>(</span>theta<span style=color:#f92672>,</span> 0<span style=color:#f92672>,</span> tmp<span style=color:#f92672>,</span> 0<span style=color:#f92672>,</span> theta<span style=color:#f92672>.</span><span style=color:#a6e22e>length</span><span style=color:#f92672>);</span>
        <span style=color:#75715e>//i-&gt;m
</span><span style=color:#75715e></span>        <span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>int</span> j <span style=color:#f92672>=</span> 0<span style=color:#f92672>;</span> j <span style=color:#f92672>&lt;</span> m<span style=color:#f92672>;</span> j<span style=color:#f92672>++)</span> <span style=color:#f92672>{</span>
            <span style=color:#66d9ef>double</span> h <span style=color:#f92672>=</span> 0<span style=color:#f92672>;</span>
            <span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> 0<span style=color:#f92672>;</span> i <span style=color:#f92672>&lt;</span> theta<span style=color:#f92672>.</span><span style=color:#a6e22e>length</span><span style=color:#f92672>;</span> i<span style=color:#f92672>++)</span> <span style=color:#f92672>{</span><span style=color:#75715e>//x[j][i]表示第j个样本的第i个特征
</span><span style=color:#75715e></span>                h <span style=color:#f92672>+=</span> x<span style=color:#f92672>[</span>j<span style=color:#f92672>][</span>i<span style=color:#f92672>]</span> <span style=color:#f92672>*</span> theta<span style=color:#f92672>[</span>i<span style=color:#f92672>];</span>
            <span style=color:#f92672>}</span>
            err_sum <span style=color:#f92672>=</span> h <span style=color:#f92672>-</span> y<span style=color:#f92672>[</span>j<span style=color:#f92672>];</span>
            <span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> 0<span style=color:#f92672>;</span> i <span style=color:#f92672>&lt;</span> theta<span style=color:#f92672>.</span><span style=color:#a6e22e>length</span><span style=color:#f92672>;</span> i<span style=color:#f92672>++)</span> <span style=color:#f92672>{</span>
                tmp<span style=color:#f92672>[</span>i<span style=color:#f92672>]</span> <span style=color:#f92672>-=</span> learningRate <span style=color:#f92672>*</span> err_sum <span style=color:#f92672>*</span> x<span style=color:#f92672>[</span>j<span style=color:#f92672>][</span>i<span style=color:#f92672>];</span>
            <span style=color:#f92672>}</span>
        <span style=color:#f92672>}</span>
        <span style=color:#75715e>//批量更新参数
</span><span style=color:#75715e></span>        theta <span style=color:#f92672>=</span> tmp<span style=color:#f92672>;</span>
    <span style=color:#f92672>}</span>
    <span style=color:#66d9ef>for</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> 0<span style=color:#f92672>;</span> i <span style=color:#f92672>&lt;</span> theta<span style=color:#f92672>.</span><span style=color:#a6e22e>length</span><span style=color:#f92672>;</span> i<span style=color:#f92672>++)</span> <span style=color:#f92672>{</span>
        System<span style=color:#f92672>.</span><span style=color:#a6e22e>out</span><span style=color:#f92672>.</span><span style=color:#a6e22e>println</span><span style=color:#f92672>(</span><span style=color:#e6db74>&#34;theta&#34;</span> <span style=color:#f92672>+</span> i <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;=&#34;</span> <span style=color:#f92672>+</span> theta<span style=color:#f92672>[</span>i<span style=color:#f92672>]);</span>
    <span style=color:#f92672>}</span>
<span style=color:#f92672>}</span>
</code></pre></div></div></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017-01-07-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>统计学习方法</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/%E6%95%B0%E5%AD%A6/2017-01-12-%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%95%B0%E6%8D%AE-%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>程序员的数据-概率统计</p></a></div></nav><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:'ffbd91469959056415a6',clientSecret:'c7348e2aa3bf5ecd634acef0df3344e3e039eb9c',repo:'writinglite.github.io',owner:'writinglite',admin:['writinglite'],id:decodeURI(location.pathname.split("/").pop()).replace(".html","").substring(0,49),distractionFreeMode:!1});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('gitalk-container').innerHTML='Gitalk comments not available by default when the website is previewed locally.';return}gitalk.render('gitalk-container')})()</script></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>