<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>machine learning - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="machine learning"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="writinglite.com/categories/machine-learning/"><meta itemprop=name content="machine learning"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="machine learning"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/writinglite.com/css/style.css><link rel=stylesheet href=/writinglite.com/css/custom.css><link rel=alternate type=application/rss+xml href=writinglite.com/categories/machine-learning/index.xml title="Writing Lite"><link rel="shortcut icon" href=/writinglite.com/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/writinglite.com title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><header class=main__header><h1 class=main__title>machine learning</h1></header><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2016-11-27-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/ rel=bookmark>最大熵模型</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2016-11-27T09:03:04Z>2016-11-27</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">最大熵原理 我们的预测应当满足全部已知条件，而对未知的情况不要做任何主观假设。也可以表述为在满足约束条件的模型集体合选取熵最大的模型。
假设现在需要做一个自动将英语到法语的翻译模型，为了方便说明，我们将这个问题简化为将英文句子中的单词{in}翻译成法语词汇。那么翻译模型p就是对于给定包含单词”in”的英文句子，需要给出选择某个法语单词f 做为”in”的翻译结果的概率p(f)。为了帮助开发这个模型，需要收集大量已经翻译好的样本数据。收集好样本之后，接下来需要做两件事情：一是从样本中抽取规则（特征），二是基于这些规则建立模型。 从样本中我们能得到的第一个规则就是in可能被翻译成的法语词汇有： {dans, en, à, au cours de, pendant}。 也就是说，我们可以给模型p施加第一个约束条件： p(dans)+p(en)+ p(à)+p(au cours de)+p(pendant) = 1。 这个等式是翻译模型可以用到的第一个对样本的统计信息。显然，有无数可以满足上面约束的模型p可供选择，例如： p(dans)=1，即这个模型总是预测dans 或者 p(pendant)=1/2 and p(à)=1/2，即模型要么选择预测pendant，要么预测à。 这两个模型都只是在没有足够经验数据的情况下，做的大胆假设。事实上我们只知道当前可能的选项是5个法语词汇，没法确定究竟哪个概率分布式正确。那么，一个更合理的模型假设可能是： p(dans) = 1/5 p(en) = 1/5 p(à) = 1/5 p(au cours de) = 1/5 p(pendant) = 1/5 即该模型将概率均等地分给5个词汇。但现实情况下，肯定不会这么简单，所以我们尝试收集更多的经验知识。假设我们从语料中发现有30%的情况下，in会被翻译成dans 或者en，那么运用这个知识来更新我们的模型，得到2模型约束： p(dans) + p(en) = 3/10 p(dans)+p(en)+ p(à)+p(au cours de)+p(pendant) = 1 同样，还是有很多概率分布满足这两个约束。在没有其他知识的情况下，最直观的模型p应该是最均匀的模型（例如，我拿出一个色子问你丢出5的概率是多少，你肯定会回答1/6），也就是在满足约束条件的情况下，将概率均等分配： p(dans) = 3/20 p(en) = 3/20 p(à) = 7/30 p(au cours de) = 7/30 p(pendant) = 7/30 假设我们再一次观察样本数据，发现：有一半的情况，in被翻译成了dans 或 à。这样，我们有就了3个模型约束： p(dans) + p(en) = 3/10 p(dans)+p(en)+ p(à)+p(au cours de)+p(pendant) = 1 p(dans)+ p(à)=1/2 我们可以再一次选择满足3个约束的最均匀的模型p，但这一次结果没有那么明显。由于经验知识的增加，问题的复杂度也增加了，归结起来，我们要解决两组问题：第一，均匀(uniform)究竟是什么意思?</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2016-11-27-em%E7%AE%97%E6%B3%95/ rel=bookmark>EM算法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">EM算法和最大似然估计一样是一种参数估计方法，与最大似然估计不同的是EM算法可以对着包含隐变量的数据进行参数估计。EM算法的思想是：若参数$\Theta$已知，则可根据训练数据推断出隐变量Z的值（E步）；反之，若Z的值已知，则可方便地对参数$\Theta$做极大似然估计（M步）。
Jensen不等式 令f(x)是一个凸函数(e.g f''(x)>=0,二阶导数大于0)，令x为随机变量。 那么， $$ f(E[x])&lt;=E[f(x)] $$ 用一句话表达Jensen不等式，当函数是凸函数，那么该函数的期望大于等于期望的函数值。当X=E(X),当X为常量概率为1，E[f(x)] = f(E[x])。
如图，有0.5的概率是a，有0.5的概率是b。（就像掷硬币一样）。X的期望值就是a和b的中值了。
同理，对于凹函数，f''&lt;=0,$f(E[x])>=E[f(x)]$。
##EM算法 假定有训练数据集 $$ { x^{(1)} , x^{(2)} , x^{(3)} \dots x^{(m)} } $$ 样本相互独立，我们想找到每个样例隐含的类别z。 模型$P(x,z;\theta)$,只能观测到x，对数似然函数， $$ \begin{align} l(\theta) &= \sum^m_{i=1}\log P(x^i;\theta) \
&= \sum^m_{i=1}\log \sum_{z^i} P(x^i,z^i;\theta) \end{align} $$ 然后我们求极大似然 $$ \begin{align} \sum^m_{i=1}\log \sum_{z^i} P(x^i,z^i;\theta) & = \sum_i\log\sum_{z^{(i)}}P(x^{(i)},z^{(i)};\theta) \
& = \sum_i \log \sum_{z^{(i)}} Q(z^{(i)}) \frac{P(x^{(i)},z^{(i)};\theta)}{Q(z^{(i)})} \
& \ge \sum_i \sum_{z^{(i)}} Q(z^{(i)}) \log \frac{P(x^{(i)},z^{(i)};\theta)}{Q(z^{(i)})} \end{align} $$ 最后一步用到了Jensen不等式，f(x)的f对应log函数，x对应$ \frac{P(x^{(i)},z^{(i)};\theta)}{Q(z^{(i)})}$，p(x)对应$Q(z^{(i)})$。那么$f(E[x])$对应$\log \sum_{z^{(i)}} Q(z^{(i)}) \frac{P(x^{(i)},z^{(i)};\theta)}{Q(z^{(i)})}$，$E[f(x)]$对应$\sum_{z^{(i)}} Q(z^{(i)}) \log \frac{P(x^{(i)},z^{(i)};\theta)}{Q(z^{(i)})}$。 因此$Q(z^{(i)})$代表的是p(x)也就是概率，所以显然 $$ \sum_{z^{(i)}} Q(z^{(i)}) = 1 , Q(z^{(i)})>0 $$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2019-04-14-gbdt%E7%90%86%E8%A7%A3/ rel=bookmark>GBDT 解理</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">GBDT 全称是 Gradient Boosting Decision Tree，我们从分别这几个词来理解 GBDT;
Boosting GBDT 整体框架属于Boosting算法。Boosting方法是一种常用的统计学习方法，应用广泛有效。分类问题中，它通过改变训练样本的权重，学习多个弱分类器，并将这些分类器进行线性组合，提高分类性能。这里只对Boosting 这做简单说明。
Decision Tree 在 GBDT中学习的弱分类器就是决策树，具体来说是回归树并不是分类树，这里也不展开说明。
Gradient GBDT中改变训练样本的权重的方式是Gradient，使用 GBDT 来做回归任务时，通过推导Gradient=残差
回归问题的GBDT的学习过程 通俗点来讲，GBDT 是由一排回归树组成的，每一颗决策树学习目标是拟合前一颗树的Gradient或残差。
初始化$f_0(x)=0$ 对$m = 1,2、、、M$ 计算残差 $$ r_{mi} = y_i-f_{m-1}(x), i=1,2、、、N $$ 拟合残差$r_{mi}$学习一棵回归树，得到$h_m(x)$ 更新$f_m(x) = f_{m-1} + h_m(x)$ 得到回归问题的提升树 参考：
https://blog.csdn.net/zpalyq110/article/details/79527653 https://blog.csdn.net/u012422446/article/details/51506392 https://blog.csdn.net/anshuai_aw1/article/details/83040541
机器学习算法GBDT的面试要点总结-上篇
GBDT算法整理 https://www.msra.cn/zh-cn/news/features/lightgbm-20170105</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-02-12-logistic%E5%9B%9E%E5%BD%92/ rel=bookmark>Logistic回归</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 logistic回归是统计学习中经典的分类方法，在深度学习也有很多应用。本文主要介绍logistic回归，然后将其推广到多分类问题-softmax回归。
什么是logistic 虽然名称中有回归，其实logistic回归模型是一个经典二分类模型。logistic回归在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数。
logstic回归模型的特点，一个事件的几率是指该事件发生的概率与不发生的概率的比值，如果事件发生的概率是p，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率为 $$ logit(p) = \log \frac{p}{1-p} $$ 对于logistic回归而言这个对数几率就是wx，也就是说线性函数的值越接近正无穷，概率值越接近1；线性函数的值越接近负无穷，概率值越接近0，这样的模型就是logistic回归模型。一句话概括的话，logistic回归模型实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。
$$ h(\theta) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}} $$ where $$ g(z) = \frac{1}{1+e^{-z}} $$
$$ P(p=1|x;\theta) = h_\theta(x) \
P(p=0|x;\theta) = 1 - h_\theta(x) $$
因此 $$ P(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y} $$
所以似然函数为 $$ \begin{align} L(\theta) & = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) \\
& =\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\
& =\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \end{align} $$
对数似然函数 $$ \begin{align} l(\theta) & = \log L(\theta) \\
& = \sum_{i=1}^my^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)})) \end{align} $$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-09-02-logistic%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%8E%A8%E5%AF%BC/ rel=bookmark>Logistic回归梯度下降推导</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">Logistic回归梯度下降推导 $$ P(p=1|x;\theta) = h_\theta(x) = \frac{exp(\theta^Tx)}{1+exp(\theta^Tx)} \
P(p=0|x;\theta) = 1 - h_\theta(x) = \frac{1}{1+exp(\theta^Tx)} $$
$$ \begin{align} L(\theta) & = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) \
& =\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \
& =\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \end{align} $$
$$ \begin{align} l(\theta) & = \log L(\theta) \
& = \sum_{i=1}^m \left[ y^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)})) \right] \
& = \sum_{i=1}^m \left[ y^{(i)}\log \frac{h(x^{(i)})}{1-h(x^{(i)})} + \log (1-h(x^{(i)})) \right] \
& = \sum_{i=1}^m \left[ y^{(i)}\theta^Tx + \log (1-h(x^{(i)})) \right] \end{align} $$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-07-06-%E4%BC%BC%E7%84%B6%E4%B8%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/ rel=bookmark>似然与极大似然估计</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">概要 本文先会介绍似然的概念，似然与概率的区别，然后介绍参数估计的方法——极大似然估计。
似然 在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。
概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性，比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；
而似然刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数），还是抛硬币的例子，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%，这个过程就是我们根据结果来判断这个事情本身的性质（参数），也就是似然。
极大似然估计 似大似然估计解决的问题是，最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。似大似然估计经常在机器学习方法中作为一种学习策略。
似然函数的最大值意味着什么？让我们回到概率和似然的定义，概率描述的是在一定条件下某个事件发生的可能性，概率越大说明这件事情越可能会发生；而似然描述的是结果已知的情况下，该事件在不同条件下发生的可能性，似然函数的值越大说明该事件在对应的条件下发生的可能性越大。
也就是说似然函数取得最大值表示相应的参数能够使得统计模型最为合理。
考虑一个抛硬币的例子。假设这个硬币正面跟反面轻重不同。我们把这个硬币抛80次（即，我们获取一个采样$x_1=H,x_2=T,&mldr;..x_{80}$并把正面的次数记下来，正面记为H，反面记为T）。并把抛出一个正面的概率记为p，抛出一个反面的概率记为1-p（因此，这里的 p即相当于上边的 $\theta$ ）。 假设我们抛出了49个正面，31个反面，即49次H，31次T。假设这个硬币是我们从一个装了三个硬币的盒子里头取出的。这三个硬币抛出正面的概率分别为p=1/3, p=1/2,p=2/3.这些硬币没有标记，所以我们无法知道哪个是哪个。 使用最大似然估计，通过这些试验数据（即采样数据），我们可以计算出哪个硬币的可能性最大。这个似然函数取以下三个值中的一个： $$ P(H=49,T=31 | p=1/3) = (1/3)^{49}(1-1/3) ^{31}= 0.000 \
P(H=49,T=31 | p=1/2) = (1/2)^{49}(1-1/2)^{31} = 0.012 \
P(H=49,T=31 | p=2/3) = (2/3)^{49}(1-2/3)^{31} = 0.054 \
$$ 我们可以看到当p=2/3时，似然函数取得最大值。这就是 p的最大似然估计。
但在机器学习中我们要估计的并不是离散的情况，因此最大似然估计的一般求解过程是：
写出似然函数； 对似然函数取对数，并整理，也就对数似然函数； 求导数，解似然方程，也就是取极值； 参考： https://zhuanlan.zhihu.com/p/22092462 https://zh.wikipedia.org/wiki/似然函数 https://zh.wikipedia.org/wiki/最大似然估计</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-07-19-%E5%86%B3%E7%AD%96%E6%A0%91/ rel=bookmark>决策树</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 决策树的优点是计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点是可能会产生过度匹配问题。适用于连续值和离散值数据。
决策树生成 训练集$D={(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$ 属性集$A={a_1,a_2,\dots,a_d}$
def function treeGenerate(D,A): 生成结node; if Dv为空: 将该分支结点标记为叶结点，其类别标记为D中样本最多的类;return; if D中所有实例属于同一类: 将该分支结点标记为叶结点，将该类作为其类别标记;return; a* = 从A中选择最优划分属性; for v in a*: 为node生成一个分支； 令Dv表示D中在a*上取值为v的样本子集； 以treeGenerate(Dv,A-a*)为分支结点 特征选择 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有特征是没有分类能力的。
信息增益 信息增益也叫互信息。当获取的信息和要研究的事物“有关系”时，这些信息才能帮助我们消除不确定性。当然“有关系”这种说法太模糊，太不科学，最好能度化的度量“相关性”。 熵： $H(X) = -\sum_i^nP(x_i)\log(P(x_i)$ 条件熵： $H(Y|X) = \sum_i^nP(x_i)H(Y|X=x_i)$ 信息增益就是熵与条件熵的差值： $$ Gain(D,a) = H(D) - H(D|a) = -\sum_i^{|Y|} P(Y_i) \log P(Y_i) - \sum_j^{|a|}P(a_j)H(D|a=a_j) \
= H(D) - \sum_{v=1}^V \frac{|D^v|}{|D|}H(D^v) $$
信息增益比 信息增益作为划分训练数据的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。 信息增益比： $$ Gain_ratio(D,a) = \frac{Gain(D,a)}{H(a)} \
= \frac{H(D) - H(D|a)}{H(a)} \</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-02-12-%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B/ rel=bookmark>感知机模型</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">感知机使用的函数： $$ g(z) = \begin{cases} 1, & \text{if z ≥ 0} \
0, & \text{if z &lt; 0} \
\end{cases} $$
因此， $$ h(\theta) = g(\theta^Tx) $$
学习算法和logistic一样 $$ \theta_j := \theta_j+ \alpha(y^i-h_\theta(x^i))x_j^i $$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-02-10-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/ rel=bookmark>拉格朗日乘数法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">求下面，约束最优化问题：
$$ \underset{w}{min}f(w) \
s.t. \ h_i(w) = 0,i=1&mldr;l $$
首先创建一个拉格朗日算子 $$ L(w,\beta)=f(w)+\sum_i\beta_ih_i(w) $$ 其中$\beta_i$被称为拉格朗日乘数
然后令 $$ \frac{\partial L}{\partial w} = 0 \
\frac{\partial L}{\partial \beta} = 0
$$ 求方程组的解
广义拉格朗日乘数法 求下面，约束最优化问题： $$ \underset{w}{min}f(w) \
s.t. \ g_i(w) \le 0,i=1&mldr;k \
s.t. \ h_i(w) = 0,i=1&mldr;l $$
首先创建一个拉格朗日算子 $$ L(w,\alpha,\beta)=f(w)+\sum_{i=1}^k\alpha_ig_i(w) +\sum_{i=1}^l\beta_ih_i(w) $$
定义 $$ \theta_p(w) = \underset{\alpha,\beta;\alpha_i>0}{max}L(w,\alpha,\beta) $$
$$ p^*=\underset{w}{min}\ \underset{\alpha,\beta;\alpha_i>0}{max}L(w,\alpha,\beta) = \underset{w}{min}\ \theta_p(w) $$
p代表primal，这类问题称为原始问题
$$ \theta_p(w) = \begin{cases} f(w), & \text{符合约束条件} \</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-01-12-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%9B%9E%E5%BD%92/ rel=bookmark>最小二乘回归</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">最小二乘线性回归与梯度下降算法 线性回归 预测函数： $$ h_\theta(x)=\theta_0 + \theta_1x_1 + \theta_2x_2 $$
将$x_0 = 1$
$$ h_\theta(x) = \sum_{i=0}^n\theta_ix_i $$
其中 $\theta$这参数，n表示特征数量
成本函数： $$ J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 $$
其中m表示样本数量，$(x^{(i)},y^{(i)})$表示第i个样本对儿，1/2是为了方便计算
我们目标： $$ \min_\theta J(\theta) $$
最小二乘成本函数的概率学解释（并不是唯一的解释） 假设该线性模型符合高斯分布 $$ p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\sqrt{(y^{(i)}-\theta^Tx^{(i)^2})}{2\sigma^2}\right) $$
似然函数为： $$ L(\theta) = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) $$ 因此，线性回归的似然函数： $$ \begin{align} L(\theta) & = \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \
& = \prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{((y^{(i)}) - \theta^Tx^{(i)})^2}{2\sigma^2}\right) \end{align} $$
对数似然度为 $$ \begin{align} l(\theta) & = \log L(\theta)\</div></article></main><div class=pagination><span class="pagination__item pagination__item--current">1/2</span>
<a class="pagination__item pagination__item--next btn" href=writinglite.com/categories/machine-learning/page/2/>»</a></div></div><aside class="sidebar sidebar--left"><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=writinglite.com></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2020-03-02-%E5%9F%BA%E4%BA%8Enginx%E7%9A%84acme%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E6%96%B9%E6%A1%88/>基于nginx的acme免费证书方案</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-11-04-%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E5%AE%89%E8%A3%85-certbot/>免费证书安装-certbot</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-07-27-%E6%A0%91%E8%8E%93%E6%B4%BE3b+-%E5%AE%89%E8%A3%85openwrt/>树莓派3b+安装openwrt 18.06.4</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-05-18-git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Git 常用命令</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-05-18-linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Linux 常用命令</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=writinglite.com/categories/algorithm/>algorithm</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/deep-learning/>deep learning</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/machine-learning/>machine learning</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/mathematics/>mathematics</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/nlp/>NLP</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/other/>other</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/reading/>reading</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/tensorflow2.x-keras/>Tensorflow2.x keras</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/acme/ title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/albert/ title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/batch-normalization/ title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/bert/ title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/crf/ title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/git/ title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/guice/ title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/hexo/ title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/kaggle/ title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/keras/ title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/layer-normalization/ title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/linux/ title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/log/ title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/logback/ title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/nginx/ title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/openwrt/ title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/slf4j/ title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/stanford-corenlp/ title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow/ title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow-hub/ title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow2.0/ title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tra/ title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/transformers/ title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/ubuntu/ title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%88%86%E8%AF%8D/ title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%93%88%E5%B8%8C/ title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/ title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%B9%B6%E5%8F%91/ title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%8E%92%E5%BA%8F/ title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%95%A3%E5%88%97%E8%A1%A8/ title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/ title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3/ title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%86%B5/ title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%AE%97%E6%B3%95/ title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/ title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/ title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/writinglite.com/js/menu.js></script><script src=/writinglite.com/js/custom.js></script></body></html>