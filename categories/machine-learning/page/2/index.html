<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>machine learning - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="machine learning"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="writinglite.com/categories/machine-learning/"><meta itemprop=name content="machine learning"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="machine learning"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/writinglite.com/css/style.css><link rel=stylesheet href=/writinglite.com/css/custom.css><link rel=alternate type=application/rss+xml href=writinglite.com/categories/machine-learning/index.xml title="Writing Lite"><link rel="shortcut icon" href=/writinglite.com/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/writinglite.com title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><header class=main__header><h1 class=main__title>machine learning</h1></header><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-03-12-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/ rel=bookmark>朴素贝叶斯</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">朴素贝叶斯是一种分类模型，属于生成模型。比较常见的应用是垃圾邮件分类；
在垃圾邮件识别为例
$$ y\in{0,1} $$ y=1表示垃圾邮件
将每封邮件表示成一个向量，向量中每一维都是字典中的一个词的0/1值，1表示该词在邮件中出现，0表示未出现。 假设有一个50000个词的词表，一封邮件x可表示（多元伯努利时间模型） $$ x= \begin{bmatrix} 0 \
1 \
1 \
… \
\end{bmatrix} $$
$$ x=\in{0,1}^n $$ n=50000
假设 我们还是对P(X|Y)建模
$$ \begin{aligned} & P(x_1,x_2,&mldr;x_{50000}|y) \
&= P(x_1|y)*P(x_2|y,x_1)*P(x_3|y,x_1,x_2)&mldr; \
&= P(x_1|y)*P(x_2|y)*P(x_3|y)&mldr;P(x_50000|y) \
&= \prod_{i=1}^{m}P(x_i|y) \end{aligned} $$ NLP中的n元语法模型有点类似，这里相当于一元文法unigram。
这里有个假设，条件独立性假设，形式化表示为，（如果给定Z的情况下，X和Y条件独立）
这里我们发现朴素贝叶斯假设是约束性很强的假设，“buy”从通常上讲与“price”是有关系，我们这里假设的是条件独立。
模型参数 $$ \begin{aligned} \phi_{i|y=1} &= P(x_i=1|y=1) \
\phi_{i|y=0} &= P(x_i=1|y=0) \
\phi_y&=P(y=1) \end{aligned} $$
joint似然函数为： $$ L(\phi_y,\phi_{i|y=0},\phi_{i|y=1})= \prod_{i=1}^{m}P(x^{(i)},y^{(i)}) $$
求该 Joint似然最大化得： $$ \begin{aligned} \phi_{i|y=1} &= \frac{\sum_{i=1}^mI{x_j^{(i)}=1,y^{(i)}=1}}{\sum_{i=1}^mI{y^{(i)}=1}} \</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-03-12-%E6%A8%A1%E5%9E%8B%E7%AE%80%E8%A6%81%E6%80%BB%E7%BB%93/ rel=bookmark>模型简要总结</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">模型简要总结 决策树 介绍 决策树是一种分类和回归方法。下面主要介绍分类部分。决策树模型呈树状结构，在分类问题中，表示基于特征对实例进行分类的过程。它也可以认为是if-then规则的集合。
模型的学习 决策树的学习本质上是从训练数据中归纳出一组分类规则。能对训练数据进行正确分类的决策树可能有多个，我们需要的是一个与训练数据矛盾较少，同时具有很好的泛化能力。决策树学习通常需要3个步骤：
特征选择 信息增益：特征对训练集的信息增益，等价于互信息。信息增益大的特征具有更强的分类能力。 $$ g(D,A)=H(D)-H(D \mid A) $$ 信息增益比：其信息增益与训练数据集关于特征A的值的熵$H_A(D)$之比 $$ g_R(D,A)=\frac{g(D,A)}{H_A(D)} $$ 决策树生成 决策树的剪枝 优缺点： logstic回归 最大熵 支持向量机 隐马尔可夫模型 条件随机场 关于熵 我们要搞清楚一件非常不确定的事，或是我们一无所知的事情，就需要了解大量的信息。相反，如果我们对某件事情已经有了较多的了解，那么不需要太多的信息就能把它搞清楚。所以，从这个角度来看，可以认为，信息量就等于不确定性的多少。
熵 $$ H(X)=-\sum_{x \in X} P(x) \log P(x) $$
条件熵 $$ H(X \mid Y)=-\sum_{x \in X,y \in Y} P(x,y) \log P(x \mid y) $$ 如果H(X)>=H(X|Y)，也就是说多了Y的信息，关于X的不确定性下降了。
互信息 $$ H(X ; Y)= H(X) - H(X \mid Y) $$ 两件事相关性的量化度量。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-03-12-%E6%B7%B7%E5%90%88%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B/ rel=bookmark>混合朴素贝叶斯模型</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">训练样本，${x_1,x_2,x_3 &mldr; x_m}$ m个文本。 $$ x^{(i)} \in {0,1}^n \
x^{(i)}_j = I{词j是否在文档x^{(i)}中 } \
z^{(i)} \in {0,1} \
z^{(i)} \sim 伯努利(\phi) $$ zi是隐含随机变量，也就是聚类类别，这里假设是两类，也因此分布为伯努利分布，也可以根据情况有更多类别。
条件独立假设： $$ P(x^{(i)} \mid z^{(i)}) = \prod_i^n P(x^{(i)}_j \mid z^{(i)}) $$ 表示，在给定类别z生成文档x的概率。
$$ P(x^{(i)}j=1 \mid z^{(i)} = 0) = \phi{j \mid z=0} $$ 对应高斯判别分析中的 $$ \begin{aligned} \phi_{i|y=1} &= P(x_i=1|y=1) \
\phi_{i|y=0} &= P(x_i=1|y=0) \
\phi_y&=P(y=1) \end{aligned} $$
E-step: $$ w^{(i)} = P(z^{(i)}=1 \mid x^{(i)};\phi_{j \mid z},\phi) \</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-02-18-%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/ rel=bookmark>混合高斯模型</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">混合高斯模型 假设z为隐变量，x,z存在如下关系。 $$ P(x^{(i)},z^{(i)}) = P(x^{(i)}|z^{(i)})P(z^{(i)}) $$ $$ z^{(i)} \sim Multinomial(\phi)$ \
\phi_j=P(z^{(i)}=j)>0 \
\sum_j\phi_j=1 \
(x^{(i)} \mid {z^{(i)}=j} ) \sim N(\mu_j ,\Sigma_j) $$ 这里的j的聚类的一个类别之一。
E-step: 对z的猜测 $$ \begin{aligned} w_j^{(i)} & := P(z^{(i)}_j \mid x^{(i)}=j;\phi,\mu_j,\Sigma_j) \
&= \frac{ P(x^{(i)} \mid z^{(i)}_j=j)P(z^{(i)}_j=j)}{\sum_l^n P(x^{(i)} \mid z^{(i)}_j=l)P(z^{(i)}_j=l)} \end{aligned} $$
其中，$P(z^{(i)}_j \mid x^{(i)}=j;\phi,\mu_j,\Sigma_j)$表示第i个样本生成z为类别j的概率。这里的n表示聚类总类别数。
M-step: 对参数的估计 $$ \phi_j:=\frac{1}{m} \sum_{i=1}^m w_j^{(i)} \
\mu_j:= \frac{\sum_{i=1}^m w_j^{(i)} x^{(i)} }{\sum_{i=1}^m w_j^{(i)}} \
\Sigma_j:=\frac{ \sum_{i=1}^m w_j^{(i)} (x^{(i)} - \mu_j) (x^{(i)} - \mu_j)^T }{\sum_{i=1}^n w_j^{(i)}} $$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-03-12-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%AB%98%E6%96%AF%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/ rel=bookmark>生成模型与高斯判别分析</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">生成模型与判别模型 判别模型 判别模型对P(y|x)或者说对y建模，直接学习得到y。 常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。
生成模型 生成模型对P(x|y)或者说对x建模，通过计算 $$ \begin{aligned} \arg\max\limits_{y}p(y\vert x) &= \arg\max\limits_y\frac{p(x\vert y)p(y)}{p(x)} \
&= \arg\max\limits_y p(x\vert y)p(y) \end{aligned} $$ 得到y。 因此给定x进行比较时，P(x)为固定值，所以P(x)可省略。 常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等。
高斯分布 若随机变量 X, X服从一个位置参数为 $\mu$ 、尺度参数为$\sigma$ 的概率分布，记为： $$ \displaystyle X \sim N(\mu ,\sigma ^{2}) $$ 则其概率密度函数为 $$ \displaystyle f(x)={1 \over \sigma {\sqrt {2\pi }}},e^{-{(x-\mu )^{2} \over 2\sigma ^{2}}} $$
多元高斯分布 $$ X ∼ N(\mu,\Sigma) $$
$$ \begin{aligned} E[X] &= \mu \
Cov(X) &= \Sigma \end{aligned} $$ 概率密度函数为 $$ \begin{equation} p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)} \end{equation} $$ 其中$|\Sigma|$是$\Sigma$的行列式，$\Sigma$是协方差矩阵，而且是对称半正定的。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-01-07-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/ rel=bookmark>统计学习方法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">统计学习 如果一个系统能够通过执行某个过程改进它的性能，这就是学习。
统计学习的前提 统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这就是统计学习的前提。
统计学习方法组成 统计计算由监督学习，非监督学习，半监督学习和强化学习等组成。
监督学习概括 从给定的、有限的、用于学习的训练数据集合出发，假设数据是独立同分布产生的；并且假设要学习模型属于某个函数集合，称为假设空间；应用某个评价准则，从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。
统计学习方法的三要素 模型 策略 算法 实现统计学习方法的步骤 得到一个有限的训练数据集合 确定包含所有可能模型的假设空间，即学习模型的集合 确定模型选择的准则，即学习的策略 实现求解最优模型的算法，取出学习的算法 通过学习方法选择最做强模型 利用学习的最优模型对新数据进行预测和分析 基本概念 输入空间 输入所有可能的空间
输出空间 输出所有可能的空间
特征空间 每个具体的输入是一个实例，通常由特征向量表示。这时所有特征向量存在的空间称为特征空间。
样本 输入输出对又称为样本或样本点。
问题的分类 回归问题 输入变量与输出变量均为连续变量的预测问题称为回归问题。
分类问题 输出变量为有限个离散变量的预测问题称为分类问题。
标注问题 输入变量与输出变量均为变量序列的预测问题称为标注问题。
联合概率分布 监督学习假设输入与输出的随机变量X与Y遵循联合概率分布。训练数据与测试数据被看作是依联合概率分布P(X,Y)独立同分布产生的。
假设空间 模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。
监督学习的模型可以是概率模型或非概率模型，由条件概率分布P(Y|X)或决策函数Y=f(X)表示，随具体学习方法而定。
统计学习的三要素 方法=模型+策略+算法
模型 策略 损失函数:度量一次预测的好坏 风险函数：度量平均意义下模型预测的好坏</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-06-04-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA/ rel=bookmark>统计学习方法概论</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">统计学习的基本概念 学习的定义 如果一个系统能够通过执行某个过程改进它的性能，这就是学习。统计学习的对象是数据；统计学习关于数据的基本假设是同类数据具有一定的统计规律，这是统计学习的前提。统计学习的上的是对未知数据进行预测和分析；对数据的预测可以使计算机更加智能化；对数据的分析可以让人们获取新的知识，给人们带来新的发现。
监督学习的学习方法 从给定的、有限的、用于学习的训练数据集合出发，假设数据是独立同分布产生的；并且假设学习的模型属于某个函数集合，称为假设空间；应用某个评价准则，从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。
实现统计学习方法的步骤 得到一个有限的训练数据集合 确定包含所有可能模型的假设空间，即学习模型的集合 确定模型选择的准则，即学习的策略 实现求解最优模型的算法，取出学习的算法 通过学习方法选择最做强模型 利用学习的最优模型对新数据进行预测和分析 不同的预测任务名称 输入变量与输出变量均为连续变量的预测问题称为回归问题。 输出变量为有限个离散变量的预测问题称为分类问题。 输入变量与输出变量均为变量序列的预测问题称为标注问题。 联合概率分布 监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)，P(X,Y)表示分布函数，或分布密度函数。训练数据与测试数据被看作是依联合概率分布P(X,Y)独立同分布产生的。
判别模型 判别模型对P(y|x)或者说对y建模，直接学习得到y。 在计算学习算法时，一般使用candidation似然 就是直接用的P(y|x) 常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。
生成模型 生成模型对P(x|y)或者说对x建模（从下面的公式可以看到），通过如下计算得到 y。 $$ \begin{aligned} \arg\max\limits_{y}p(y\vert x) &= \arg\max\limits_y\frac{p(x,y)}{p(x)} \
&= \arg\max\limits_y\frac{p(x\vert y)p(y)}{p(x)} \
&= \arg\max\limits_y p(x\vert y)p(y) \end{aligned} $$ 在给定x进行比较时，P(x)为固定值，所以P(x)可省略。在计算学习算法时，一般使用joint似然，就是用的P(x,y)=P(x|y)*P(y)，其实和P(y|x) 一样的。 生成模型表示的是数据生成的方式 ，就是P(x,y) x和y的联合概率。一般来说数据的生成方式是比较复杂的，所以一般都会对数据的生成方式做一定的假设（比如隐马尔科夫模型、朴素贝叶斯模型）。 常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等
统计学习的三要素 统计学习方法包括模型的假设空间、模型选择的准则以及模型的学习算法，称其为统计学习方法的三要素，简称为模型、策略、算法。
模型 在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。 假设空间用F表示，假设空间可以定义为决策函数的集合 $$ F={f|Y=f(x)} $$ 假设空间也可以定义为条件概率的集合 $$ F={P|P(Y|X)} $$
策略 首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。
常用的损失函数 **0-1损失函数 ** $$ L(Y,f(X))= \begin{cases} 1,Y \neq f(X)\</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2016-11-25-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/ rel=bookmark>隐马尔可夫模型</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">隐马尔可夫模型解决的序列标注问题，它是一个生成模型。本文先介绍显马尔可夫模型（或者叫马尔可夫链），然后介绍显马尔可夫模型的扩展，即隐马尔可夫模型。显与隐指的是状态序列是否可观测。
显（可视）马尔可夫模型 显马尔可夫模型、可视马尔可夫模型、马尔可夫链都是指马尔可夫模型。
随机过程又称随机函数，是随时间而随机变化的过程。马尔可夫模型描述了一类（这些随机变量并非相互独立，每个随机变量的值依赖于这个序列前面的状态）重要的随机过程。 随机过程有两层含义：
它是一个时间的函数，随着时间的改变可改变。 每个时刻上的函数值是不确定的，是随机的，也就是说，每一时刻上的函数值按照一定的概率而分布。 马尔可夫模型与有限状态机 马尔可夫模型又可视为随机的有限状态机，更准确的说马尔可夫模型和隐马尔可夫模型都是有限自动机的扩充。
马尔可夫模型与n元文法模型 前面提到在马尔可夫模型中每个随机变量受这个序列前面的状态影响的，如果我们只考虑前面一个状态对后面一个状态出现的概率的影响，这样的链叫做一重马尔可夫链，也就量二元文法模型，如果考虑前面两个状态，这样的叫二重马可尔可夫链，也就是三元文法模型，依此类推，n重马尔可夫链对应n-1元文法模型。
隐马尔可夫模型 马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可可观测 的状态随机序列，再由各个状态生成一个观测而产生观测 随机序列的过程。 马尔可夫模型是一个生成模型，因此它也是学习
$$ p(y \vert x) = \frac{p(x,y)}{p(x)} = \frac{p(x \vert y)p(y)}{p(x)} $$
相关符号 高Q是所有可能状态集合，V是所有可能的观测集合。 $$ Q={q_1,q_2, &mldr; q_N },V={v_1,v_2, &mldr; v_M } $$ N是可能的状态数，M是可能的观测数。 I是长度为T的状态序列，O是对应的观测序列。 $$ I={i_1,i_2, &mldr; i_T },O={o_1,o_2, &mldr; o_T } $$ A是状态转移矩阵： $$ A=[a_{ij}]_{M \times N} $$ 其中 $$ a_{ij}=P(i_{t+1}=q_j|i_t=q_i), \qquad i=1,2&mldr;N;j=1,2&mldr;N $$ 表示在时刻t处于状态qi的状态下在时刻t+1转移到状态qj的概率。
B是观测概率矩阵： $$ B=[b_j(k)]_{N \times M} $$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-07-15-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/ rel=bookmark>集成学习简单介绍</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 目前在kaggle中，GBDT和RF都是非常流行的算法，在一些比赛中，排名先前的算法都有这两种算法的身影。这两种算法都属于Ensemble算法，因此，本文将介绍介绍下Ensemble的思想，以及实现Ensemble的两种方法。
什么是集成学习 Ensemble通过构建多个学习器来完成学习任务，直觉上来说，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但并不是说，随机进行组合就能得到好的效果，各个子模型应该“好而不同”，即个体学习器要有一定的“准确性”又要有“互补性”。
根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即
个体学习器间存在强依赖关系、必须串行生成的序列化算法。 个体学习器间不存在依赖关系、可同时生成的并行化算法。 前者的代表是Bossting，后者的代表是Bagging。从偏差-方差的角度看Bossting主要关注降低偏差，因此Bossting能基于泛化性能相当弱的学习器构建出很强的学习器，比如一棵只有5层的决策树。而Bagging主要关注降低方差，因此它在不剪枝层次较深的决策树、神经网络等易样本干扰的学习器上效用更加明显。 要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。
Bostting Bostting是一族可将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这个T个基学习器进行加权结合。
Bagging 如上所述，一个可行的集成学习算法，要满足两个要点，一是“多样性”二是“准确性”。一个可行的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据集中训练出一个学习器，这样就满足了“多样性”。然而，采样的每个子集都完全不同，则每个学习器只用到了一个小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器，为了解决这个问题可以采用自助采样法。
《机器学习》 周志华 https://www.zhihu.com/question/29036379</div></article></main><div class=pagination><a class="pagination__item pagination__item--prev btn" href=writinglite.com/categories/machine-learning/>«</a>
<span class="pagination__item pagination__item--current">2/2</span></div></div><aside class="sidebar sidebar--left"><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=writinglite.com></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2020-03-02-%E5%9F%BA%E4%BA%8Enginx%E7%9A%84acme%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E6%96%B9%E6%A1%88/>基于nginx的acme免费证书方案</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-11-04-%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E5%AE%89%E8%A3%85-certbot/>免费证书安装-certbot</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-07-27-%E6%A0%91%E8%8E%93%E6%B4%BE3b+-%E5%AE%89%E8%A3%85openwrt/>树莓派3b+安装openwrt 18.06.4</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-05-18-git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Git 常用命令</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-05-18-linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Linux 常用命令</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=writinglite.com/categories/algorithm/>algorithm</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/deep-learning/>deep learning</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/machine-learning/>machine learning</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/mathematics/>mathematics</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/nlp/>NLP</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/other/>other</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/reading/>reading</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/tensorflow2.x-keras/>Tensorflow2.x keras</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/acme/ title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/albert/ title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/batch-normalization/ title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/bert/ title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/crf/ title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/git/ title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/guice/ title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/hexo/ title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/kaggle/ title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/keras/ title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/layer-normalization/ title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/linux/ title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/log/ title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/logback/ title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/nginx/ title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/openwrt/ title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/slf4j/ title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/stanford-corenlp/ title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow/ title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow-hub/ title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow2.0/ title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tra/ title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/transformers/ title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/ubuntu/ title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%88%86%E8%AF%8D/ title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%93%88%E5%B8%8C/ title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/ title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%B9%B6%E5%8F%91/ title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%8E%92%E5%BA%8F/ title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%95%A3%E5%88%97%E8%A1%A8/ title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/ title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3/ title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%86%B5/ title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%AE%97%E6%B3%95/ title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/ title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/ title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/writinglite.com/js/menu.js></script><script src=/writinglite.com/js/custom.js></script></body></html>