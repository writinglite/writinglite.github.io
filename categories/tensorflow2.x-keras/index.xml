<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tensorflow2.x keras on Writing Lite</title><link>writinglite.com/categories/tensorflow2.x-keras/</link><description>Recent content in Tensorflow2.x keras on Writing Lite</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="writinglite.com/categories/tensorflow2.x-keras/index.xml" rel="self" type="application/rss+xml"/><item><title>Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络</title><link>writinglite.com/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%981-tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>writinglite.com/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%981-tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>大家好，今天是Tensorflow实战系列的第一次分享，本次分享共有3个主题：
Tensorflow介绍 Tensorflow核心概念 使用Tensorflow实现手写数字识别任务 由于本次分享内容以实战为主不会涉及过多理论的讲解，但是分享过程中涉及到的理论知识也会做一个快速回顾。
Tensorflow Introduction Tensorflow 是由Google研发的开源软件库，它既是一个实现机器学习算法的接口，同时也是执行机器学习算法的框架，它对深度学习中常用的神经网络结构等算法进行了封装，因此开发人员可以快速的进行模型搭建。
1、Tensorflow 发展史 2011 年，Google Brain内部孵化出一个项目叫做DistBelief, 它是为深度神经网络构建的一个机器学习系统，是Tensorflow的前身。 2015年11月，Google正式发布了Tensorflow的白皮书并开源TensorFlow 0.1 版本。 2017年02月，Tensorflow正式发布了1.0.0版本，同时也标志着稳定版的诞生。 2019年10月，TensorFlow在经历七个多月(2019年3月1日-2019年10月1日)的2.0 Alpha 版本的更新迭代后发布 2.0 正式版。 通过上面的发展史我们可以看到，虽然经过了9年时间Tensorflow依然是目前最流行的深度学习框架之一。
2、Tensorflow VS Pytorch 上面说到Tensorflow是目前最流行的深度学习框架之一，那另一款可以和Tensorflow一较高下的深度学习框架就是-Pytorch了。Pytorch是由Facebook研发的一款开源的机器学习库，自16年发布以来发展非常迅猛。Tensorflow和Pytorch如何选择呢，我的看法是：都可以，虽然刚开始时Pytorch和Tensorflow还是差别较大的，比较Pytorch有动态图、类python的编程方式，Tensorflow则支持可视化，生产部署更加简单易用，但通过这几年的发展Pytorch和Tensorfow越来越像了，Tensorflow添加了动态图，而Pytorch也在工业部署上有了很大改善。因此在两都的选择上不必太过纠结。
import tensorflow as tf import matplotlib as mpl %matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd import os import sys print(sys.version) 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] for module in tf, np, mpl: print(module.</description></item><item><title>Tensorflow实战(2)_循环神经网络实战_唐诗生成器</title><link>writinglite.com/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%982_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%94%90%E8%AF%97%E7%94%9F%E6%88%90%E5%99%A8/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>writinglite.com/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%982_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%94%90%E8%AF%97%E7%94%9F%E6%88%90%E5%99%A8/</guid><description>本次分享内容： 上次分享内容回顾 RNN 理论知识 唐诗生成器实例讲解 本次课程内容总结 上次分享我们介绍了关于Tensorflow2的基础语法，以及通过手写数字识别任务讲解了如何通过Tensorflow2来搭建全连接神经网络模型。今天我们来介绍用于处理序列信息的网络结构-RNN。
RNN理论知识 循环神经网络（Recurrent neural network：RNN）是神经网络的一种。循环神经网络可以描述动态时间行为，与前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。 如果说一个全连接的神经网络的计算公式是：$y^t=f(x^t)$，那么RNN的公式可以这样表示$y^t, h^t = f(x^t, h^{t-1})$ 。一个单层的RNN可以用下图表示： Deep RNN 与全连接神经网络一样，RNN也可以叠加多层： Bidirectional RNN Bidirectional RNN 是将传统RNN的状态神经元拆分为两个部分，一个负责forward states，另一个负责backward states。Forward states的输出并不会连接到Backward states的输入。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。 RNN 计算方式 Native RNN $$ \begin{align} &amp;amp; h_t = \sigma(W^h h_{t-1} + W^i x_t) \
&amp;amp; y_t = \sigma(W^o h_t) \end{align} $$
LSTM $$ \begin{align} &amp;amp; i_t = sigm(W^{xi}x_t + W^{hi}h_{t-1}) \
&amp;amp; f_t = sigm(W^{xf}x_t + W^{hf}h_{t-1}) \</description></item><item><title>卷积神经网络实战_图片分类</title><link>writinglite.com/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%984-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>writinglite.com/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%984-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/</guid><description>理论部分 CNN解决的问题 在CNN出现之前，图像对于人工智能来说是一个难题，有2个原因：
图像需要处理的数据量太大，导致成本很高，效率很低。 图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高。 另一个角度（用全连接神经网络处理大尺寸图像的缺点）：
其次参数过多效率低下，训练困难，同时大量的参数也很快会导致网络过拟合 图像展开为向量会丢失空间信息； 卷积层 这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。
在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：
池化层 池化层简单说就是下采样，他可以大大降低数据的维度，也可以缓解卷积层对位置的过度敏感性。其过程如下：
上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。
之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。
总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。
实战部分 import matplotlib.pyplot as plt import numpy as np import os import PIL import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import Sequential # 卷积层 x = tf.constant(range(9), dtype=tf.float32) x = tf.reshape(x, shape=(1, 3, 3, 1)) print(&amp;#39;x :&amp;#39;, tf.reshape(x, shape=(3, 3))) conv_layer = tf.keras.layers.Conv2D(filters=1, kernel_size=(2, 2), strides=(1, 1), kernel_initializer=tf.</description></item></channel></rss>