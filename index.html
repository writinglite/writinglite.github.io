<!doctype html><html class=no-js lang=en><head><meta name=generator content="Hugo 0.81.0"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Writing Lite"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="/"><meta itemprop=name content="Writing Lite"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Writing Lite"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel=alternate type=application/rss+xml href=/index.xml title="Writing Lite"><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/archives/><span class=menu__text>archives</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-21-%E5%9F%BA%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%B9%B3%E5%9D%87%E7%9A%84%E7%83%AD%E8%AF%8D%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%95.html rel=bookmark>基于贝叶斯平均的新词发现算法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2021-05-20T16:18:00Z>2021-05-20</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/nlp.html rel=category>NLP</a></span></div></div></header><div class="content list__excerpt post__content clearfix">这里对“热词”的定义是：
某一时间段内起点低，增长迅速
贝叶斯平均算法的计算公式是：
\[ WR=\frac{v}{v+m}R+\frac{m}{v+m}C \]
WR， 加权得分。 R，该词基础得分。 v，该词的词频。 m，平均词频数。 C， 所有词的平均基础得分。 R的计算公式：
\[ R = \frac{今天词频}{今天词频 + 前一天的词频} \]
举例：
词 前一天的词频 今天的词频 基础得分 加权得分 A 5 10 0.66 0.619 B 50 100 0.66 0.640 C 50 50 0.50 0.552 average m = 88.33 C = 0.61 简单解释上面的公式，加权得分由两部分组成，一个是该词的基础得分，另一个是所有词的平均基础得分，$\frac{v}{v+m}$和$\frac{m}{v+m}$分别是这两部分的系数；当v=0时，WR实际取的C；当v越来越大时，R的占比会越来越大。达到效果是，当该词的R > C时，在相同R的前提下，v越大WR越大。如上面的例子，A和B的基础得分都是0.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-13-han-for-document-classification.html rel=bookmark>HAN for Document Classification</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2021-05-12T16:18:00Z>2021-05-12</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/nlp.html rel=category>NLP</a></span></div></div></header><div class="content list__excerpt post__content clearfix">论文链接：Hierarchical Attention Networks for Document Classification
HAN (Hierarchical Attention Networks for Document Classification) 是一个针对文本分类任务的层次化 attention 模型。它有个显著的特点:通过"词-句子-文章"的层次化结构来表示一篇文本。
HAN 模型的灵感来源于人在阅读 document 的时候，不同的词和句子对人理解 document 信息有不同的影响。因为，词和句子的重要性是和上下文息息相关的，即使是相同的词和句子，在不同的上下文中重要性也不一样。人在阅读一篇文章时，对 document 不同的内容是有着不同的注意度的。
attention 的目的是要把一个句子中，对句子的含义影响最大的词语找出来。
论文通过将 $h_{it}$ 输入到一个 dense 网络中得到的结果$u_{it}$ 作为$h_{it}$的隐含表示。$h_{it}$可以是双向RNN的结果，或者bert中经过encoder后的结果。
\[ u_{it}=\tanh(W_wh_{it}+b_w) \]
为了衡量单词的重要性，这里用$u_{it}$ 和一个随机初始化的上下文向量$uw$ 的相似度来表示，然后经过 softmax 操作获得了一个归一化的 attention 权重矩阵 $\alpha{it}$，代表句子$i$中第 $t$个词的权重。
\[ \alpha_{it}=\frac{\exp(u^T_{it}u_w)}{\sum_t(\exp(u^T_{it}u_w))} \]
得到了 attention 权重矩阵之后，句子向量 $s_i$ 可以看作这些词向量的加权求和。这里的上下文向量$u_w$ 是在训练网络的过程中学习获得的。我们可以把 $u_w$当作一种询问的高级表示，比如"哪些词含有比较重要的信息?
\[ s_i=\sum_t\alpha_{it}h_{it} \]
使用这个句子向量就可以做句子level的任务，例如文本分类等。
主要查考：https://zhuanlan.zhihu.com/p/54165155</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/other/2021-05-06-%E4%BD%BF%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%88%B0github-pages.html rel=bookmark>使用GitHub Actions自动部署hugo到GitHub Pages</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2021-05-05T14:16:16Z>2021-05-05</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/other.html rel=category>other</a></span></div></div></header><div class="content list__excerpt post__content clearfix">之前一直使用Bitcron 来搭建人个博客，但是最近Bitcron 的官网显示，未来Bitcron 将升级到FarBox 2.0，22年底就不再支持Bitcron 了，并且不再支持几十元一年的套餐了。因此打算寻找其它方案来替代Bitcron。对于新方案的要求主要有几下几个：
支持将原有博客进行迁移 对系统环境要求低，部署方便 支持自定义域名 对Markdown支持良好，支持数学公式 经过各种权衡，选择了hugo + Github Actions + Github Pages作为最终的方案。hugo 作为博客生成工具，Github Actions将hugo生成的博客发布到Github Pages。我们要做的只是在Client将新建的博客push到Github的私有仓库，剩下的工作Github会帮助我们自动完成。下面介绍该方案的具体实现。
需要准备的环境 hugo 安装（非必需） Github 私有仓库（用来放Blog文件） Github Pages 仓库（用来存放静态页面） Step 1 , 生成hugo文件 假设上面提到的环境已经准备好，那么第一步是通过hugo生成一个hugo网站的文件夹。下面的命令可以生成一个blog的文件夹；
hugo new blog 如果没有安装hugo的话，可以在github上clone一个hugo网站的项目。
Step 2，设置Blog仓库 这里假设我们的Github私有仓库的名称也是blog，那么首先，将本地blog文件夹与blog仓库进行关联；然后开始设置Github Actions，方式是在blog目录下新建文件.github/workflows/hugo_deploy.yml，具体内容如下：
name: Deploy Blog on: push: branches: - main jobs: build: # 一项叫做build的任务 runs-on: ubuntu-18.04 # 在最新版的Ubuntu系统下运行 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-10-16-deeplearning%E4%B8%ADcrf%E7%9A%84tensorflow%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html rel=bookmark>DeepLearning中CRF的Tensorflow代码实现</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-10-16T21:07:00Z>2020-10-16</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">主方法 def crf_log_likelihood( inputs: TensorLike, tag_indices: TensorLike, sequence_lengths: TensorLike, transition_params: Optional[TensorLike] = None, ) -> tf.Tensor: """Computes the log-likelihood of tag sequences in a CRF. Args: inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials to use as input to the CRF layer. tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which we compute the log-likelihood. sequence_lengths: A [batch_size] vector of true sequence lengths. transition_params: A [num_tags, num_tags] transition matrix, if available.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-10-16-deeplearning%E4%B8%ADcrf%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86.html rel=bookmark>DeepLearning中CRF计算原理</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-10-16T21:07:00Z>2020-10-16</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">主要内容来处：https://createmomo.github.io：
CRF Layer on the Top of BiLSTM - 1 Outline and Introduction CRF Layer on the Top of BiLSTM - 2 CRF Layer (Emission and Transition Score) CRF Layer on the Top of BiLSTM - 3 CRF Loss Function CRF Layer on the Top of BiLSTM - 4 Real Path Score CRF Layer on the Top of BiLSTM - 5 The Total Score of All the Paths CRF Layer on the Top of BiLSTM - 6 Infer the Labels for a New Sentence CRF Layer on the Top of BiLSTM - 7 Chainer Implementation Warm Up CRF Layer on the Top of BiLSTM - 8 Demo Code 通常在序列标注模型的最后一层layer会添加CRF计算，因为序列标注任务中lable之前有较强的约束性，例如，B-Person与I-Person之前有强关联，B-Person和I-Locations之间有强“非关联”，而CRF模型中的转移矩阵则可以很好体现这些特性。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/tensorflow2.x_keras/tensorflow%E5%AE%9E%E6%88%984-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB.html rel=bookmark>卷积神经网络实战_图片分类</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-10-13T20:43:00Z>2020-10-13</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/tensorflow2.x-keras.html rel=category>Tensorflow2.x keras</a></span></div></div></header><div class="content list__excerpt post__content clearfix">理论部分 CNN解决的问题 在CNN出现之前，图像对于人工智能来说是一个难题，有2个原因：
图像需要处理的数据量太大，导致成本很高，效率很低。 图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高。 另一个角度（用全连接神经网络处理大尺寸图像的缺点）：
其次参数过多效率低下，训练困难，同时大量的参数也很快会导致网络过拟合 图像展开为向量会丢失空间信息； 卷积层 这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。
在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：
池化层 池化层简单说就是下采样，他可以大大降低数据的维度，也可以缓解卷积层对位置的过度敏感性。其过程如下：
上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。
之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。
总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。
实战部分 import matplotlib.pyplot as plt import numpy as np import os import PIL import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import Sequential # 卷积层 x = tf.constant(range(9), dtype=tf.float32) x = tf.reshape(x, shape=(1, 3, 3, 1)) print('x :', tf.reshape(x, shape=(3, 3))) conv_layer = tf.keras.layers.Conv2D(filters=1, kernel_size=(2, 2), strides=(1, 1), kernel_initializer=tf.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-07-29-batch_normalization%E4%B8%8Elayer_normalization%E7%9A%84%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86.html rel=bookmark>Batch Normalization与Layer Normalization的理解整理</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-07-29T21:07:00Z>2020-07-29</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">问题背景　 　接着引入covariate shift的概念：如果ML系统实例集合&lt;X,Y>中的输入值X的分布老是变，这不符合IID假设，网络模型很难稳定的学规律，这不得引入迁移学习才能搞定吗，我们的ML系统还得去学习怎么迎合这种分布变化啊。对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。　我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal Covariate Shift”。Paper所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了Batch Normalization，这个牛逼算法的诞生。
Normalization 的通用框架 $$ \hat{x}=\frac{x-\mu}{\sigma^2} $$ $$ y = g\hat{x}+b $$
$\mu$和$\sigma^2$分别是均值和方差，它们是根据特征值计算出来的，g和b是需要训练过程中去学习的参数。
总结 Layer Normalization与Batch Normalization对比：　BN针对一个minibatch的输入样本，计算均值和方差，基于计算的均值和方差来对某一层神经网络的输入X中每一个case进行归一化操作。但BN有两个明显不足：1、高度依赖于mini-batch的大小，实际使用中会对mini-Batch大小进行约束，不适合类似在线学习（mini-batch为1）情况；2、不适用于RNN网络中normalize操作：BN实际使用时需要计算并且保存某一层神经网络mini-batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；但对于RNN来说，sequence的长度是不一致的，换句话说RNN的深度不是固定的，不同的time-step需要保存不同的statics特征，可能存在一个特殊sequence比其的sequence长很多，这样training时，计算很麻烦。但LN可以有效解决上面这两个问题。
LN适用于LSTM的加速，但用于CNN加速时并没有取得比BN更好的效果。
BN的特点
但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。
LN的缺点
BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。
参考： Batch Normalization（BN，批量归一化） 【深度学习】深入理解Batch Normalization批标准化 深度学习（二十九）Batch Normalization 学习笔记 深度学习加速策略BN、WN和LN的联系与区别，各自的优缺点和适用的场景？？</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/tensorflow2.x_keras/tensorflow%E5%AE%9E%E6%88%982_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%94%90%E8%AF%97%E7%94%9F%E6%88%90%E5%99%A8.html rel=bookmark>Tensorflow实战(2)_循环神经网络实战_唐诗生成器</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-07-22T20:43:00Z>2020-07-22</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/tensorflow2.x-keras.html rel=category>Tensorflow2.x keras</a></span></div></div></header><div class="content list__excerpt post__content clearfix">本次分享内容： 上次分享内容回顾 RNN 理论知识 唐诗生成器实例讲解 本次课程内容总结 上次分享我们介绍了关于Tensorflow2的基础语法，以及通过手写数字识别任务讲解了如何通过Tensorflow2来搭建全连接神经网络模型。今天我们来介绍用于处理序列信息的网络结构-RNN。
RNN理论知识 循环神经网络（Recurrent neural network：RNN）是神经网络的一种。循环神经网络可以描述动态时间行为，与前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。 如果说一个全连接的神经网络的计算公式是：$yt=f(xt)$，那么RNN的公式可以这样表示$y^t, h^t = f(x^t, h^{t-1})$ 。一个单层的RNN可以用下图表示：
Deep RNN 与全连接神经网络一样，RNN也可以叠加多层：
Bidirectional RNN Bidirectional RNN 是将传统RNN的状态神经元拆分为两个部分，一个负责forward states，另一个负责backward states。Forward states的输出并不会连接到Backward states的输入。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。
RNN 计算方式 Native RNN \[ \begin{align} & h_t = \sigma(W^h h_{t-1} + W^i x_t) \\ & y_t = \sigma(W^o h_t) \end{align} \]
LSTM \( \begin{align} & i_t = sigm(W^{xi}x_t + W^{hi}h_{t-1}) \\ & f_t = sigm(W^{xf}x_t + W^{hf}h_{t-1}) \\ & o_t = sigm(W^{xo}x_t + W^{ho}h_{t-1}) \\ & \tilde{c_t} = tanh(W^{xc}x_t + W^{hc}h_{t-1}) \\ & c_t = f_t \bigodot c_{t-1} + i_t \bigodot \tilde{c_t} \\ & h_t = o_t \bigodot tanh(c_t) \\ & y_t = \sigma(W^o h_t) \end{align} \) LSTM内部主要有阶段：</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-07-16-%E9%80%9A%E8%BF%87tensorflow2%E4%BD%BF%E7%94%A8bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F.html rel=bookmark>通过Tensorflow2使用Bert预训练模型的两种方式</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-07-16T21:07:00Z>2020-07-16</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">以中文Bert为例
下面的例子均以中文Bert预训练模型为例
方式1：使用Tensorflow Hub import tensorflow as tf import tensorflow_hub as hub hub_url_or_local_path = "https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/2" # 或者将模型下载到本地 # !wget "https://storage.googleapis.com/tfhub-modules/tensorflow/bert_zh_L-12_H-768_A-12/2.tar.gz" # !tar -xzvf 2.tar.gz -C bert_zh_L-12_H-768_A-12 # hub_url_or_local_path = "./bert_zh_L-12_H-768_A-12" def build_model(): input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_word_ids") input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="input_mask") segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name="segment_ids") bert_layer = hub.KerasLayer(hub_url_or_local_path, name='bert', trainable=True) pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output) return model model = build_model() model.summary() 使用这种方式的时候在执行build_model时bert的参数已经被加载进来了。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/tensorflow2.x_keras/tensorflow%E5%AE%9E%E6%88%981-tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html rel=bookmark>Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-06-24T20:43:00Z>2020-06-24</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/tensorflow2.x-keras.html rel=category>Tensorflow2.x keras</a></span></div></div></header><div class="content list__excerpt post__content clearfix">大家好，今天是Tensorflow实战系列的第一次分享，本次分享共有3个主题：
Tensorflow介绍 Tensorflow核心概念 使用Tensorflow实现手写数字识别任务 由于本次分享内容以实战为主不会涉及过多理论的讲解，但是分享过程中涉及到的理论知识也会做一个快速回顾。
Tensorflow Introduction Tensorflow 是由Google研发的开源软件库，它既是一个实现机器学习算法的接口，同时也是执行机器学习算法的框架，它对深度学习中常用的神经网络结构等算法进行了封装，因此开发人员可以快速的进行模型搭建。
1、Tensorflow 发展史 2011 年，Google Brain内部孵化出一个项目叫做DistBelief, 它是为深度神经网络构建的一个机器学习系统，是Tensorflow的前身。 2015年11月，Google正式发布了Tensorflow的白皮书并开源TensorFlow 0.1 版本。 2017年02月，Tensorflow正式发布了1.0.0版本，同时也标志着稳定版的诞生。 2019年10月，TensorFlow在经历七个多月(2019年3月1日-2019年10月1日)的2.0 Alpha 版本的更新迭代后发布 2.0 正式版。 通过上面的发展史我们可以看到，虽然经过了9年时间Tensorflow依然是目前最流行的深度学习框架之一。
2、Tensorflow VS Pytorch 上面说到Tensorflow是目前最流行的深度学习框架之一，那另一款可以和Tensorflow一较高下的深度学习框架就是-Pytorch了。Pytorch是由Facebook研发的一款开源的机器学习库，自16年发布以来发展非常迅猛。Tensorflow和Pytorch如何选择呢，我的看法是：都可以，虽然刚开始时Pytorch和Tensorflow还是差别较大的，比较Pytorch有动态图、类python的编程方式，Tensorflow则支持可视化，生产部署更加简单易用，但通过这几年的发展Pytorch和Tensorfow越来越像了，Tensorflow添加了动态图，而Pytorch也在工业部署上有了很大改善。因此在两都的选择上不必太过纠结。
import tensorflow as tf import matplotlib as mpl %matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd import os import sys print(sys.version) 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] for module in tf, np, mpl: print(module.</div></article></main><div class=pagination><span class="pagination__item pagination__item--current">1/8</span>
<a class="pagination__item pagination__item--next btn" href=/page/2.html>»</a></div></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-21-%E5%9F%BA%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%B9%B3%E5%9D%87%E7%9A%84%E7%83%AD%E8%AF%8D%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%95.html>基于贝叶斯平均的新词发现算法</a></li><li class=widget__item><a class=widget__link href=/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-13-han-for-document-classification.html>HAN for Document Classification</a></li><li class=widget__item><a class=widget__link href=/post/other/2021-05-06-%E4%BD%BF%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%88%B0github-pages.html>使用GitHub Actions自动部署hugo到GitHub Pages</a></li><li class=widget__item><a class=widget__link href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-10-16-deeplearning%E4%B8%ADcrf%E7%9A%84tensorflow%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html>DeepLearning中CRF的Tensorflow代码实现</a></li><li class=widget__item><a class=widget__link href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-10-16-deeplearning%E4%B8%ADcrf%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86.html>DeepLearning中CRF计算原理</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/categories/algorithm.html>algorithm</a></li><li class=widget__item><a class=widget__link href=/categories/deep-learning.html>deep learning</a></li><li class=widget__item><a class=widget__link href=/categories/machine-learning.html>machine learning</a></li><li class=widget__item><a class=widget__link href=/categories/mathematics.html>mathematics</a></li><li class=widget__item><a class=widget__link href=/categories/nlp.html>NLP</a></li><li class=widget__item><a class=widget__link href=/categories/other.html>other</a></li><li class=widget__item><a class=widget__link href=/categories/reading.html>reading</a></li><li class=widget__item><a class=widget__link href=/categories/tensorflow2.x-keras.html>Tensorflow2.x keras</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=/tags/acme.html title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=/tags/albert.html title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/batch-normalization.html title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/bert.html title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/blog.html title=blog>blog</a>
<a class="widget-taglist__link widget__link btn" href=/tags/crf.html title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=/tags/deep-learning.html title="Deep Learning">Deep Learning</a>
<a class="widget-taglist__link widget__link btn" href=/tags/git.html title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=/tags/github-actions.html title="github actions">github actions</a>
<a class="widget-taglist__link widget__link btn" href=/tags/github-pages.html title="github pages">github pages</a>
<a class="widget-taglist__link widget__link btn" href=/tags/guice.html title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hexo.html title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hugo.html title=hugo>hugo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/kaggle.html title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=/tags/keras.html title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=/tags/layer-normalization.html title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/linux.html title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=/tags/log.html title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=/tags/logback.html title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=/tags/nginx.html title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=/tags/openwrt.html title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=/tags/slf4j.html title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=/tags/stanford-corenlp.html title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow.html title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow-hub.html title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow2.0.html title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tra.html title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=/tags/transformers.html title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=/tags/ubuntu.html title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%88%86%E8%AF%8D.html title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%93%88%E5%B8%8C.html title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D.html title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%B9%B6%E5%8F%91.html title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%8E%92%E5%BA%8F.html title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%A3%E5%88%97%E8%A1%A8.html title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E.html title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0.html title=新词发现>新词发现</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%97%A0%E7%9B%91%E7%9D%A3.html title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%86%B5.html title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%AE%97%E6%B3%95.html title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0.html title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.html title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script></body></html>