<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>transformers on Writing Lite</title><link>/tags/transformers/</link><description>Recent content in transformers on Writing Lite</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="/tags/transformers/index.xml" rel="self" type="application/rss+xml"/><item><title>Transformers的一些迷思</title><link>/post/deep-learning/2020-05-09-transformers%E7%9A%84%E4%B8%80%E4%BA%9B%E8%BF%B7%E6%80%9D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/post/deep-learning/2020-05-09-transformers%E7%9A%84%E4%B8%80%E4%BA%9B%E8%BF%B7%E6%80%9D/</guid><description>通过from_pretrained缓存的模型在哪 如果调用from_pretrained方法时指定了cache_dir 则保存到cache_dir，
cache_dir = kwargs.pop(&amp;#34;cache_dir&amp;#34;, None) 如果没指定则去通过系统环境变量寻找（&amp;ldquo;PYTORCH_TRANSFORMERS_CACHE&amp;rdquo;&amp;quot;, &amp;ldquo;PYTORCH_PRETRAINED_BERT_CACHE&amp;rdquo;）
os.getenv(&amp;#34;PYTORCH_TRANSFORMERS_CACHE&amp;#34;, os.getenv(&amp;#34;PYTORCH_PRETRAINED_BERT_CACHE&amp;#34;, default_cache_path)) 如果还没找到则设置为pytorch_home下的transformers目录下
from torch.hub import _get_torch_home torch_cache_home = _get_torch_home() os.path.join(torch_cache_home, &amp;#34;transformers&amp;#34;) from_pretrained方法是如何加载模型的 首先判断是否在pretrained_model_archive_map中，然后判断是否为目录或文件，如果都不是则默认为hf_bucket_url
https://s3.amazonaws.com/models.huggingface.co/bert/{pretrained_model_name_or_path}/{pytorch_model.bin/tf_model.h5} pytorch_model.bin或tf_model.h5 通过from_tf判断
不同模型实现from_pretrained的方式 from_pretrained 的根据不同 cls 来实现加载不同模型的差异， 以bert为例， cls -&amp;gt; BertPreTrainedModel；
class BertPreTrainedModel(PreTrainedModel): &amp;#34;&amp;#34;&amp;#34; An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models. &amp;#34;&amp;#34;&amp;#34; config_class = BertConfig pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP load_tf_weights = load_tf_weights_in_bert base_model_prefix = &amp;#34;bert&amp;#34; def _init_weights(self, module): &amp;#34;&amp;#34;&amp;#34; Initialize the weights &amp;#34;&amp;#34;&amp;#34; if isinstance(module, (nn.</description></item><item><title>通过Tensorflow2使用Bert预训练模型的两种方式</title><link>/post/deep-learning/2020-07-16-%E9%80%9A%E8%BF%87tensorflow2%E4%BD%BF%E7%94%A8bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/post/deep-learning/2020-07-16-%E9%80%9A%E8%BF%87tensorflow2%E4%BD%BF%E7%94%A8bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/</guid><description>以中文Bert为例
下面的例子均以中文Bert预训练模型为例
方式1：使用Tensorflow Hub import tensorflow as tf import tensorflow_hub as hub hub_url_or_local_path = &amp;#34;https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/2&amp;#34; # 或者将模型下载到本地 # !wget &amp;#34;https://storage.googleapis.com/tfhub-modules/tensorflow/bert_zh_L-12_H-768_A-12/2.tar.gz&amp;#34; # !tar -xzvf 2.tar.gz -C bert_zh_L-12_H-768_A-12 # hub_url_or_local_path = &amp;#34;./bert_zh_L-12_H-768_A-12&amp;#34; def build_model(): input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&amp;#34;input_word_ids&amp;#34;) input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&amp;#34;input_mask&amp;#34;) segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&amp;#34;segment_ids&amp;#34;) bert_layer = hub.KerasLayer(hub_url_or_local_path, name=&amp;#39;bert&amp;#39;, trainable=True) pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output) return model model = build_model() model.summary() 使用这种方式的时候在执行build_model时bert的参数已经被加载进来了。</description></item></channel></rss>