<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>tra on Writing Lite</title><link>/tags/tra/</link><description>Recent content in tra on Writing Lite</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><atom:link href="/tags/tra/index.xml" rel="self" type="application/rss+xml"/><item><title>Tensorflow2.0使用bert：transformers与kashgaria</title><link>/post/deep-learning/2019-11-22-tensorflow2.0%E4%BD%BF%E7%94%A8berttransformers%E4%B8%8Ekashgari/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/post/deep-learning/2019-11-22-tensorflow2.0%E4%BD%BF%E7%94%A8berttransformers%E4%B8%8Ekashgari/</guid><description>**背景：**最近打算做NER任务，打算使用kashgaria当baseline，然后使用transformers来做模型开发。
**问题：**使用kashgaria来做Baseline，使用人民日报语料可以得到99.99%的准确率，但是使用transformers的TFBertForTokenClassification来做的话只能得到75%左右的效果， 看了两个项目的源码开始怀疑是模型结构的问题，因此使用transformers模拟kashgaria的结果又做了一次，效果还是在75%左右。相关代码如下：
kashgaria代码：
from kashgari.corpus import ChineseDailyNerCorpus from kashgari.tasks.labeling import BiLSTM_Model from kashgari.embeddings import BERTEmbedding import kashgari train_x, train_y = ChineseDailyNerCorpus.load_data(&amp;#39;train&amp;#39;) test_x, test_y = ChineseDailyNerCorpus.load_data(&amp;#39;test&amp;#39;) valid_x, valid_y = ChineseDailyNerCorpus.load_data(&amp;#39;valid&amp;#39;) bert_embed = BERTEmbedding(&amp;#39;chinese_L-12_H-768_A-12&amp;#39;, task=kashgari.LABELING, sequence_length=100) model = BiLSTM_Model(bert_embed) model.fit(train_x, train_y, valid_x, valid_y, epochs=1) transformers代码：
# 数据的处理省略，但使用的同样语料 model = TFBertForTokenClassification.from_pretrained(&amp;#39;bert-base-chinese&amp;#39;) model.compile(loss=&amp;#39;categorical_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;], optimizer=&amp;#39;adam&amp;#39;) model.fit(all_input_ids, tf.constant(all_label_ids_ca, dtype=tf.float32), epochs=2, batch_size=32) transformers 模拟kashgaria代码：
class BERT_NER(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self.</description></item></channel></rss>