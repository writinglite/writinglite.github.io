<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>bert on Writing Lite</title><link>/tags/bert.html</link><description>Recent content in bert on Writing Lite</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 16 Jul 2020 21:07:00 +0000</lastBuildDate><atom:link href="/tags/bert/index.xml" rel="self" type="application/rss+xml"/><item><title>通过Tensorflow2使用Bert预训练模型的两种方式</title><link>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-07-16-%E9%80%9A%E8%BF%87tensorflow2%E4%BD%BF%E7%94%A8bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F.html</link><pubDate>Thu, 16 Jul 2020 21:07:00 +0000</pubDate><guid>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-07-16-%E9%80%9A%E8%BF%87tensorflow2%E4%BD%BF%E7%94%A8bert%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F.html</guid><description>以中文Bert为例
下面的例子均以中文Bert预训练模型为例
方式1：使用Tensorflow Hub import tensorflow as tf import tensorflow_hub as hub hub_url_or_local_path = &amp;#34;https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/2&amp;#34; # 或者将模型下载到本地 # !wget &amp;#34;https://storage.googleapis.com/tfhub-modules/tensorflow/bert_zh_L-12_H-768_A-12/2.tar.gz&amp;#34; # !tar -xzvf 2.tar.gz -C bert_zh_L-12_H-768_A-12 # hub_url_or_local_path = &amp;#34;./bert_zh_L-12_H-768_A-12&amp;#34; def build_model(): input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&amp;#34;input_word_ids&amp;#34;) input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&amp;#34;input_mask&amp;#34;) segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=&amp;#34;segment_ids&amp;#34;) bert_layer = hub.KerasLayer(hub_url_or_local_path, name=&amp;#39;bert&amp;#39;, trainable=True) pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output) return model model = build_model() model.summary() 使用这种方式的时候在执行build_model时bert的参数已经被加载进来了。</description></item><item><title>Transformers的一些迷思</title><link>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-05-09-transformers%E7%9A%84%E4%B8%80%E4%BA%9B%E8%BF%B7%E6%80%9D.html</link><pubDate>Sat, 09 May 2020 21:07:00 +0000</pubDate><guid>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-05-09-transformers%E7%9A%84%E4%B8%80%E4%BA%9B%E8%BF%B7%E6%80%9D.html</guid><description>通过from_pretrained缓存的模型在哪 如果调用from_pretrained方法时指定了cache_dir 则保存到cache_dir，
cache_dir = kwargs.pop(&amp;#34;cache_dir&amp;#34;, None) 如果没指定则去通过系统环境变量寻找（&amp;ldquo;PYTORCH_TRANSFORMERS_CACHE&amp;rdquo;&amp;quot;, &amp;ldquo;PYTORCH_PRETRAINED_BERT_CACHE&amp;rdquo;）
os.getenv(&amp;#34;PYTORCH_TRANSFORMERS_CACHE&amp;#34;, os.getenv(&amp;#34;PYTORCH_PRETRAINED_BERT_CACHE&amp;#34;, default_cache_path)) 如果还没找到则设置为pytorch_home下的transformers目录下
from torch.hub import _get_torch_home torch_cache_home = _get_torch_home() os.path.join(torch_cache_home, &amp;#34;transformers&amp;#34;) from_pretrained方法是如何加载模型的 首先判断是否在pretrained_model_archive_map中，然后判断是否为目录或文件，如果都不是则默认为hf_bucket_url
https://s3.amazonaws.com/models.huggingface.co/bert/{pretrained_model_name_or_path}/{pytorch_model.bin/tf_model.h5} pytorch_model.bin或tf_model.h5 通过from_tf判断
不同模型实现from_pretrained的方式 from_pretrained 的根据不同 cls 来实现加载不同模型的差异， 以bert为例， cls -&amp;gt; BertPreTrainedModel；
class BertPreTrainedModel(PreTrainedModel): &amp;#34;&amp;#34;&amp;#34; An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models. &amp;#34;&amp;#34;&amp;#34; config_class = BertConfig pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP load_tf_weights = load_tf_weights_in_bert base_model_prefix = &amp;#34;bert&amp;#34; def _init_weights(self, module): &amp;#34;&amp;#34;&amp;#34; Initialize the weights &amp;#34;&amp;#34;&amp;#34; if isinstance(module, (nn.</description></item><item><title>Albert在Bert基础上的几点改进</title><link>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-02-20-albert%E5%9C%A8bert%E5%9F%BA%E7%A1%80%E4%B8%8A%E7%9A%84%E5%87%A0%E7%82%B9%E6%94%B9%E8%BF%9B.html</link><pubDate>Thu, 20 Feb 2020 21:07:00 +0000</pubDate><guid>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-02-20-albert%E5%9C%A8bert%E5%9F%BA%E7%A1%80%E4%B8%8A%E7%9A%84%E5%87%A0%E7%82%B9%E6%94%B9%E8%BF%9B.html</guid><description>减少参数 减少Embeding参数 ，用两层替代之前的一层，参数从原来的V * H 变成 V * E + E * H ， 这个E &amp;laquo; H 共享Block参数 ​ 这样做的好处是，将参数减少，进而增加模型的深度和宽度来提升模型效果，但同时带来了计算量的增加（大概3倍）
改进训练任务 通过实验表示，Next Sentence Predict 任务太过简单，使用 Reverce 的方式会更好；
去掉Dropout Dropout实际的操作是防止过拟合，但对于无监督学习来说，训练语料是很多的不会有过拟合的问题，使用Dropout反而会增加内存的使用（会有一些缓存），去掉Dropout会有0.3的性能提升
增加训练数据 这个就没啥说的了
最重要的一点还是减少参数增加模型的深度和宽度带来的</description></item><item><title>Tensorflow2.0 基于BERT的开发实战（huggingface-transformers）</title><link>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-12-12-tensorflow2.0-%E5%9F%BA%E4%BA%8Ebert%E7%9A%84%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98huggingface-transformers.html</link><pubDate>Thu, 12 Dec 2019 08:07:00 +0000</pubDate><guid>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-12-12-tensorflow2.0-%E5%9F%BA%E4%BA%8Ebert%E7%9A%84%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98huggingface-transformers.html</guid><description>tensorflow2.0 刚刚发布正式版，网上关于tf2.0使用bert的文章较少。之前想tf2.0做基于bert的NER任务，想了不少资料踩了许多坑，因此将期间的过程总结成文。
在tf2.0上使用bert，主要的难点还是：自己写tf2.0代码适配google的官方源码的方案工作量大对tf熟练度的要求比较高。因此选择了第三方的实现，这里使用的是huggingface的transformers。
除了transformers，其它兼容tf2.0的bert项目还有：
keras-bert（Star:1.3k, Kashgari在使用）, 现在也开始兼容tf2.0了，但它只支持bert一种预训练模型
bert-for-tf2（Star:280），缺点是不是很正规，只给了tf2.0 pipeline示例
在tf2.0正式版发布前后huggingface的transformers也发布了transformers2.0，开始支持tf.2.0的各个预训练模型，虽然没有对pytorch支持的那么全面但在我们的场景已经足够适用了。
环境 tensorflow版本：2.0.0
transformers版本：2.2.1
构建模型 class BertNerModel(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self.bert_layer = TFBertMainLayer(config, name=&amp;#39;bert&amp;#39;) self.bert_layer.trainable = False self.concat_layer = tf.keras.layers.Concatenate(name=&amp;#39;concat_bert&amp;#39;) def call(self, inputs): outputs = self.bert_layer(inputs) #将后n层的结果相连 tensor = self.concat_layer(list(outputs[2][-4:])) 这里给出的是简要的代码，可以自行根据任务在bert_layer之后加入RNN等
自定义模型的写法可以参考官方源码里的TFBertForSequenceClassification， 继承TFBertPreTrainedModel
self.bert_layer(inputs)的返回值为tuple类型：
最后1层隐藏层的输出值，shape=(batch_size, max_length, hidden_dimention) [CLS] 对应的输出值，shape=(batch_size, hidden_dimention) 只有设置了config.output_hidden_states = True，才有该值，所有隐藏层的输出值，返回值类型是list 每个list里的值的shape是`(batch_size, max_length, hidden_dimention)`` 模型的初始化 bert_ner_model = BertNerModel.</description></item><item><title>Tensorflow2.0使用bert：transformers与kashgaria</title><link>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-11-22-tensorflow2.0%E4%BD%BF%E7%94%A8berttransformers%E4%B8%8Ekashgari.html</link><pubDate>Fri, 22 Nov 2019 11:07:00 +0000</pubDate><guid>/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-11-22-tensorflow2.0%E4%BD%BF%E7%94%A8berttransformers%E4%B8%8Ekashgari.html</guid><description>**背景：**最近打算做NER任务，打算使用kashgaria当baseline，然后使用transformers来做模型开发。
**问题：**使用kashgaria来做Baseline，使用人民日报语料可以得到99.99%的准确率，但是使用transformers的TFBertForTokenClassification来做的话只能得到75%左右的效果， 看了两个项目的源码开始怀疑是模型结构的问题，因此使用transformers模拟kashgaria的结果又做了一次，效果还是在75%左右。相关代码如下：
kashgaria代码：
from kashgari.corpus import ChineseDailyNerCorpus from kashgari.tasks.labeling import BiLSTM_Model from kashgari.embeddings import BERTEmbedding import kashgari train_x, train_y = ChineseDailyNerCorpus.load_data(&amp;#39;train&amp;#39;) test_x, test_y = ChineseDailyNerCorpus.load_data(&amp;#39;test&amp;#39;) valid_x, valid_y = ChineseDailyNerCorpus.load_data(&amp;#39;valid&amp;#39;) bert_embed = BERTEmbedding(&amp;#39;chinese_L-12_H-768_A-12&amp;#39;, task=kashgari.LABELING, sequence_length=100) model = BiLSTM_Model(bert_embed) model.fit(train_x, train_y, valid_x, valid_y, epochs=1) transformers代码：
# 数据的处理省略，但使用的同样语料 model = TFBertForTokenClassification.from_pretrained(&amp;#39;bert-base-chinese&amp;#39;) model.compile(loss=&amp;#39;categorical_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;], optimizer=&amp;#39;adam&amp;#39;) model.fit(all_input_ids, tf.constant(all_label_ids_ca, dtype=tf.float32), epochs=2, batch_size=32) transformers 模拟kashgaria代码：
class BERT_NER(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self.</description></item></channel></rss>