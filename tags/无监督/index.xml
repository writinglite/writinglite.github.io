<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>无监督 on Writing Lite</title><link>/tags/%E6%97%A0%E7%9B%91%E7%9D%A3.html</link><description>Recent content in 无监督 on Writing Lite</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 27 Nov 2016 09:03:00 +0000</lastBuildDate><atom:link href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3/index.xml" rel="self" type="application/rss+xml"/><item><title>EM算法</title><link>/post/machine-learning/2016-11-27-em%E7%AE%97%E6%B3%95.html</link><pubDate>Sun, 27 Nov 2016 09:03:00 +0000</pubDate><guid>/post/machine-learning/2016-11-27-em%E7%AE%97%E6%B3%95.html</guid><description>EM算法和最大似然估计一样是一种参数估计方法，与最大似然估计不同的是EM算法可以对着包含隐变量的数据进行参数估计。EM算法的思想是：若参数$\Theta$已知，则可根据训练数据推断出隐变量Z的值（E步）；反之，若Z的值已知，则可方便地对参数$\Theta$做极大似然估计（M步）。
Jensen不等式 令f(x)是一个凸函数(e.g f''(x)&amp;gt;=0,二阶导数大于0)，令x为随机变量。 那么， $$ f(E[x])&amp;lt;=E[f(x)] $$ 用一句话表达Jensen不等式，当函数是凸函数，那么该函数的期望大于等于期望的函数值。当X=E(X),当X为常量概率为1，E[f(x)] = f(E[x])。
如图，有0.5的概率是a，有0.5的概率是b。（就像掷硬币一样）。X的期望值就是a和b的中值了。
同理，对于凹函数，f''&amp;lt;=0,$f(E[x])&amp;gt;=E[f(x)]$。
##EM算法 假定有训练数据集 $$ { x^{(1)} , x^{(2)} , x^{(3)} \dots x^{(m)} } $$ 样本相互独立，我们想找到每个样例隐含的类别z。 模型$P(x,z;\theta)$,只能观测到x，对数似然函数， $$ \begin{align} l(\theta) &amp;amp;= \sum^m_{i=1}\log P(x^i;\theta) \
&amp;amp;= \sum^m_{i=1}\log \sum_{z^i} P(x^i,z^i;\theta) \end{align} $$ 然后我们求极大似然 $$ \begin{align} \sum^m_{i=1}\log \sum_{z^i} P(x^i,z^i;\theta) &amp;amp; = \sum_i\log\sum_{z^{(i)}}P(x^{(i)},z^{(i)};\theta) \
&amp;amp; = \sum_i \log \sum_{z^{(i)}} Q(z^{(i)}) \frac{P(x^{(i)},z^{(i)};\theta)}{Q(z^{(i)})} \
&amp;amp; \ge \sum_i \sum_{z^{(i)}} Q(z^{(i)}) \log \frac{P(x^{(i)},z^{(i)};\theta)}{Q(z^{(i)})} \end{align} $$ 最后一步用到了Jensen不等式，f(x)的f对应log函数，x对应$ \frac{P(x^{(i)},z^{(i)};\theta)}{Q(z^{(i)})}$，p(x)对应$Q(z^{(i)})$。那么$f(E[x])$对应$\log \sum_{z^{(i)}} Q(z^{(i)}) \frac{P(x^{(i)},z^{(i)};\theta)}{Q(z^{(i)})}$，$E[f(x)]$对应$\sum_{z^{(i)}} Q(z^{(i)}) \log \frac{P(x^{(i)},z^{(i)};\theta)}{Q(z^{(i)})}$。 因此$Q(z^{(i)})$代表的是p(x)也就是概率，所以显然 $$ \sum_{z^{(i)}} Q(z^{(i)}) = 1 , Q(z^{(i)})&amp;gt;0 $$</description></item></channel></rss>