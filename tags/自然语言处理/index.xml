<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>自然语言处理 on Writing Lite</title><link>/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.html</link><description>Recent content in 自然语言处理 on Writing Lite</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 20 May 2021 16:18:00 +0000</lastBuildDate><atom:link href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><item><title>基于贝叶斯平均的新词发现算法</title><link>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-21-%E5%9F%BA%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%B9%B3%E5%9D%87%E7%9A%84%E7%83%AD%E8%AF%8D%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%95.html</link><pubDate>Thu, 20 May 2021 16:18:00 +0000</pubDate><guid>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-21-%E5%9F%BA%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%B9%B3%E5%9D%87%E7%9A%84%E7%83%AD%E8%AF%8D%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%95.html</guid><description>这里对“热词”的定义是：
某一时间段内起点低，增长迅速
贝叶斯平均算法的计算公式是：
\[ WR=\frac{v}{v+m}R+\frac{m}{v+m}C \]
WR， 加权得分。 R，该词基础得分。 v，该词的词频。 m，平均词频数。 C， 所有词的平均基础得分。 R的计算公式：
\[ R = \frac{今天词频}{今天词频 + 前一天的词频} \]
举例：
词 前一天的词频 今天的词频 基础得分 加权得分 A 5 10 0.66 0.619 B 50 100 0.66 0.640 C 50 50 0.50 0.552 average m = 88.33 C = 0.61 简单解释上面的公式，加权得分由两部分组成，一个是该词的基础得分，另一个是所有词的平均基础得分，$\frac{v}{v+m}$和$\frac{m}{v+m}$分别是这两部分的系数；当v=0时，WR实际取的C；当v越来越大时，R的占比会越来越大。达到效果是，当该词的R &amp;gt; C时，在相同R的前提下，v越大WR越大。如上面的例子，A和B的基础得分都是0.</description></item><item><title>HAN for Document Classification</title><link>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-13-han-for-document-classification.html</link><pubDate>Wed, 12 May 2021 16:18:00 +0000</pubDate><guid>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-13-han-for-document-classification.html</guid><description>论文链接：Hierarchical Attention Networks for Document Classification
HAN (Hierarchical Attention Networks for Document Classification) 是一个针对文本分类任务的层次化 attention 模型。它有个显著的特点:通过&amp;quot;词-句子-文章&amp;quot;的层次化结构来表示一篇文本。
HAN 模型的灵感来源于人在阅读 document 的时候，不同的词和句子对人理解 document 信息有不同的影响。因为，词和句子的重要性是和上下文息息相关的，即使是相同的词和句子，在不同的上下文中重要性也不一样。人在阅读一篇文章时，对 document 不同的内容是有着不同的注意度的。
attention 的目的是要把一个句子中，对句子的含义影响最大的词语找出来。
论文通过将 $h_{it}$ 输入到一个 dense 网络中得到的结果$u_{it}$ 作为$h_{it}$的隐含表示。$h_{it}$可以是双向RNN的结果，或者bert中经过encoder后的结果。
\[ u_{it}=\tanh(W_wh_{it}+b_w) \]
为了衡量单词的重要性，这里用$u_{it}$ 和一个随机初始化的上下文向量$uw$ 的相似度来表示，然后经过 softmax 操作获得了一个归一化的 attention 权重矩阵 $\alpha{it}$，代表句子$i$中第 $t$个词的权重。
\[ \alpha_{it}=\frac{\exp(u^T_{it}u_w)}{\sum_t(\exp(u^T_{it}u_w))} \]
得到了 attention 权重矩阵之后，句子向量 $s_i$ 可以看作这些词向量的加权求和。这里的上下文向量$u_w$ 是在训练网络的过程中学习获得的。我们可以把 $u_w$当作一种询问的高级表示，比如&amp;quot;哪些词含有比较重要的信息?
\[ s_i=\sum_t\alpha_{it}h_{it} \]
使用这个句子向量就可以做句子level的任务，例如文本分类等。
主要查考：https://zhuanlan.zhihu.com/p/54165155</description></item><item><title>基于统计的词切分和标注一体化模型</title><link>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017-09-09-%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E8%AF%8D%E5%88%87%E5%88%86%E5%92%8C%E6%A0%87%E6%B3%A8%E4%B8%80%E4%BD%93%E5%8C%96%E6%A8%A1%E5%9E%8B.html</link><pubDate>Sat, 09 Sep 2017 16:18:00 +0000</pubDate><guid>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017-09-09-%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E8%AF%8D%E5%88%87%E5%88%86%E5%92%8C%E6%A0%87%E6%B3%A8%E4%B8%80%E4%BD%93%E5%8C%96%E6%A8%A1%E5%9E%8B.html</guid><description>基于统计的切记和标注一体化模型 假设$C=c_1,c_2 \dots c_m$为m个的字符串，$W=w_1,_2 \dots w_n$是把C切分后得到的由n个词组成的词序列，$T=t_1,t_2 \dots t_n$是对W进行标注后的标记序列。
$$ P(W|C) = \frac{P(C|W)P(W)}{P(C)} $$ 可知P(C)是一个确定的值，P(C|W)是在给定词序列的情况下字符串的概率，可以认为是1。因此可得： $$ \max P(W|C) = \max P(W) $$ 即是说，分词的过程即是寻找概率最大词序列的过程。 我们再来考虑词序列与词序列与词性标注序列的关系。 $$ P(T|W) = \frac{P(W|T)P(T)}{P(W)} $$ 可推导出： $$ P(W) = \frac{P(W|T)P(T)}{P(T|W)} $$ 如果隐马尔可夫假设和独立输出假设 即： $$ P(T) = \prod_i^n P(t_i)P(t_{i-1}) \
P(W|T) = \prod_i^n P(|w_i,t_i) \
P(T|W) = \prod_i^n P(|t_i,w_i) $$
计算方法：
找到一条分词路径W 用记性标注的模型计算具有最大概率的记性标注序列，得到对应的标注路径T。 利用公式可得到W的概率 若干个健忘路径中，概率最大的W即为分词结果 引用： 白栓虎 - 基于统计的切记和标注一体化模型</description></item><item><title>MMSeg分词方法</title><link>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017-09-08-mmseg%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95.html</link><pubDate>Fri, 08 Sep 2017 20:18:00 +0000</pubDate><guid>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017-09-08-mmseg%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95.html</guid><description>概述 陈和刘（1992）完成的最大匹配的另一个变体比基本形式更复杂。 这种演算法指出，最可能的分词方式是三个单词，方式为从一个字串的第一个字开始，寻找分词的方式，只要存在有不同意义的词。
MMSeg的四个规则 规则 1：最大匹配(Maximum matching) Simple方法：取最大长度的单词。 Complex方法：匹配出所有的“三个词的词组”（原文中使用了chunk，这里感觉用“词组”比较合适），即从某一既定的字为起始位置，得到所有可能的“以三个词为一组”的所有组合。比如“研究生命起源”，可以得到
研_究_生 研_究_生命 研究生_命_起源 研究_生命_起源 规则 2：最大平均单词长度(Largest average word length) 经过规则1过滤后，如果剩余的词组超过1个，那就选择平均词语长度最大的那个（平均词长＝词组总字数／词语数量）。比如“生活水平”，可能得到如下词组：
生_活水_平 (4/3=1.33) 生活_水_平 (4/3=1.33) 生活_水平 (4/2=2) 根据此规则，就可以确定选择“生活_水平”这个词组
规则 3：单词长度的最小方差(Smallest variance of word lengths) 由于词语长度的变化率可以由标准差反映，所以此处直接套用标准差公式即可。比如
研究_生命_起源 （标准差=sqrt(((2-2)^2+(2-2)^2+(2-2^2))/3)=0） 研究生_命_起源 （标准差=sqrt(((2-3)^2+(2-1)^2+(2-2)^2)/3)=0.8165） 于是选择“研究_生命_起源”这个词组。
规则 4：单字单词的语素自由度的最大和(Largest sum of degree of morphemic freedom of one-character words) 其中degree of morphemic freedom可以用一个数学公式表达：log(frequency)，即词频的自然对数（这里log表示数学中的ln）。这个规则的意思是“计算词组中的所有单字词词频的自然对数，然后将得到的值相加，取总和最大的词组”。比如：
设施_和服_务 设施_和_服务 这两个词组中分别有“务”和“和”这两个单字词，假设“务”作为单字词时候的频率是5，“和”作为单字词时候的频率是10，对5和10取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施_和_服务”。 也许会问为什么要对“词频”取自然对数呢？可以这样理解，词组中单字词词频总和可能一样，但是实际的效果并不同，比如
A_BBB_C （单字词词频，A:3， C:7） DD_E_F （单字词词频，E:5，F:5） 表示两个词组，A、C、E、F表示不同的单字词，如果不取自然对数，单纯就词频来计算，那么这两个词组是一样的（3+7=5+5），但实际上不同的词频范围所表示的效果也不同，所以这里取自然对数，以表区分（ln(3)+ln(7) &amp;lt; ln(5)+ln(5)， 3.0445&amp;lt;3.2189）。
总结 这个四个过滤规则中，如果使用simple的匹配方法，只能使用第一个规则过滤，如果使用complex的匹配方法，则四个规则都可以使用。实际使用中，一般都是使用complex的匹配方法＋四个规则过滤。 看到这里也许对MMSEG的分词方法有了一个大致的了解，它是一个“直观”的分词方法。它把一个句子“尽可能长（这里的长，是指所切分的词尽可能的长）”“尽可能均匀”的去切分，与中文的语法习惯比较相符。</description></item><item><title>信息的度量熵</title><link>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017-07-06-%E4%BF%A1%E6%81%AF%E7%9A%84%E5%BA%A6%E9%87%8F%E7%86%B5.html</link><pubDate>Thu, 06 Jul 2017 23:18:00 +0000</pubDate><guid>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017-07-06-%E4%BF%A1%E6%81%AF%E7%9A%84%E5%BA%A6%E9%87%8F%E7%86%B5.html</guid><description>熵 一条信息的信息量与它的不确定性成正比关系。比如说，我们要搞清楚一件非常不确定的事，或是我们一无所知的事情，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，那么不需要太多的信息就能把它搞清楚。所以从这个角度来看，可以认为，信息量就等于不确定性的多少。
它的定义如下： $$ H(X)=-\sum_{x\in X}P(x) \log P(x) $$
条件熵 如果 H(Y|X=x) 为变数 Y 在 X 取特定值 x 条件下的熵，那么 H(Y|X)就是 H(Y|X=x) 在 X 取遍所有可能 x 后平均的结果： $$ \begin{align} H(Y|X) &amp;amp;= \sum_i^n P(x_i)H(Y|X=x_i) \
&amp;amp;= \sum_i^n P(x_i) \sum_j^m P(y_j) \log P(y_j|x_i) \
&amp;amp;= \sum_i^n \sum_j^m P(x_i,y_j) \log P(y_j|x_i) \
&amp;amp;= - \sum_{x\in X,y \in Y}P(x,y) \log P(y|x) \
\end{align} $$ 现在假定我们还知道X的一些情况，包括它和Y一起出现的概率，以及在X取不同值的前提下X的概率分布。定义在X的条件下Y的条件熵为： $$ H(Y|X)=-\sum_{x\in X,y \in Y}P(x,y) \log P(y|x) $$ H(Y)&amp;gt;=H(Y|X)，也就是说Y的不确定性下降了。
互信息 互信息有时也叫“信息增益”。当获取的信息和要研究的事物“有关系”时，这些信息才能帮助我们消除不确定性。当然“有关系”这种说法太模糊，太不科学，最好能度化的度量“相关性”。香农在信息论中提出了一个“互信息”的概念作为对两个随机事件“相关性”的量化度量。假定有两个随机事件X和Y，它们的互信息定义如下： $$ I(X;Y)=\sum_{x \in X,y \in Y}P(x,y) \log \frac{P(x,y)}{P(x)P(y))} $$ 其实这个互信息就是，随机事件X的不确定性或者说熵H(X)，以及在知道随机事件Y条件下的不确定性或者说H(X|Y)之间的差异，即： $$ I(X;Y)=H(X) - H(X|Y) $$ 也就是说，**所谓的两个事件相关性的度量，就是在了解了其中一个Y的前提下，对消除另一个X不确定性所提供的信息量**。当X和Y完全相关是，它的取值是1；当两者完全无关时，它的取值是0。</description></item><item><title>Stanford CoreNLP 使用</title><link>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017-05-21-stanford-corenlp-%E4%BD%BF%E7%94%A8.html</link><pubDate>Sun, 21 May 2017 13:18:00 +0000</pubDate><guid>/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017-05-21-stanford-corenlp-%E4%BD%BF%E7%94%A8.html</guid><description>介绍 斯坦福CoreNLP提供了一套自然语言分析工具。斯坦福CoreNLP集成了许多斯坦福的NLP工具，包括词性（POS），命名实体识别（NER）， 语法解析，指代消解，情感分析，自举模式学习和开放信息提取工具。
如果有如下需求，要以选择Stanford CoreNLP：
具有良好语法分析工具的集成工具包 快速，可靠地分析任意文本 整体最高品质的文字分析 支持一些主要（人类）语言 可用于大多数主要现代编程语言的接口 能够作为简单的Web服务运行 Github地址：Stanford CoreNLP GitHub site.
使用前准备 Stanford CoreNLP是基于JAVA的，最新版本需要JDK1.8。JAR包可以通用官网，或者MAVEN下载到。如果使用MAVEN的话，可以使用如下配置。
&amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;edu.stanford.nlp&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;stanford-corenlp&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.7.0&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;edu.stanford.nlp&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;stanford-corenlp&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.7.0&amp;lt;/version&amp;gt; &amp;lt;classifier&amp;gt;models&amp;lt;/classifier&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; 如果你想从阿拉伯语，中文，德语或西班牙语中获得Maven的语言模型，请将其添加到您的pom.xml：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;edu.stanford.nlp&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;stanford-corenlp&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.7.0&amp;lt;/version&amp;gt; &amp;lt;classifier&amp;gt;models-chinese&amp;lt;/classifier&amp;gt; &amp;lt;/dependency&amp;gt; 将“models-chinese”替换为“models-english”，“models-chinese-kbp”，“models-arabic”，“models-french”，“models-german”或“models-spanish”为其他语言！
由于Stanford CoreNLP的模型文件太大（中文的模型就几百M），建议到官网下载。如果使用MAVEN的话，最好使用阿里的MAVEN仓库，这样速度更快些。
通过命令行使用 Quick Start java -cp &amp;quot;*&amp;quot; -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt -cp &amp;quot;*&amp;quot;是加载当前路径下的所有文件（主要是JAR包）
该-props参数是可选的。默认情况下，Stanford CoreNLP将在您的类路径中搜索StanfordCoreNLP.properties，并使用分发中包含的默认值。
该-annotators参数实际上是可选的。如果你离开它，代码使用一个内建的属性文件，它可以使用以下注释器：标记化和句子分割，POS标记，缩小，NER，解析和关联解析（也就是我们在这些例子中使用的）。
如果要使用其它语言的话，
java -mx3g -cp &amp;quot;*&amp;quot; edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -file chinese.txt -outputFormat text 参考： https://stanfordnlp.</description></item></channel></rss>