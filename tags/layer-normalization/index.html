<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Layer Normalization - Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Layer Normalization"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="writinglite.com/tags/layer-normalization/"><meta itemprop=name content="Layer Normalization"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Layer Normalization"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/writinglite.com/css/style.css><link rel=stylesheet href=/writinglite.com/css/custom.css><link rel=alternate type=application/rss+xml href=writinglite.com/tags/layer-normalization/index.xml title="Writing Lite"><link rel="shortcut icon" href=/writinglite.com/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/writinglite.com title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><header class=main__header><h1 class=main__title>Layer Normalization</h1></header><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/deep-learning/2020-07-29-batch_normalization%E4%B8%8Elayer_normalization%E7%9A%84%E7%90%86%E8%A7%A3%E6%95%B4%E7%90%86/ rel=bookmark>Batch Normalization与Layer Normalization的理解整理</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">问题背景　 　接着引入covariate shift的概念：如果ML系统实例集合&lt;X,Y>中的输入值X的分布老是变，这不符合IID假设，网络模型很难稳定的学规律，这不得引入迁移学习才能搞定吗，我们的ML系统还得去学习怎么迎合这种分布变化啊。对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。　我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal Covariate Shift”。Paper所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了Batch Normalization，这个牛逼算法的诞生。
Normalization 的通用框架 $$ \hat{x}=\frac{x-\mu}{\sigma^2} $$ $$ y = g\hat{x}+b $$
$\mu$和$\sigma^2$分别是均值和方差，它们是根据特征值计算出来的，g和b是需要训练过程中去学习的参数。
总结 Layer Normalization与Batch Normalization对比：　BN针对一个minibatch的输入样本，计算均值和方差，基于计算的均值和方差来对某一层神经网络的输入X中每一个case进行归一化操作。但BN有两个明显不足：1、高度依赖于mini-batch的大小，实际使用中会对mini-Batch大小进行约束，不适合类似在线学习（mini-batch为1）情况；2、不适用于RNN网络中normalize操作：BN实际使用时需要计算并且保存某一层神经网络mini-batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；但对于RNN来说，sequence的长度是不一致的，换句话说RNN的深度不是固定的，不同的time-step需要保存不同的statics特征，可能存在一个特殊sequence比其的sequence长很多，这样training时，计算很麻烦。但LN可以有效解决上面这两个问题。
LN适用于LSTM的加速，但用于CNN加速时并没有取得比BN更好的效果。
BN的特点
但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。
LN的缺点
BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。
参考： Batch Normalization（BN，批量归一化） 【深度学习】深入理解Batch Normalization批标准化 深度学习（二十九）Batch Normalization 学习笔记 深度学习加速策略BN、WN和LN的联系与区别，各自的优缺点和适用的场景？？</div></article></main></div><aside class="sidebar sidebar--left"><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=writinglite.com></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2020-03-02-%E5%9F%BA%E4%BA%8Enginx%E7%9A%84acme%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E6%96%B9%E6%A1%88/>基于nginx的acme免费证书方案</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-11-04-%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E5%AE%89%E8%A3%85-certbot/>免费证书安装-certbot</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-07-27-%E6%A0%91%E8%8E%93%E6%B4%BE3b+-%E5%AE%89%E8%A3%85openwrt/>树莓派3b+安装openwrt 18.06.4</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-05-18-git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Git 常用命令</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-05-18-linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Linux 常用命令</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=writinglite.com/categories/algorithm/>algorithm</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/deep-learning/>deep learning</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/machine-learning/>machine learning</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/mathematics/>mathematics</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/nlp/>NLP</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/other/>other</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/reading/>reading</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/tensorflow2.x-keras/>Tensorflow2.x keras</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/acme/ title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/albert/ title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/batch-normalization/ title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/bert/ title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/crf/ title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/git/ title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/guice/ title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/hexo/ title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/kaggle/ title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/keras/ title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/layer-normalization/ title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/linux/ title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/log/ title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/logback/ title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/nginx/ title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/openwrt/ title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/slf4j/ title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/stanford-corenlp/ title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow/ title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow-hub/ title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow2.0/ title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tra/ title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/transformers/ title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/ubuntu/ title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%88%86%E8%AF%8D/ title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%93%88%E5%B8%8C/ title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/ title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%B9%B6%E5%8F%91/ title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%8E%92%E5%BA%8F/ title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%95%A3%E5%88%97%E8%A1%A8/ title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/ title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3/ title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%86%B5/ title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%AE%97%E6%B3%95/ title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/ title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/ title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/writinglite.com/js/menu.js></script><script src=/writinglite.com/js/custom.js></script></body></html>