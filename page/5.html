<!doctype html><html class=no-js lang=en><head><meta name=generator content="Hugo 0.81.0"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Writing Lite"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="/"><meta itemprop=name content="Writing Lite"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Writing Lite"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel=alternate type=application/rss+xml href=/index.xml title="Writing Lite"><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/archives/><span class=menu__text>archives</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/nlp/2017-07-06-%E4%BF%A1%E6%81%AF%E7%9A%84%E5%BA%A6%E9%87%8F%E7%86%B5.html rel=bookmark>信息的度量熵</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-07-06T23:18:00Z>2017-07-06</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/nlp.html rel=category>NLP</a></span></div></div></header><div class="content list__excerpt post__content clearfix">熵 一条信息的信息量与它的不确定性成正比关系。比如说，我们要搞清楚一件非常不确定的事，或是我们一无所知的事情，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，那么不需要太多的信息就能把它搞清楚。所以从这个角度来看，可以认为，信息量就等于不确定性的多少。
它的定义如下： $$ H(X)=-\sum_{x\in X}P(x) \log P(x) $$
条件熵 如果 H(Y|X=x) 为变数 Y 在 X 取特定值 x 条件下的熵，那么 H(Y|X)就是 H(Y|X=x) 在 X 取遍所有可能 x 后平均的结果： $$ \begin{align} H(Y|X) &= \sum_i^n P(x_i)H(Y|X=x_i) \
&= \sum_i^n P(x_i) \sum_j^m P(y_j) \log P(y_j|x_i) \
&= \sum_i^n \sum_j^m P(x_i,y_j) \log P(y_j|x_i) \
&= - \sum_{x\in X,y \in Y}P(x,y) \log P(y|x) \
\end{align} $$ 现在假定我们还知道X的一些情况，包括它和Y一起出现的概率，以及在X取不同值的前提下X的概率分布。定义在X的条件下Y的条件熵为： $$ H(Y|X)=-\sum_{x\in X,y \in Y}P(x,y) \log P(y|x) $$ H(Y)>=H(Y|X)，也就是说Y的不确定性下降了。
互信息 互信息有时也叫“信息增益”。当获取的信息和要研究的事物“有关系”时，这些信息才能帮助我们消除不确定性。当然“有关系”这种说法太模糊，太不科学，最好能度化的度量“相关性”。香农在信息论中提出了一个“互信息”的概念作为对两个随机事件“相关性”的量化度量。假定有两个随机事件X和Y，它们的互信息定义如下： $$ I(X;Y)=\sum_{x \in X,y \in Y}P(x,y) \log \frac{P(x,y)}{P(x)P(y))} $$ 其实这个互信息就是，随机事件X的不确定性或者说熵H(X)，以及在知道随机事件Y条件下的不确定性或者说H(X|Y)之间的差异，即： $$ I(X;Y)=H(X) - H(X|Y) $$ 也就是说，**所谓的两个事件相关性的度量，就是在了解了其中一个Y的前提下，对消除另一个X不确定性所提供的信息量**。当X和Y完全相关是，它的取值是1；当两者完全无关时，它的取值是0。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/machine-learning/2017-07-06-%E4%BC%BC%E7%84%B6%E4%B8%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1.html rel=bookmark>似然与极大似然估计</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-07-06T21:30:00Z>2017-07-06</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/machine-learning.html rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">概要 本文先会介绍似然的概念，似然与概率的区别，然后介绍参数估计的方法——极大似然估计。
似然 在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。
概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性，比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；
而似然刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数），还是抛硬币的例子，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%，这个过程就是我们根据结果来判断这个事情本身的性质（参数），也就是似然。
极大似然估计 似大似然估计解决的问题是，最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。似大似然估计经常在机器学习方法中作为一种学习策略。
似然函数的最大值意味着什么？让我们回到概率和似然的定义，概率描述的是在一定条件下某个事件发生的可能性，概率越大说明这件事情越可能会发生；而似然描述的是结果已知的情况下，该事件在不同条件下发生的可能性，似然函数的值越大说明该事件在对应的条件下发生的可能性越大。
也就是说似然函数取得最大值表示相应的参数能够使得统计模型最为合理。
考虑一个抛硬币的例子。假设这个硬币正面跟反面轻重不同。我们把这个硬币抛80次（即，我们获取一个采样$x_1=H,x_2=T,&mldr;..x_{80}$并把正面的次数记下来，正面记为H，反面记为T）。并把抛出一个正面的概率记为p，抛出一个反面的概率记为1-p（因此，这里的 p即相当于上边的 $\theta$ ）。 假设我们抛出了49个正面，31个反面，即49次H，31次T。假设这个硬币是我们从一个装了三个硬币的盒子里头取出的。这三个硬币抛出正面的概率分别为p=1/3, p=1/2,p=2/3.这些硬币没有标记，所以我们无法知道哪个是哪个。 使用最大似然估计，通过这些试验数据（即采样数据），我们可以计算出哪个硬币的可能性最大。这个似然函数取以下三个值中的一个： $$ P(H=49,T=31 | p=1/3) = (1/3)^{49}(1-1/3) ^{31}= 0.000 \
P(H=49,T=31 | p=1/2) = (1/2)^{49}(1-1/2)^{31} = 0.012 \
P(H=49,T=31 | p=2/3) = (2/3)^{49}(1-2/3)^{31} = 0.054 \
$$ 我们可以看到当p=2/3时，似然函数取得最大值。这就是 p的最大似然估计。
但在机器学习中我们要估计的并不是离散的情况，因此最大似然估计的一般求解过程是：
写出似然函数； 对似然函数取对数，并整理，也就对数似然函数； 求导数，解似然方程，也就是取极值； 参考： https://zhuanlan.zhihu.com/p/22092462 https://zh.wikipedia.org/wiki/似然函数 https://zh.wikipedia.org/wiki/最大似然估计</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/machine-learning/2017-06-04-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA.html rel=bookmark>统计学习方法概论</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-06-04T20:30:00Z>2017-06-04</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/machine-learning.html rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">统计学习的基本概念 学习的定义 如果一个系统能够通过执行某个过程改进它的性能，这就是学习。统计学习的对象是数据；统计学习关于数据的基本假设是同类数据具有一定的统计规律，这是统计学习的前提。统计学习的上的是对未知数据进行预测和分析；对数据的预测可以使计算机更加智能化；对数据的分析可以让人们获取新的知识，给人们带来新的发现。
监督学习的学习方法 从给定的、有限的、用于学习的训练数据集合出发，假设数据是独立同分布产生的；并且假设学习的模型属于某个函数集合，称为假设空间；应用某个评价准则，从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。
实现统计学习方法的步骤 得到一个有限的训练数据集合 确定包含所有可能模型的假设空间，即学习模型的集合 确定模型选择的准则，即学习的策略 实现求解最优模型的算法，取出学习的算法 通过学习方法选择最做强模型 利用学习的最优模型对新数据进行预测和分析 不同的预测任务名称 输入变量与输出变量均为连续变量的预测问题称为回归问题。 输出变量为有限个离散变量的预测问题称为分类问题。 输入变量与输出变量均为变量序列的预测问题称为标注问题。 联合概率分布 监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)，P(X,Y)表示分布函数，或分布密度函数。训练数据与测试数据被看作是依联合概率分布P(X,Y)独立同分布产生的。
判别模型 判别模型对P(y|x)或者说对y建模，直接学习得到y。 在计算学习算法时，一般使用candidation似然 就是直接用的P(y|x) 常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。
生成模型 生成模型对P(x|y)或者说对x建模（从下面的公式可以看到），通过如下计算得到 y。 $$ \begin{aligned} \arg\max\limits_{y}p(y\vert x) &= \arg\max\limits_y\frac{p(x,y)}{p(x)} \
&= \arg\max\limits_y\frac{p(x\vert y)p(y)}{p(x)} \
&= \arg\max\limits_y p(x\vert y)p(y) \end{aligned} $$ 在给定x进行比较时，P(x)为固定值，所以P(x)可省略。在计算学习算法时，一般使用joint似然，就是用的P(x,y)=P(x|y)*P(y)，其实和P(y|x) 一样的。 生成模型表示的是数据生成的方式 ，就是P(x,y) x和y的联合概率。一般来说数据的生成方式是比较复杂的，所以一般都会对数据的生成方式做一定的假设（比如隐马尔科夫模型、朴素贝叶斯模型）。 常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等
统计学习的三要素 统计学习方法包括模型的假设空间、模型选择的准则以及模型的学习算法，称其为统计学习方法的三要素，简称为模型、策略、算法。
模型 在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。 假设空间用F表示，假设空间可以定义为决策函数的集合 $$ F={f|Y=f(x)} $$ 假设空间也可以定义为条件概率的集合 $$ F={P|P(Y|X)} $$
策略 首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。
常用的损失函数 **0-1损失函数 ** $$ L(Y,f(X))= \begin{cases} 1,Y \neq f(X)\</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/nlp/2017-05-21-stanford-corenlp-%E4%BD%BF%E7%94%A8.html rel=bookmark>Stanford CoreNLP 使用</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-05-21T13:18:00Z>2017-05-21</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/nlp.html rel=category>NLP</a></span></div></div></header><div class="content list__excerpt post__content clearfix">介绍 斯坦福CoreNLP提供了一套自然语言分析工具。斯坦福CoreNLP集成了许多斯坦福的NLP工具，包括词性（POS），命名实体识别（NER）， 语法解析，指代消解，情感分析，自举模式学习和开放信息提取工具。
如果有如下需求，要以选择Stanford CoreNLP：
具有良好语法分析工具的集成工具包 快速，可靠地分析任意文本 整体最高品质的文字分析 支持一些主要（人类）语言 可用于大多数主要现代编程语言的接口 能够作为简单的Web服务运行 Github地址：Stanford CoreNLP GitHub site.
使用前准备 Stanford CoreNLP是基于JAVA的，最新版本需要JDK1.8。JAR包可以通用官网，或者MAVEN下载到。如果使用MAVEN的话，可以使用如下配置。
&lt;dependencies> &lt;dependency> &lt;groupId>edu.stanford.nlp&lt;/groupId> &lt;artifactId>stanford-corenlp&lt;/artifactId> &lt;version>3.7.0&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>edu.stanford.nlp&lt;/groupId> &lt;artifactId>stanford-corenlp&lt;/artifactId> &lt;version>3.7.0&lt;/version> &lt;classifier>models&lt;/classifier> &lt;/dependency> &lt;/dependencies> 如果你想从阿拉伯语，中文，德语或西班牙语中获得Maven的语言模型，请将其添加到您的pom.xml：
&lt;dependency> &lt;groupId>edu.stanford.nlp&lt;/groupId> &lt;artifactId>stanford-corenlp&lt;/artifactId> &lt;version>3.7.0&lt;/version> &lt;classifier>models-chinese&lt;/classifier> &lt;/dependency> 将“models-chinese”替换为“models-english”，“models-chinese-kbp”，“models-arabic”，“models-french”，“models-german”或“models-spanish”为其他语言！
由于Stanford CoreNLP的模型文件太大（中文的模型就几百M），建议到官网下载。如果使用MAVEN的话，最好使用阿里的MAVEN仓库，这样速度更快些。
通过命令行使用 Quick Start java -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
-cp "*"是加载当前路径下的所有文件（主要是JAR包）
该-props参数是可选的。默认情况下，Stanford CoreNLP将在您的类路径中搜索StanfordCoreNLP.properties，并使用分发中包含的默认值。
该-annotators参数实际上是可选的。如果你离开它，代码使用一个内建的属性文件，它可以使用以下注释器：标记化和句子分割，POS标记，缩小，NER，解析和关联解析（也就是我们在这些例子中使用的）。
如果要使用其它语言的话，
java -mx3g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -file chinese.txt -outputFormat text
参考： https://stanfordnlp.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/algorithm/2017-05-16-%E5%85%A8%E5%9F%9F%E5%93%88%E5%B8%8C.html rel=bookmark>全域哈希</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-05-16T20:18:00Z>2017-05-16</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/algorithm.html rel=category>algorithm</a></span></div></div></header><div class="content list__excerpt post__content clearfix">全域哈希的讲解有些不同，它是先给出一个奇怪的定义，然后推论出一些较好的性质。最后介绍构造方法，并证明符合定义。
定义 设H为一组有限散列函数，它将给定的关键字全域 U映射到{1,2,&mldr;m-1}中。这样的一组函数称为全域的，如果对每对不同的关键字$x,y \in U$，满足h(x)=h(y)的散列函数的个数至多为|H|/m。
性质 推论1 当关键字x!=y时，两者发生冲突的概率小于等于1/m。
推论2 随机选择一个函数，将n个关键字放进T表的m个槽中，对于给定的关键字x，它发生冲突的期望次数小于n/m（也就是装载因子）。 $$ E[#与x碰撞]&lt;n/m $$ 证明： 设$C_{x}$表示哈希表T里与x发生碰撞的次数 $$ C_{xy}= \begin{cases} 1 \ if \ h(x) = h(y)\
0 \ otherwise\
\end{cases} $$ 根据全域函数的定义可知 $$ E[C_{xy}]=1/m $$ 因此 $$ C_x = \sum_{y\in T-{x}}C_{xy} $$ $$ \begin{align} E[C_x] &= E[\sum_{y\in T-{x}}C_{xy}] \
&=\sum_{y\in T-{x}}E\left[C_{xy} \right] \
&=\sum_{y\in T-{x}} 1/m \
&= \frac{n-1}{m} \end{align} $$
全域哈希构造方法 假设m是一个质数，将关键字k转换成r+1位的m进制数，$k=&lt;k_0,k_1,\dots,k_r>,0&lt;=k_r&lt;=m-1$ 设随机数$a=&lt;a_0,a_1,\dots,a_r>$同样为成r+1位的m进制数，defin 哈希函数为 $$ h_a(x)=(\sum_{i=0}^ra_ik_i) \mod m $$ 哈希函数集H的大小为$m^{r+1}$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/algorithm/2017-05-14-%E6%95%A3%E5%88%97%E8%A1%A8.html rel=bookmark>散列表</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-05-14T11:17:00Z>2017-05-14</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/algorithm.html rel=category>algorithm</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背影 散列表是普通数组概念的推广。由于对普通数组可以直接寻址，使得能在O(1)时间内访问数组中的任意位置。
如果存储空间允许，我们可以提供一个数组，为每个关键字保留一个位置，以利用直接寻址的技术。
当实际关键字数目比全部的可能关键字总数要小时，采用散列表就成为直接数组寻址的一种有效替代，因为散列表使用一个长度与实际存储的关键字数目成比例的数组来存储在散列表中，不是直接把关键字作为数组下标，而是根据关键字计算出相应的下标。
当实际关键字数目比全部的可能关键字总数要小时，可能会导致多个关键字映射到同一个下标。解决这种冲突有以下三种方式：
链接方法 利用散列函数 开放寻址法 当关键字集合是静态存储时（即关键字一但存入后不再改变）时，通过“完全散列”在最坏时间为O(1)的情况下完成关键字查找。
直接寻址法 链接法 开放寻址法 完全散列 参考 《算法导论》</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/algorithm/2016-11-25-pagerank%E7%AE%97%E6%B3%95.html rel=bookmark>PageRank算法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-05-13T15:53:00Z>2017-05-13</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/algorithm.html rel=category>algorithm</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 总的来讲，对于一个特定的查询，搜索结果的排名取决于两组信息，关于网页的质量信息，和这个查询与每个网页的相关信息。PageRank算法就是一种衡量网页质量的方法。
核心思想（原理） 在互联网上，如果一个网页被很多其它网页所连接，说明它受到普遍的承认和信赖，那么它的排名就高。
算法细节 算法的思想如上文所诉，但实际上要复杂得多。比如说，对于来自不同网页的链接区别对待，因此网页排名高的那些网页的链接更可靠，于是给这些链接以较大的权重。
可以想像，一个新开的质量差的网站与另一个质量好的老网站它们的权重显然是不同的，否则就可以轻易作弊。
那接下来的问题就是这些网站的权重要如何度量呢，可以想到应该是网页本身的网页排名。现在麻烦来了，计算搜索结果的网页排名的过程中需要用到网页本身的排名。
解决方法是，把这个问题变成一个二维矩阵相乘的问题，并且用迭代的方法解决。先假定所有网页的排名是相同的，并且根据这个初始值，算出各个网页的第一次迭代排名，然后再根据第一次迭代排名算出第二次的排名 。
PageRank计算方法 假定如下向量，为第一、第二、&mldr;第N个网页的网页排名。 $$ B=(b_1,b_2,&mldr;b_N)^T $$ 如下矩阵为网页之间的链接数目，其中$a_{mn}$代表第m个网页指向第n个网页的链接数。 $$ A= \begin{bmatrix} a_{11} & \cdots & a_{1n} & \cdots & a_{1M} \
\cdots \
a_{m1} & \cdots & a_{mn} & \cdots & a_{mM} \
\cdots \
a_{M1} & \cdots & a_{Mn} & \cdots & a_{MM} \
\end{bmatrix} $$ A是已知的，B是未知的，是我们需要计算的。 假定$B_i$是第i次的迭代的结果，那么 $$ B_i=A \cdot B_{i-1} $$ 初始假设：所有网页的排名都是1/N，即 $$ B_0=\left( \frac{1}{N},\frac{1}{N},\cdots,\frac{1}{N} \right) $$
**注：**关于矩阵计算的优化与技巧和链接数量的平滑，这里并没有给出，可以查阅相关资源。
参考 《数学之美》</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/other/2017-05-09-%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA.html rel=bookmark>静态博客搭建</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-05-09T20:07:00Z>2017-05-09</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/other.html rel=category>other</a></span></div></div></header><div class="content list__excerpt post__content clearfix">为什么要写博客 能够获得反馈，然后你可以继续做出更复杂、更优秀的产品。
如何写博客 下文会先介绍较流行的几种书写博客的方式，然后重点介绍通过静态网页生成器的方式如何来搭建博客。首先通过静态页面托管平台来简易搭建博客，并推荐一些实用的博客组件，最后简单介绍将博客搭建在服务器上的思路。
书写博客的几种方式 目前比较地的方式主要有两种：
在线博客平台 例如：CSDN 博客中国 简书
WordPress wordpress样例
WordPress是个人博客系统，并逐步演化成一款内容管理系统软件，它是使用PHP语言和MySQL数据库开发的。用户可以在支持 PHP 和 MySQL数据库的服务器上使用自己的博客。
在线博客平台的优点的简单，除了书写博客内容，不需要关心与网站有关的任何事情。缺点是有广告，死板，不geek。WordPress，则正好相反，操作麻烦，学习成本较高，对硬件资源有一定要求。优点是高度自由化。
静态博客生成器介绍 静态博客生成器使用Markdown（或其他渲染引擎）解析文章，可利用靓丽的主题生成静态网页。
Jekyll hexo
hexo 样例 Jekyll 样例
通过静态页面托管平台（github pages）极速搭建 github pages介绍
Github Pages 是面向用户、组织和项目开放的公共静态页面搭建托管服务，站点可以被免费托管在Github 上，你可以选择使用Github Pages 默认提供的域名github.io 或者自定义域名来发布站点。
搭建的过程分如下几个步骤：
搜索theme 搜索
fork
设置github pages Your site is published at https://hwyoung.github.io/minimal-mistakes/
预览 https://hwyoung.github.io/minimal-mistakes/
写博客 minimal-mistakes/_posts/2016-07-20-helloworld.md hello world!!!</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/algorithm/2017-04-16-%E5%A4%9A%E6%A8%A1%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95-ahocorasick.html rel=bookmark>多模字符串匹配算法-Aho–Corasick</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-04-16T10:59:00Z>2017-04-16</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/algorithm.html rel=category>algorithm</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 在做实际工作中，最简单也最常用的一种自然语言处理方法就是关键词匹配，例如我们要对n条文本进行过滤，那本身是一个过滤词表的，通常进行过滤的代码如下
for (String document : documents) { for (String filterWord : filterWords) { if (document.contains(filterWord)) { //process ... } } } 如果文本的数量是n，过滤词的数量是k，那么复杂度为O(nk)；如果关键词的数量较多，那么支行效率是非常低的。
计算机科学中，Aho–Corasick算法是由Alfred V. Aho和Margaret J.Corasick 发明的字符串搜索算法，用于在输入的一串字符串中匹配有限组“字典”中的子串。它与普通字符串匹配的不同点在于同时与所有字典串进行匹配。算法均摊情况下具有近似于线性的时间复杂度，约为字符串的长度加所有匹配的数量。然而由于需要找到所有匹配数，如果每个子串互相匹配（如字典为a，aa，aaa，aaaa，输入的字符串为aaaa），算法的时间复杂度会近似于匹配的二次函数。
原理 在一般的情况下，针对一个文本进行关键词匹配，在匹配的过程中要与每个关键词一一进行计算。也就是说，每与一个关键词进行匹配，都要重新从文档的开始到结束进行扫描。AC自动机的思想是，在开始时先通过词表，对以下三种情况进行缓存：
按照字符转移成功进行跳转（success表） 按照字符转移失败进行跳转（fail表） 匹配成功输出表（output表） 因此在匹配的过程中，无需从新从文档的开始进行匹配，而是通过缓存直接进行跳转，从而实现近似于线性的时间复杂度。
构建 构建的过程分三个步骤，分别对success表，fail表，output表进行构建。其中output表在构建sucess和fail表进行都进行了补充。fail表是一对一的，output表是一对多的。
按照字符转移成功进行跳转（success表） sucess表实际就是一棵trie树，构建的方式和trie树是一样的，这里就不赘述。
按照字符转移失败进行跳转（fail表） 设这个节点上的字母为C，沿着他父亲的失败指针走，直到走到一个节点，他的儿子中也有字母为C的节点。然后把当前节点的失败指针指向那个字母也为C的儿子。如果一直走到了root都没找到，那就把失败指针指向root。 使用广度优先搜索BFS，层次遍历节点来处理，每一个节点的失败路径。
匹配成功输出表（output表） 匹配 举例说明，按顺序先后添加关键词he，she，,his，hers。在匹配ushers过程中。先构建三个表，如下图，实线是sucess表，虚线是fail表，结点后的单词是ourput表。
代码 gist 2fed6f4569d4da8029e7ef08458cad6b</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/java/2017-04-06-guice%E6%B3%A8%E5%85%A5slf4j.html rel=bookmark>Guice注入SLF4J</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-04-06T21:57:00Z>2017-04-06</time></div></div></header><div class="content list__excerpt post__content clearfix">创建注解 import javax.inject.Scope; import java.lang.annotation.Documented; import java.lang.annotation.Retention; import java.lang.annotation.Target; import static java.lang.annotation.ElementType.FIELD; import static java.lang.annotation.RetentionPolicy.RUNTIME; @Scope @Documented @Retention(RUNTIME) @Target(FIELD) public @interface Log { } 自定义MembersInjector import com.google.inject.MembersInjector; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.lang.reflect.Field; public class SLF4JMembersInjector&lt;T> implements MembersInjector&lt;T> { private final Field field; private final Logger logger; public SLF4JMembersInjector(Field field) { this.field = field; this.logger = LoggerFactory.getLogger(field.getDeclaringClass()); field.setAccessible(true); } public void injectMembers(T t) { try { field.set(t, logger); } catch (IllegalAccessException e) { throw new RuntimeException(e); } } } 创建TypeListener import project.</div></article></main><div class=pagination><a class="pagination__item pagination__item--prev btn" href=/page/4.html>«</a>
<span class="pagination__item pagination__item--current">5/8</span>
<a class="pagination__item pagination__item--next btn" href=/page/6.html>»</a></div></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/post/nlp/2021-05-13-han-for-document-classification.html>HAN for Document Classification</a></li><li class=widget__item><a class=widget__link href=/post/other/2021-05-06-%E4%BD%BF%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%88%B0github-pages.html>使用GitHub Actions自动部署hugo到GitHub Pages</a></li><li class=widget__item><a class=widget__link href=/post/deep-learning/2020-10-16-deeplearning%E4%B8%ADcrf%E7%9A%84tensorflow%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html>DeepLearning中CRF的Tensorflow代码实现</a></li><li class=widget__item><a class=widget__link href=/post/deep-learning/2020-10-16-deeplearning%E4%B8%ADcrf%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86.html>DeepLearning中CRF计算原理</a></li><li class=widget__item><a class=widget__link href=/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%984-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB.html>卷积神经网络实战_图片分类</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/categories/algorithm.html>algorithm</a></li><li class=widget__item><a class=widget__link href=/categories/deep-learning.html>deep learning</a></li><li class=widget__item><a class=widget__link href=/categories/machine-learning.html>machine learning</a></li><li class=widget__item><a class=widget__link href=/categories/mathematics.html>mathematics</a></li><li class=widget__item><a class=widget__link href=/categories/nlp.html>NLP</a></li><li class=widget__item><a class=widget__link href=/categories/other.html>other</a></li><li class=widget__item><a class=widget__link href=/categories/reading.html>reading</a></li><li class=widget__item><a class=widget__link href=/categories/tensorflow2.x-keras.html>Tensorflow2.x keras</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=/tags/acme.html title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=/tags/albert.html title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/batch-normalization.html title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/bert.html title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/blog.html title=blog>blog</a>
<a class="widget-taglist__link widget__link btn" href=/tags/crf.html title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=/tags/deep-learning.html title="Deep Learning">Deep Learning</a>
<a class="widget-taglist__link widget__link btn" href=/tags/git.html title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=/tags/github-actions.html title="github actions">github actions</a>
<a class="widget-taglist__link widget__link btn" href=/tags/github-pages.html title="github pages">github pages</a>
<a class="widget-taglist__link widget__link btn" href=/tags/guice.html title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hexo.html title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hugo.html title=hugo>hugo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/kaggle.html title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=/tags/keras.html title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=/tags/layer-normalization.html title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/linux.html title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=/tags/log.html title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=/tags/logback.html title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=/tags/nginx.html title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=/tags/openwrt.html title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=/tags/slf4j.html title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=/tags/stanford-corenlp.html title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow.html title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow-hub.html title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow2.0.html title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tra.html title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=/tags/transformers.html title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=/tags/ubuntu.html title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%88%86%E8%AF%8D.html title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%93%88%E5%B8%8C.html title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D.html title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%B9%B6%E5%8F%91.html title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%8E%92%E5%BA%8F.html title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%A3%E5%88%97%E8%A1%A8.html title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E.html title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%97%A0%E7%9B%91%E7%9D%A3.html title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%86%B5.html title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%AE%97%E6%B3%95.html title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0.html title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.html title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script></body></html>