<!doctype html><html class=no-js lang=en><head><meta name=generator content="Hugo 0.81.0"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Writing Lite"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="/"><meta itemprop=name content="Writing Lite"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Writing Lite"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel=alternate type=application/rss+xml href=/index.xml title="Writing Lite"><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/archives/><span class=menu__text>archives</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/other/2018-8-11-shadowsocks%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%9C%8D%E5%8A%A1%E5%AE%89%E8%A3%85.html rel=bookmark>shadowsocks服务端服务安装</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2018-08-11T14:27:00Z>2018-08-11</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/other.html rel=category>other</a></span></div></div></header><div class="content list__excerpt post__content clearfix">Shadowsocks服务端服务安装 环境：centos
安装 安装python pip环境
sudo yum install python-setuptools && easy_install pip 通过pip安装shadowsocks
sudo pip install shadowsocks 配置 sudo vi /etc/shadowsocks.json { "server":"0.0.0.0", "local_address": "127.0.0.1", "local_port":1080, "server_port":my_server_port, "password":"my_password", "timeout":300, "method":"aes-256-cfb" } 防火墙设置 firewall-cmd --permanent --add-port=my_server_port/tcp firewall-cmd --permanent --add-port=my_server_port/udp firewall-cmd --reload 服务启动 ssserver -c /etc/shadowsocks.json -d start</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E9%98%85%E8%AF%BB/2018-06-19-%E6%96%9C%E6%9D%A0%E9%9D%92%E5%B9%B4.html rel=bookmark>斜杠青年</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2018-06-19T16:21:00Z>2018-06-19</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/reading.html rel=category>reading</a></span></div></div></header><div class="content list__excerpt post__content clearfix">“斜杠青年”代表的是一种全新的人生价值观，它的核心不在于多重收，也不在于多重身份，而在于多元化的人生。
我绝对没有想要否定上班这种生活方式，毕竟朝九晚五的上班生活是现在的主流生活方式，但我们需要意识到，它既不是赚钱的唯一方式，也不是实现自我价值的唯一方式。
我们应该允许不一样的生活方式的存在，不去过分评判他人的人生选择，因为当我们把自己的价值观强加于他人时，我们也在限制自己，同时推动了本可以打拥有更多生活的可能性。
当你的才华还撑不起你的野心的时候，你就应该静下心来学习；当你的能力还驾驭不了你的目标时，就应该沉下心来历练。梦想，不是浮躁，而是沉淀和积累。
我们在工作中无法获得快乐最核心的原因之一，就在于我们被剥夺了独立自主的权利，因为根据美国心理学家德西和瑞安提出的自我决定理论，人类有自主、独立、寻求归属感的内在动机。
价格实际上是实现将源高配的一种重要手段，价格反应了商品供求关系的变化。
按照现代经济学理论，企业本质上只不过是一种将源配置机制，它能够按照一定的组织和管理方式实现整个社会经济资源的优化配置，降低整个社会的“交易成本”。尽管企业本身存在着管理成本，但只要管理成本低于交易成本，企业就有存在的价值。
一个人的收入不是和他的劳动时间成正比，而是和他的劳动的不可替代性成正比。
真正的自由不是“拥有”的自由而是“拒绝”的自由，当我们不再需要为了钱而去做自己不喜欢或者不愿意做的事情的时候，我们才获得了真正意义上的财务自由。
在以后的日子里，当我再次遇到恶意的贬低和攻击时，我也不再以愤怒、反驳或者对抗来回应，而是大大方方承认：我的确不够好，但我在进步。
内心和头脑的丰富让我越来越淡定和从容；自我认可让我不再在意别人对我的看法；而对自身价值和能力的肯定给了我足够的安全感。我不再害怕失去，因为我知道自己内在所拥有的是别人夺不走的，也不会因为外在的变化而有所增减。
所有能够快速获得或者能够用金钱换来的都无法成为核心竞争力。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/other/2017-11-18-git%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6.html rel=bookmark>Git版本控制</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-11-18T10:11:00Z>2017-11-18</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/other.html rel=category>other</a></span></div></div></header><div class="content list__excerpt post__content clearfix">git是日常工作常用的开发工具，本篇文章将粗略讲解关于git的两部分内容，一是git中的基本概念这将有助于了解git命令的具体含义，二是日常工作中常用到的一些git命令和git的开发流程。
Git中的基础概念 git中比较重要的两个概念是空间和版本（commit），git中的命令都是对这两个概念的操作。了解了这两个概念，就可以感知到git中可以支持哪些操作，即使有些命令我们没有学过，也可以通过到搜索了解到。
第一个概念：空间 git中有三种空间
工作区（Word Space） 储存库（Repo） 暂存区（Staged Area，Index） 工作区就是git所在目录，就是文件系统中的一个普通目录。
暂存区指.git目录下的index文件，通过git add命令可以将工作区目录下的文件添加到暂存区中。同时它也有索引作用，它对应到储存库中的文件（索引理解的还不是很透彻可能有错误）。
储存库指工作区下的.git目录（应该是里面的某个目录），里面记录了各个版本的所有内容，只有commit的内容才是真正会保存到储存库中。
第二个概念：版本（commit） 我理解的版本就是每一次commit的内容。各个版本之间是一种增量并且依赖的关系，新的版本会依赖旧的版本。每一个版本都有一个key来唯一标识这个commit。
通过commit可以引出分支的概念，每个分支其实是其实就是指向了一个commit，HEAD指向的就是当前的commit。
Git日常开发 git命令比较多，但日常开发中实际用到的比较小，下面会通过一些实际场景介绍一些常用的git命令。
git配置 $ git config --global user.name "John Doe"
$ git config --global user.email johndoe@example.com
git提交时会注明提交人的姓名和邮箱，当多个协同工作时以标名操作人。
git初始化 git的初始化分为两种场景，一是git库已经有了，要在这个git库基本上进行开发；另外一种是，我们先在本地进行的开发，然后想上传到服务器上。
第一种的话可以使用git clone命令：
$ git clone git://github.com/schacon/grit.git
第二种话，要先添加远程git然后将自己的代码提交上去。
$ git remote add origin git@github.xxx.cn:xxx/xxx.git
$ git push -u origin master
程序开发 git有不同的工作的流（gitflow），但共同的特点是，对于先功能的开发，要先新建分支开发完成后再merge回master分支。
$ git branch &lt;branch name>
$ git checkout &lt;branch name>
...开发
$ git checkout master
$ git merge &lt;branch name>
其它 git的分支是一种指标，指向commit</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017-09-09-%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E8%AF%8D%E5%88%87%E5%88%86%E5%92%8C%E6%A0%87%E6%B3%A8%E4%B8%80%E4%BD%93%E5%8C%96%E6%A8%A1%E5%9E%8B.html rel=bookmark>基于统计的词切分和标注一体化模型</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-09-09T16:18:00Z>2017-09-09</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/nlp.html rel=category>NLP</a></span></div></div></header><div class="content list__excerpt post__content clearfix">基于统计的切记和标注一体化模型 假设$C=c_1,c_2 \dots c_m$为m个的字符串，$W=w_1,_2 \dots w_n$是把C切分后得到的由n个词组成的词序列，$T=t_1,t_2 \dots t_n$是对W进行标注后的标记序列。
$$ P(W|C) = \frac{P(C|W)P(W)}{P(C)} $$ 可知P(C)是一个确定的值，P(C|W)是在给定词序列的情况下字符串的概率，可以认为是1。因此可得： $$ \max P(W|C) = \max P(W) $$ 即是说，分词的过程即是寻找概率最大词序列的过程。 我们再来考虑词序列与词序列与词性标注序列的关系。 $$ P(T|W) = \frac{P(W|T)P(T)}{P(W)} $$ 可推导出： $$ P(W) = \frac{P(W|T)P(T)}{P(T|W)} $$ 如果隐马尔可夫假设和独立输出假设 即： $$ P(T) = \prod_i^n P(t_i)P(t_{i-1}) \
P(W|T) = \prod_i^n P(|w_i,t_i) \
P(T|W) = \prod_i^n P(|t_i,w_i) $$
计算方法：
找到一条分词路径W 用记性标注的模型计算具有最大概率的记性标注序列，得到对应的标注路径T。 利用公式可得到W的概率 若干个健忘路径中，概率最大的W即为分词结果 引用： 白栓虎 - 基于统计的切记和标注一体化模型</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2017-09-08-mmseg%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95.html rel=bookmark>MMSeg分词方法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-09-08T20:18:00Z>2017-09-08</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/nlp.html rel=category>NLP</a></span></div></div></header><div class="content list__excerpt post__content clearfix">概述 陈和刘（1992）完成的最大匹配的另一个变体比基本形式更复杂。 这种演算法指出，最可能的分词方式是三个单词，方式为从一个字串的第一个字开始，寻找分词的方式，只要存在有不同意义的词。
MMSeg的四个规则 规则 1：最大匹配(Maximum matching) Simple方法：取最大长度的单词。 Complex方法：匹配出所有的“三个词的词组”（原文中使用了chunk，这里感觉用“词组”比较合适），即从某一既定的字为起始位置，得到所有可能的“以三个词为一组”的所有组合。比如“研究生命起源”，可以得到
研_究_生 研_究_生命 研究生_命_起源 研究_生命_起源 规则 2：最大平均单词长度(Largest average word length) 经过规则1过滤后，如果剩余的词组超过1个，那就选择平均词语长度最大的那个（平均词长＝词组总字数／词语数量）。比如“生活水平”，可能得到如下词组：
生_活水_平 (4/3=1.33) 生活_水_平 (4/3=1.33) 生活_水平 (4/2=2) 根据此规则，就可以确定选择“生活_水平”这个词组
规则 3：单词长度的最小方差(Smallest variance of word lengths) 由于词语长度的变化率可以由标准差反映，所以此处直接套用标准差公式即可。比如
研究_生命_起源 （标准差=sqrt(((2-2)^2+(2-2)^2+(2-2^2))/3)=0） 研究生_命_起源 （标准差=sqrt(((2-3)^2+(2-1)^2+(2-2)^2)/3)=0.8165） 于是选择“研究_生命_起源”这个词组。
规则 4：单字单词的语素自由度的最大和(Largest sum of degree of morphemic freedom of one-character words) 其中degree of morphemic freedom可以用一个数学公式表达：log(frequency)，即词频的自然对数（这里log表示数学中的ln）。这个规则的意思是“计算词组中的所有单字词词频的自然对数，然后将得到的值相加，取总和最大的词组”。比如：
设施_和服_务 设施_和_服务 这两个词组中分别有“务”和“和”这两个单字词，假设“务”作为单字词时候的频率是5，“和”作为单字词时候的频率是10，对5和10取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施_和_服务”。 也许会问为什么要对“词频”取自然对数呢？可以这样理解，词组中单字词词频总和可能一样，但是实际的效果并不同，比如
A_BBB_C （单字词词频，A:3， C:7） DD_E_F （单字词词频，E:5，F:5） 表示两个词组，A、C、E、F表示不同的单字词，如果不取自然对数，单纯就词频来计算，那么这两个词组是一样的（3+7=5+5），但实际上不同的词频范围所表示的效果也不同，所以这里取自然对数，以表区分（ln(3)+ln(7) &lt; ln(5)+ln(5)， 3.0445&lt;3.2189）。
总结 这个四个过滤规则中，如果使用simple的匹配方法，只能使用第一个规则过滤，如果使用complex的匹配方法，则四个规则都可以使用。实际使用中，一般都是使用complex的匹配方法＋四个规则过滤。 看到这里也许对MMSEG的分词方法有了一个大致的了解，它是一个“直观”的分词方法。它把一个句子“尽可能长（这里的长，是指所切分的词尽可能的长）”“尽可能均匀”的去切分，与中文的语法习惯比较相符。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017-09-02-logistic%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%8E%A8%E5%AF%BC.html rel=bookmark>Logistic回归梯度下降推导</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-09-02T20:34:00Z>2017-09-02</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/machine-learning.html rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">Logistic回归梯度下降推导 \[ P(p=1|x;\theta) = h_\theta(x) = \frac{exp(\theta^Tx)}{1+exp(\theta^Tx)} \\ P(p=0|x;\theta) = 1 - h_\theta(x) = \frac{1}{1+exp(\theta^Tx)} \]
\[ \begin{align} L(\theta) & = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) \\ & =\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\ & =\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \end{align} \]
\[ \begin{align} l(\theta) & = \log L(\theta) \\ & = \sum_{i=1}^m \left[ y^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)})) \right] \\ & = \sum_{i=1}^m \left[ y^{(i)}\log \frac{h(x^{(i)})}{1-h(x^{(i)})} + \log (1-h(x^{(i)})) \right] \\ & = \sum_{i=1}^m \left[ y^{(i)}\theta^Tx + \log (1-h(x^{(i)})) \right] \end{align} \]</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017-07-19-%E5%86%B3%E7%AD%96%E6%A0%91.html rel=bookmark>决策树</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-07-20T21:00:00Z>2017-07-20</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/machine-learning.html rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 决策树的优点是计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点是可能会产生过度匹配问题。适用于连续值和离散值数据。
决策树生成 训练集$D={(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$ 属性集$A={a_1,a_2,\dots,a_d}$
def function treeGenerate(D,A): 生成结node; if Dv为空: 将该分支结点标记为叶结点，其类别标记为D中样本最多的类;return; if D中所有实例属于同一类: 将该分支结点标记为叶结点，将该类作为其类别标记;return; a* = 从A中选择最优划分属性; for v in a*: 为node生成一个分支； 令Dv表示D中在a*上取值为v的样本子集； 以treeGenerate(Dv,A-a*)为分支结点 特征选择 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有特征是没有分类能力的。
信息增益 信息增益也叫互信息。当获取的信息和要研究的事物“有关系”时，这些信息才能帮助我们消除不确定性。当然“有关系”这种说法太模糊，太不科学，最好能度化的度量“相关性”。 熵： $H(X) = -\sum_i^nP(x_i)\log(P(x_i)$ 条件熵： $H(Y|X) = \sum_i^nP(x_i)H(Y|X=x_i)$ 信息增益就是熵与条件熵的差值： $$ Gain(D,a) = H(D) - H(D|a) = -\sum_i^{|Y|} P(Y_i) \log P(Y_i) - \sum_j^{|a|}P(a_j)H(D|a=a_j) \
= H(D) - \sum_{v=1}^V \frac{|D^v|}{|D|}H(D^v) $$
信息增益比 信息增益作为划分训练数据的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。 信息增益比： $$ Gain_ratio(D,a) = \frac{Gain(D,a)}{H(a)} \
= \frac{H(D) - H(D|a)}{H(a)} \</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017-02-12-logistic%E5%9B%9E%E5%BD%92.html rel=bookmark>Logistic回归</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-07-19T21:41:00Z>2017-07-19</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/machine-learning.html rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 logistic回归是统计学习中经典的分类方法，在深度学习也有很多应用。本文主要介绍logistic回归，然后将其推广到多分类问题-softmax回归。
什么是logistic 虽然名称中有回归，其实logistic回归模型是一个经典二分类模型。logistic回归在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数。
logstic回归模型的特点，一个事件的几率是指该事件发生的概率与不发生的概率的比值，如果事件发生的概率是p，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率为 $$ logit(p) = \log \frac{p}{1-p} $$ 对于logistic回归而言这个对数几率就是wx，也就是说线性函数的值越接近正无穷，概率值越接近1；线性函数的值越接近负无穷，概率值越接近0，这样的模型就是logistic回归模型。一句话概括的话，logistic回归模型实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率。
$$ h(\theta) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}} $$ where $$ g(z) = \frac{1}{1+e^{-z}} $$
$$ P(p=1|x;\theta) = h_\theta(x) \
P(p=0|x;\theta) = 1 - h_\theta(x) $$
因此 $$ P(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y} $$
所以似然函数为 $$ \begin{align} L(\theta) & = L(\theta;X,\vec{y}) = p(\vec{y}|X;θ) \\
& =\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\
& =\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \end{align} $$
对数似然函数 $$ \begin{align} l(\theta) & = \log L(\theta) \\
& = \sum_{i=1}^my^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)})) \end{align} $$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017-07-15-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D.html rel=bookmark>集成学习简单介绍</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-07-15T14:32:00Z>2017-07-15</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/machine-learning.html rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 目前在kaggle中，GBDT和RF都是非常流行的算法，在一些比赛中，排名先前的算法都有这两种算法的身影。这两种算法都属于Ensemble算法，因此，本文将介绍介绍下Ensemble的思想，以及实现Ensemble的两种方法。
什么是集成学习 Ensemble通过构建多个学习器来完成学习任务，直觉上来说，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但并不是说，随机进行组合就能得到好的效果，各个子模型应该“好而不同”，即个体学习器要有一定的“准确性”又要有“互补性”。
根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即
个体学习器间存在强依赖关系、必须串行生成的序列化算法。 个体学习器间不存在依赖关系、可同时生成的并行化算法。 前者的代表是Bossting，后者的代表是Bagging。从偏差-方差的角度看Bossting主要关注降低偏差，因此Bossting能基于泛化性能相当弱的学习器构建出很强的学习器，比如一棵只有5层的决策树。而Bagging主要关注降低方差，因此它在不剪枝层次较深的决策树、神经网络等易样本干扰的学习器上效用更加明显。 要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”，即学习器间具有差异。
Bostting Bostting是一族可将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这个T个基学习器进行加权结合。
Bagging 如上所述，一个可行的集成学习算法，要满足两个要点，一是“多样性”二是“准确性”。一个可行的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据集中训练出一个学习器，这样就满足了“多样性”。然而，采样的每个子集都完全不同，则每个学习器只用到了一个小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器，为了解决这个问题可以采用自助采样法。
《机器学习》 周志华 https://www.zhihu.com/question/29036379</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2017-07-11-%E5%9C%A8kaggle%E4%B8%AD%E4%BD%BF%E7%94%A8keras%E5%81%9A%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB.html rel=bookmark>在kaggle中使用keras做数字识别</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-07-11T20:43:00Z>2017-07-11</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 本文将讲述通过keras写一个深度学习模型，来完成 kaggle 的 Digit Recognizer 准确率可达到98%。
下载数据 首先登入kaggle网站，点击competitions，在类型筛选中选择 Getting Started。然后点击Digit Recognizer。
下载Data中的train.csv（模型的训练数据）和test.csv（测试数据）。 使用keras训练卷积神经网络模型 下面的代码分为三部分：
读取数据 训练模型 预测测试数据 from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D from keras import backend as K import pandas as pd from keras.utils import np_utils from keras.layers import Dense, Dropout, Activation, Flatten batch_size = 128 num_classes = 10 epochs = 12 # Read data train = pd.</div></article></main><div class=pagination><a class="pagination__item pagination__item--prev btn" href=/page/3.html>«</a>
<span class="pagination__item pagination__item--current">4/8</span>
<a class="pagination__item pagination__item--next btn" href=/page/5.html>»</a></div></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-13-han-for-document-classification.html>HAN for Document Classification</a></li><li class=widget__item><a class=widget__link href=/post/other/2021-05-06-%E4%BD%BF%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%88%B0github-pages.html>使用GitHub Actions自动部署hugo到GitHub Pages</a></li><li class=widget__item><a class=widget__link href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-10-16-deeplearning%E4%B8%ADcrf%E7%9A%84tensorflow%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html>DeepLearning中CRF的Tensorflow代码实现</a></li><li class=widget__item><a class=widget__link href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-10-16-deeplearning%E4%B8%ADcrf%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86.html>DeepLearning中CRF计算原理</a></li><li class=widget__item><a class=widget__link href=/post/tensorflow2.x_keras/tensorflow%E5%AE%9E%E6%88%984-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB.html>卷积神经网络实战_图片分类</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/categories/algorithm.html>algorithm</a></li><li class=widget__item><a class=widget__link href=/categories/deep-learning.html>deep learning</a></li><li class=widget__item><a class=widget__link href=/categories/machine-learning.html>machine learning</a></li><li class=widget__item><a class=widget__link href=/categories/mathematics.html>mathematics</a></li><li class=widget__item><a class=widget__link href=/categories/nlp.html>NLP</a></li><li class=widget__item><a class=widget__link href=/categories/other.html>other</a></li><li class=widget__item><a class=widget__link href=/categories/reading.html>reading</a></li><li class=widget__item><a class=widget__link href=/categories/tensorflow2.x-keras.html>Tensorflow2.x keras</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=/tags/acme.html title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=/tags/albert.html title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/batch-normalization.html title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/bert.html title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/blog.html title=blog>blog</a>
<a class="widget-taglist__link widget__link btn" href=/tags/crf.html title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=/tags/deep-learning.html title="Deep Learning">Deep Learning</a>
<a class="widget-taglist__link widget__link btn" href=/tags/git.html title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=/tags/github-actions.html title="github actions">github actions</a>
<a class="widget-taglist__link widget__link btn" href=/tags/github-pages.html title="github pages">github pages</a>
<a class="widget-taglist__link widget__link btn" href=/tags/guice.html title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hexo.html title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hugo.html title=hugo>hugo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/kaggle.html title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=/tags/keras.html title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=/tags/layer-normalization.html title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/linux.html title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=/tags/log.html title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=/tags/logback.html title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=/tags/nginx.html title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=/tags/openwrt.html title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=/tags/slf4j.html title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=/tags/stanford-corenlp.html title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow.html title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow-hub.html title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow2.0.html title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tra.html title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=/tags/transformers.html title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=/tags/ubuntu.html title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%88%86%E8%AF%8D.html title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%93%88%E5%B8%8C.html title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D.html title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%B9%B6%E5%8F%91.html title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%8E%92%E5%BA%8F.html title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%A3%E5%88%97%E8%A1%A8.html title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E.html title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%97%A0%E7%9B%91%E7%9D%A3.html title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%86%B5.html title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%AE%97%E6%B3%95.html title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0.html title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.html title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script></body></html>