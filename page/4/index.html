<!doctype html><html class=no-js lang=en><head><meta name=generator content="Hugo 0.81.0"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Writing Lite"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="/"><meta itemprop=name content="Writing Lite"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Writing Lite"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel=alternate type=application/rss+xml href=/index.xml title="Writing Lite"><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/algorithm/2016-11-25-pagerank%E7%AE%97%E6%B3%95/ rel=bookmark>PageRank算法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/algorithm/ rel=category>algorithm</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 总的来讲，对于一个特定的查询，搜索结果的排名取决于两组信息，关于网页的质量信息，和这个查询与每个网页的相关信息。PageRank算法就是一种衡量网页质量的方法。
核心思想（原理） 在互联网上，如果一个网页被很多其它网页所连接，说明它受到普遍的承认和信赖，那么它的排名就高。
算法细节 算法的思想如上文所诉，但实际上要复杂得多。比如说，对于来自不同网页的链接区别对待，因此网页排名高的那些网页的链接更可靠，于是给这些链接以较大的权重。
可以想像，一个新开的质量差的网站与另一个质量好的老网站它们的权重显然是不同的，否则就可以轻易作弊。
那接下来的问题就是这些网站的权重要如何度量呢，可以想到应该是网页本身的网页排名。现在麻烦来了，计算搜索结果的网页排名的过程中需要用到网页本身的排名。
解决方法是，把这个问题变成一个二维矩阵相乘的问题，并且用迭代的方法解决。先假定所有网页的排名是相同的，并且根据这个初始值，算出各个网页的第一次迭代排名，然后再根据第一次迭代排名算出第二次的排名 。
PageRank计算方法 假定如下向量，为第一、第二、&mldr;第N个网页的网页排名。 $$ B=(b_1,b_2,&mldr;b_N)^T $$ 如下矩阵为网页之间的链接数目，其中$a_{mn}$代表第m个网页指向第n个网页的链接数。 $$ A= \begin{bmatrix} a_{11} & \cdots & a_{1n} & \cdots & a_{1M} \
\cdots \
a_{m1} & \cdots & a_{mn} & \cdots & a_{mM} \
\cdots \
a_{M1} & \cdots & a_{Mn} & \cdots & a_{MM} \
\end{bmatrix} $$ A是已知的，B是未知的，是我们需要计算的。 假定$B_i$是第i次的迭代的结果，那么 $$ B_i=A \cdot B_{i-1} $$ 初始假设：所有网页的排名都是1/N，即 $$ B_0=\left( \frac{1}{N},\frac{1}{N},\cdots,\frac{1}{N} \right) $$
**注：**关于矩阵计算的优化与技巧和链接数量的平滑，这里并没有给出，可以查阅相关资源。
参考 《数学之美》</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/deep-learning/2019-09-24-pytorch-bi-lstm-crf%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/ rel=bookmark>pytorch BI-LSTM CRF 代码解读</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">词性标注是比较基本的NLP任务，通常我们可以使用BI-LSTM CRF模型来完成；CRF在这里起的作用是能够约束标签序列使结果的合法性更好。例如，如果不使用CRF可能的结果是B-LOC O E-LOC
BLSTM这里就没什么说的了，我们这篇文章主要通过pytorch官网给的代码，讲解下CRF的实现部分。
我们定义两种概率矩阵，发射概率（emission ）和转移概率（transition）。$\text{EMIT}(y_i \rightarrow x_i)$表示 $x_i$映射到$y_i$的非归一化概率，$\text{TRANS}(y_{i-1} \rightarrow y_i)$表示 $y_{i-1}$转移到$y_{i}$的概率。 $$ P(y|x) = \frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})} \
\text{Score}(x,y) = \sum_i \log \psi_i(x,y) \
\text{Score}(x,y) = \sum_i \log \psi_\text{EMIT}(y_i \rightarrow x_i) + \log \psi_\text{TRANS}(y_{i-1} \rightarrow y_i) $$
因而在训练中，我们只需要最大化似然概率$P(y|x)$即可，这里我们利用对数似然 $$ \log{P(y|x)} = \log{(\frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})})} \
= \text{Score}(x, y) - \log{(\sum_{y'} \exp{(\text{Score}(x, y')}))} $$ 所以我们将损失函数定义为$-\log{P(y|x)}$，就可以利用梯度下降法来进行网络的学习了。
在对损失函数进行计算的时候，$\text{Score}(x,y)$的计算很简单，而$\log{(\sum_{y'} \exp{(\text{Score}(x, y')}))}$（下面记作logsumexp）的计算稍微复杂一些，这里使用前向算法（forward algorithm）来进行计算。
如下代码中，feats 代表[W1, W2, W3]，next_tags代表[t1, t2, t3, t4]，当feat=W1，next_tag=t1时，feat里的值是[0.1, 0.2, 0.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/other/2018-8-11-shadowsocks%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%9C%8D%E5%8A%A1%E5%AE%89%E8%A3%85/ rel=bookmark>shadowsocks服务端服务安装</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/other/ rel=category>other</a></span></div></div></header><div class="content list__excerpt post__content clearfix">Shadowsocks服务端服务安装 环境：centos
安装 安装python pip环境
sudo yum install python-setuptools && easy_install pip 通过pip安装shadowsocks
sudo pip install shadowsocks 配置 sudo vi /etc/shadowsocks.json { "server":"0.0.0.0", "local_address": "127.0.0.1", "local_port":1080, "server_port":my_server_port, "password":"my_password", "timeout":300, "method":"aes-256-cfb" } 防火墙设置 firewall-cmd --permanent --add-port=my_server_port/tcp firewall-cmd --permanent --add-port=my_server_port/udp firewall-cmd --reload 服务启动 ssserver -c /etc/shadowsocks.json -d start</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/nlp/2017-05-21-stanford-corenlp-%E4%BD%BF%E7%94%A8/ rel=bookmark>Stanford CoreNLP 使用</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/nlp/ rel=category>NLP</a></span></div></div></header><div class="content list__excerpt post__content clearfix">介绍 斯坦福CoreNLP提供了一套自然语言分析工具。斯坦福CoreNLP集成了许多斯坦福的NLP工具，包括词性（POS），命名实体识别（NER）， 语法解析，指代消解，情感分析，自举模式学习和开放信息提取工具。
如果有如下需求，要以选择Stanford CoreNLP：
具有良好语法分析工具的集成工具包 快速，可靠地分析任意文本 整体最高品质的文字分析 支持一些主要（人类）语言 可用于大多数主要现代编程语言的接口 能够作为简单的Web服务运行 Github地址：Stanford CoreNLP GitHub site.
使用前准备 Stanford CoreNLP是基于JAVA的，最新版本需要JDK1.8。JAR包可以通用官网，或者MAVEN下载到。如果使用MAVEN的话，可以使用如下配置。
&lt;dependencies> &lt;dependency> &lt;groupId>edu.stanford.nlp&lt;/groupId> &lt;artifactId>stanford-corenlp&lt;/artifactId> &lt;version>3.7.0&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>edu.stanford.nlp&lt;/groupId> &lt;artifactId>stanford-corenlp&lt;/artifactId> &lt;version>3.7.0&lt;/version> &lt;classifier>models&lt;/classifier> &lt;/dependency> &lt;/dependencies> 如果你想从阿拉伯语，中文，德语或西班牙语中获得Maven的语言模型，请将其添加到您的pom.xml：
&lt;dependency> &lt;groupId>edu.stanford.nlp&lt;/groupId> &lt;artifactId>stanford-corenlp&lt;/artifactId> &lt;version>3.7.0&lt;/version> &lt;classifier>models-chinese&lt;/classifier> &lt;/dependency> 将“models-chinese”替换为“models-english”，“models-chinese-kbp”，“models-arabic”，“models-french”，“models-german”或“models-spanish”为其他语言！
由于Stanford CoreNLP的模型文件太大（中文的模型就几百M），建议到官网下载。如果使用MAVEN的话，最好使用阿里的MAVEN仓库，这样速度更快些。
通过命令行使用 Quick Start java -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt -cp "*"是加载当前路径下的所有文件（主要是JAR包）
该-props参数是可选的。默认情况下，Stanford CoreNLP将在您的类路径中搜索StanfordCoreNLP.properties，并使用分发中包含的默认值。
该-annotators参数实际上是可选的。如果你离开它，代码使用一个内建的属性文件，它可以使用以下注释器：标记化和句子分割，POS标记，缩小，NER，解析和关联解析（也就是我们在这些例子中使用的）。
如果要使用其它语言的话，
java -mx3g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -file chinese.txt -outputFormat text 参考： https://stanfordnlp.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/deep-learning/2019-08-29-tensorflow-dataset/ rel=bookmark>Tensorflow dataset</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">tf.data API 在 TensorFlow 中引入了两个新的抽象类：
tf.data.Dataset 表示一系列元素，其中每个元素包含一个或多个 Tensor 对象。例如，在图像管道中，元素可能是单个训练样本，具有一对表示图像数据和标签的张量。可以通过两种不同的方式来创建数据集： 创建来源（例如 Dataset.from_tensor_slices()），以通过一个或多个 tf.Tensor 对象构建数据集。 应用转换（例如 Dataset.batch()），以通过一个或多个 tf.data.Dataset 对象构建数据集。 tf.data.Iterator 提供了从数据集中提取元素的主要方法。Iterator.get_next() 返回的操作会在执行时生成 Dataset 的下一个元素，并且此操作通常充当输入管道代码和模型之间的接口。最简单的迭代器是“单次迭代器”，它与特定的 Dataset 相关联，并对其进行一次迭代。要实现更复杂的用途，您可以通过 Iterator.initializer 操作使用不同的数据集重新初始化和参数化迭代器，这样一来，您就可以在同一个程序中对训练和验证数据进行多次迭代（举例而言）。 1、从dataset中获取数据 1.1 one_shot_iterator import tensorflow as tf features = [[1,1,2,2], [3,3,4,4], [5,5,6,6]] # [1,1,2,2] 是一个sample ，1 1 2 2 是四个特征值 labels = [0, 1, 1] # 每个sample的标签值 dataset = tf.data.Dataset.from_tensor_slices((features, labels)) iterator = dataset.make_one_shot_iterator() next_element = iterator.get_next() with tf.Session() as sess: while True: try: value = sess.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/deep-learning/2019-09-03-tensorflow-example-%E5%92%8C-tfrecord/ rel=bookmark>Tensorflow example 和 TFRecord</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">生成tf.example import tensorflow as tf import numpy as np def generate_example(feature0, feature1, feature2, feature3): """ Creates a tf.Example message ready to be written to a file. """ # Create a dictionary mapping the feature name to the tf.Example-compatible # data type. feature = { 'feature0': _int64_feature(feature0), 'feature1': _int64_feature(feature1), 'feature2': _bytes_feature(feature2), 'feature3': _float_feature(feature3), } # Create a Features message using tf.train.Example. example_proto = tf.train.Example(features=tf.train.Features(feature=feature)) return example_proto def _bytes_feature(value): """Returns a bytes_list from a string / byte.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/deep-learning/2019-08-20-tensorflow-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E6%81%A2%E5%A4%8D/ rel=bookmark>Tensorflow 模型的保存和恢复</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">tensorflow模型的保存和恢复方法基本上可以参考其官方文档： https://www.tensorflow.org/guide/saved_model?hl=zh-cn 本文讨论 自定义estimato保存恢复方法r 和 fine-tune 的参数恢复方法
自定义estimator的保存和恢复 run_config = tf.estimator.RunConfig(model_dir=args.output_dir, save_summary_steps=500, save_checkpoints_steps=500, session_config=session_config) estimator = tf.estimator.Estimator( model_fn, params=params, config=run_config) 在train时会根据参数自动读取和保存模型 如果checkpoint 中的状态与描述的模型不兼容，因此重新训练失败并出现以下错误：
... InvalidArgumentError (see above for traceback): tensor_name = dnn/hiddenlayer_1/bias/t_0/Adagrad; shape in shape_and_slice spec [10] does not match the shape stored in checkpoint: [20] fine-tune 参数恢复方法 tvars = tf.trainable_variables() (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint) tf.train.init_from_checkpoint(init_checkpoint, assignment_map) graph = tf.get_default_graph() test_varibal = graph.get_tensor_by_name('bert/encoder/layer_9/output/dense/bias:0') with tf.Session() as sess: # 最后初始化变量 sess.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/deep-learning/2019-12-12-tensorflow2.0-%E5%9F%BA%E4%BA%8Ebert%E7%9A%84%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98huggingface-transformers/ rel=bookmark>Tensorflow2.0 基于BERT的开发实战（huggingface-transformers）</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">tensorflow2.0 刚刚发布正式版，网上关于tf2.0使用bert的文章较少。之前想tf2.0做基于bert的NER任务，想了不少资料踩了许多坑，因此将期间的过程总结成文。
在tf2.0上使用bert，主要的难点还是：自己写tf2.0代码适配google的官方源码的方案工作量大对tf熟练度的要求比较高。因此选择了第三方的实现，这里使用的是huggingface的transformers。
除了transformers，其它兼容tf2.0的bert项目还有：
keras-bert（Star:1.3k, Kashgari在使用）, 现在也开始兼容tf2.0了，但它只支持bert一种预训练模型
bert-for-tf2（Star:280），缺点是不是很正规，只给了tf2.0 pipeline示例
在tf2.0正式版发布前后huggingface的transformers也发布了transformers2.0，开始支持tf.2.0的各个预训练模型，虽然没有对pytorch支持的那么全面但在我们的场景已经足够适用了。
环境 tensorflow版本：2.0.0
transformers版本：2.2.1
构建模型 class BertNerModel(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self.bert_layer = TFBertMainLayer(config, name='bert') self.bert_layer.trainable = False self.concat_layer = tf.keras.layers.Concatenate(name='concat_bert') def call(self, inputs): outputs = self.bert_layer(inputs) #将后n层的结果相连 tensor = self.concat_layer(list(outputs[2][-4:])) 这里给出的是简要的代码，可以自行根据任务在bert_layer之后加入RNN等
自定义模型的写法可以参考官方源码里的TFBertForSequenceClassification， 继承TFBertPreTrainedModel
self.bert_layer(inputs)的返回值为tuple类型：
最后1层隐藏层的输出值，shape=(batch_size, max_length, hidden_dimention) [CLS] 对应的输出值，shape=(batch_size, hidden_dimention) 只有设置了config.output_hidden_states = True，才有该值，所有隐藏层的输出值，返回值类型是list 每个list里的值的shape是`(batch_size, max_length, hidden_dimention)`` 模型的初始化 bert_ner_model = BertNerModel.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/deep-learning/2019-11-22-tensorflow2.0%E4%BD%BF%E7%94%A8berttransformers%E4%B8%8Ekashgari/ rel=bookmark>Tensorflow2.0使用bert：transformers与kashgaria</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">**背景：**最近打算做NER任务，打算使用kashgaria当baseline，然后使用transformers来做模型开发。
**问题：**使用kashgaria来做Baseline，使用人民日报语料可以得到99.99%的准确率，但是使用transformers的TFBertForTokenClassification来做的话只能得到75%左右的效果， 看了两个项目的源码开始怀疑是模型结构的问题，因此使用transformers模拟kashgaria的结果又做了一次，效果还是在75%左右。相关代码如下：
kashgaria代码：
from kashgari.corpus import ChineseDailyNerCorpus from kashgari.tasks.labeling import BiLSTM_Model from kashgari.embeddings import BERTEmbedding import kashgari train_x, train_y = ChineseDailyNerCorpus.load_data('train') test_x, test_y = ChineseDailyNerCorpus.load_data('test') valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid') bert_embed = BERTEmbedding('chinese_L-12_H-768_A-12', task=kashgari.LABELING, sequence_length=100) model = BiLSTM_Model(bert_embed) model.fit(train_x, train_y, valid_x, valid_y, epochs=1) transformers代码：
# 数据的处理省略，但使用的同样语料 model = TFBertForTokenClassification.from_pretrained('bert-base-chinese') model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') model.fit(all_input_ids, tf.constant(all_label_ids_ca, dtype=tf.float32), epochs=2, batch_size=32) transformers 模拟kashgaria代码：
class BERT_NER(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%981-tensorflow%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%90%AD%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ rel=bookmark>Tensorflow实战(1)-Tensorflow基本介绍及搭建全连接神经网络</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/tensorflow2.x-keras/ rel=category>Tensorflow2.x keras</a></span></div></div></header><div class="content list__excerpt post__content clearfix">大家好，今天是Tensorflow实战系列的第一次分享，本次分享共有3个主题：
Tensorflow介绍 Tensorflow核心概念 使用Tensorflow实现手写数字识别任务 由于本次分享内容以实战为主不会涉及过多理论的讲解，但是分享过程中涉及到的理论知识也会做一个快速回顾。
Tensorflow Introduction Tensorflow 是由Google研发的开源软件库，它既是一个实现机器学习算法的接口，同时也是执行机器学习算法的框架，它对深度学习中常用的神经网络结构等算法进行了封装，因此开发人员可以快速的进行模型搭建。
1、Tensorflow 发展史 2011 年，Google Brain内部孵化出一个项目叫做DistBelief, 它是为深度神经网络构建的一个机器学习系统，是Tensorflow的前身。 2015年11月，Google正式发布了Tensorflow的白皮书并开源TensorFlow 0.1 版本。 2017年02月，Tensorflow正式发布了1.0.0版本，同时也标志着稳定版的诞生。 2019年10月，TensorFlow在经历七个多月(2019年3月1日-2019年10月1日)的2.0 Alpha 版本的更新迭代后发布 2.0 正式版。 通过上面的发展史我们可以看到，虽然经过了9年时间Tensorflow依然是目前最流行的深度学习框架之一。
2、Tensorflow VS Pytorch 上面说到Tensorflow是目前最流行的深度学习框架之一，那另一款可以和Tensorflow一较高下的深度学习框架就是-Pytorch了。Pytorch是由Facebook研发的一款开源的机器学习库，自16年发布以来发展非常迅猛。Tensorflow和Pytorch如何选择呢，我的看法是：都可以，虽然刚开始时Pytorch和Tensorflow还是差别较大的，比较Pytorch有动态图、类python的编程方式，Tensorflow则支持可视化，生产部署更加简单易用，但通过这几年的发展Pytorch和Tensorfow越来越像了，Tensorflow添加了动态图，而Pytorch也在工业部署上有了很大改善。因此在两都的选择上不必太过纠结。
import tensorflow as tf import matplotlib as mpl %matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as pd import os import sys print(sys.version) 3.6.9 (default, Nov 7 2019, 10:44:02) [GCC 8.3.0] for module in tf, np, mpl: print(module.</div></article></main><div class=pagination><a class="pagination__item pagination__item--prev btn" href=/page/3/>«</a>
<span class="pagination__item pagination__item--current">4/8</span>
<a class="pagination__item pagination__item--next btn" href=/page/5/>»</a></div></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/post/other/2021-05-06-%E4%BD%BF%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%88%B0github-pages/>使用GitHub Actions自动部署hugo到GitHub Pages</a></li><li class=widget__item><a class=widget__link href=/post/other/2020-03-02-%E5%9F%BA%E4%BA%8Enginx%E7%9A%84acme%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E6%96%B9%E6%A1%88/>基于nginx的acme免费证书方案</a></li><li class=widget__item><a class=widget__link href=/post/other/2019-11-04-%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E5%AE%89%E8%A3%85-certbot/>免费证书安装-certbot</a></li><li class=widget__item><a class=widget__link href=/post/other/2019-07-27-%E6%A0%91%E8%8E%93%E6%B4%BE3b+-%E5%AE%89%E8%A3%85openwrt/>树莓派3b+安装openwrt 18.06.4</a></li><li class=widget__item><a class=widget__link href=/post/other/2019-05-18-git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Git 常用命令</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/categories/algorithm/>algorithm</a></li><li class=widget__item><a class=widget__link href=/categories/deep-learning/>deep learning</a></li><li class=widget__item><a class=widget__link href=/categories/machine-learning/>machine learning</a></li><li class=widget__item><a class=widget__link href=/categories/mathematics/>mathematics</a></li><li class=widget__item><a class=widget__link href=/categories/nlp/>NLP</a></li><li class=widget__item><a class=widget__link href=/categories/other/>other</a></li><li class=widget__item><a class=widget__link href=/categories/reading/>reading</a></li><li class=widget__item><a class=widget__link href=/categories/tensorflow2.x-keras/>Tensorflow2.x keras</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=/tags/acme/ title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=/tags/albert/ title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/batch-normalization/ title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/bert/ title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/blog/ title=blog>blog</a>
<a class="widget-taglist__link widget__link btn" href=/tags/crf/ title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=/tags/git/ title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=/tags/github-actions/ title="github actions">github actions</a>
<a class="widget-taglist__link widget__link btn" href=/tags/github-pages/ title="github pages">github pages</a>
<a class="widget-taglist__link widget__link btn" href=/tags/guice/ title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hexo/ title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hugo/ title=hugo>hugo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/kaggle/ title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=/tags/keras/ title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=/tags/layer-normalization/ title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/linux/ title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=/tags/log/ title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=/tags/logback/ title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=/tags/nginx/ title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=/tags/openwrt/ title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=/tags/slf4j/ title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=/tags/stanford-corenlp/ title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow/ title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow-hub/ title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow2.0/ title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tra/ title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=/tags/transformers/ title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=/tags/ubuntu/ title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%88%86%E8%AF%8D/ title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%93%88%E5%B8%8C/ title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/ title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%B9%B6%E5%8F%91/ title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%8E%92%E5%BA%8F/ title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%A3%E5%88%97%E8%A1%A8/ title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/ title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%97%A0%E7%9B%91%E7%9D%A3/ title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%86%B5/ title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%AE%97%E6%B3%95/ title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/ title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/ title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script></body></html>