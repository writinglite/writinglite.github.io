<!doctype html><html class=no-js lang=en><head><meta name=generator content="Hugo 0.81.0"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Writing Lite"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="/"><meta itemprop=name content="Writing Lite"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Writing Lite"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel=alternate type=application/rss+xml href=/index.xml title="Writing Lite"><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/archives/><span class=menu__text>archives</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-05-09-transformers%E7%9A%84%E4%B8%80%E4%BA%9B%E8%BF%B7%E6%80%9D.html rel=bookmark>Transformers的一些迷思</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-05-09T21:07:00Z>2020-05-09</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">通过from_pretrained缓存的模型在哪 如果调用from_pretrained方法时指定了cache_dir 则保存到cache_dir，
cache_dir = kwargs.pop("cache_dir", None) 如果没指定则去通过系统环境变量寻找（&ldquo;PYTORCH_TRANSFORMERS_CACHE&rdquo;", &ldquo;PYTORCH_PRETRAINED_BERT_CACHE&rdquo;）
os.getenv("PYTORCH_TRANSFORMERS_CACHE", os.getenv("PYTORCH_PRETRAINED_BERT_CACHE", default_cache_path)) 如果还没找到则设置为pytorch_home下的transformers目录下
from torch.hub import _get_torch_home torch_cache_home = _get_torch_home() os.path.join(torch_cache_home, "transformers") from_pretrained方法是如何加载模型的 首先判断是否在pretrained_model_archive_map中，然后判断是否为目录或文件，如果都不是则默认为hf_bucket_url
https://s3.amazonaws.com/models.huggingface.co/bert/{pretrained_model_name_or_path}/{pytorch_model.bin/tf_model.h5} pytorch_model.bin或tf_model.h5 通过from_tf判断
不同模型实现from_pretrained的方式 from_pretrained 的根据不同 cls 来实现加载不同模型的差异， 以bert为例， cls -> BertPreTrainedModel；
class BertPreTrainedModel(PreTrainedModel): """ An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models. """ config_class = BertConfig pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP load_tf_weights = load_tf_weights_in_bert base_model_prefix = "bert" def _init_weights(self, module): """ Initialize the weights """ if isinstance(module, (nn.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/other/2020-03-02-%E5%9F%BA%E4%BA%8Enginx%E7%9A%84acme%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E6%96%B9%E6%A1%88.html rel=bookmark>基于nginx的acme免费证书方案</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-03-02T17:36:15Z>2020-03-02</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/other.html rel=category>other</a></span></div></div></header><div class="content list__excerpt post__content clearfix">为了保护用户信息使网站更加安全，需要给网站添加https协议。搭建一个https网站的前提是要先拥有一个证书，当然一般的证书是需要收费，文本提供一个免费的解决方案。
我们使用的github上开源的免费证书工具ACME.SH，网站地址：https://github.com/acmesh-official/acme.sh，
在其github主页上已经有了一些很好的入门说明。这里根据实际情况进行操作。
依赖安装 apt-get install cron socat -y 安装ACME.sh curl https://get.acme.sh | sh 安装成功后会提示Install success!，这个命令会将acme命令写到batchrc里，为了方便使用需要 ：
source ~/.bashrc 申请证书 acme.sh --issue -d example.com --standalone 生成的证书被放在 /root/.acme.sh/
复制证书到nginx acme.sh --installcert -d freegoooovideos.ml \ --key-file /etc/nginx/ssl/freegoooovideos.key \ --fullchain-file /etc/nginx/ssl/fullchain.cer \ --reloadcmd "systemctl force-reload nginx.service" #或sudo service nginx force-reload reloadcmd 会记住让nginx重新加载的方式 ，这样证书更新的时候就可以让你nginx重新加载了。
这个命令不只是复制，它会把信息记录到本地中（.acme/example.com/example.com.conf），这样在更新证书的时候会自动将文件复制到容器中，并让其重新加载配置。
附：nginx的配置文件（/etc/nginx/nginx.conf 默认位置）
user nginx; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { map $http_upgrade $connection_upgrade { default upgrade; '' close; } server{ listen 443 ssl; ssl on; proxy_redirect off; ssl_certificate /etc/nginx/ssl/fullchain.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-02-20-albert%E5%9C%A8bert%E5%9F%BA%E7%A1%80%E4%B8%8A%E7%9A%84%E5%87%A0%E7%82%B9%E6%94%B9%E8%BF%9B.html rel=bookmark>Albert在Bert基础上的几点改进</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-02-20T21:07:00Z>2020-02-20</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">减少参数 减少Embeding参数 ，用两层替代之前的一层，参数从原来的V * H 变成 V * E + E * H ， 这个E &#171; H 共享Block参数 ​ 这样做的好处是，将参数减少，进而增加模型的深度和宽度来提升模型效果，但同时带来了计算量的增加（大概3倍）
改进训练任务 通过实验表示，Next Sentence Predict 任务太过简单，使用 Reverce 的方式会更好；
去掉Dropout Dropout实际的操作是防止过拟合，但对于无监督学习来说，训练语料是很多的不会有过拟合的问题，使用Dropout反而会增加内存的使用（会有一些缓存），去掉Dropout会有0.3的性能提升
增加训练数据 这个就没啥说的了
最重要的一点还是减少参数增加模型的深度和宽度带来的</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-12-12-tensorflow2.0-%E5%9F%BA%E4%BA%8Ebert%E7%9A%84%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98huggingface-transformers.html rel=bookmark>Tensorflow2.0 基于BERT的开发实战（huggingface-transformers）</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-12-12T08:07:00Z>2019-12-12</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">tensorflow2.0 刚刚发布正式版，网上关于tf2.0使用bert的文章较少。之前想tf2.0做基于bert的NER任务，想了不少资料踩了许多坑，因此将期间的过程总结成文。
在tf2.0上使用bert，主要的难点还是：自己写tf2.0代码适配google的官方源码的方案工作量大对tf熟练度的要求比较高。因此选择了第三方的实现，这里使用的是huggingface的transformers。
除了transformers，其它兼容tf2.0的bert项目还有：
keras-bert（Star:1.3k, Kashgari在使用）, 现在也开始兼容tf2.0了，但它只支持bert一种预训练模型
bert-for-tf2（Star:280），缺点是不是很正规，只给了tf2.0 pipeline示例
在tf2.0正式版发布前后huggingface的transformers也发布了transformers2.0，开始支持tf.2.0的各个预训练模型，虽然没有对pytorch支持的那么全面但在我们的场景已经足够适用了。
环境 tensorflow版本：2.0.0
transformers版本：2.2.1
构建模型 class BertNerModel(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self.bert_layer = TFBertMainLayer(config, name='bert') self.bert_layer.trainable = False self.concat_layer = tf.keras.layers.Concatenate(name='concat_bert') def call(self, inputs): outputs = self.bert_layer(inputs) #将后n层的结果相连 tensor = self.concat_layer(list(outputs[2][-4:])) 这里给出的是简要的代码，可以自行根据任务在bert_layer之后加入RNN等
自定义模型的写法可以参考官方源码里的TFBertForSequenceClassification， 继承TFBertPreTrainedModel
self.bert_layer(inputs)的返回值为tuple类型：
最后1层隐藏层的输出值，shape=(batch_size, max_length, hidden_dimention) [CLS] 对应的输出值，shape=(batch_size, hidden_dimention) 只有设置了config.output_hidden_states = True，才有该值，所有隐藏层的输出值，返回值类型是list 每个list里的值的shape是`(batch_size, max_length, hidden_dimention)`` 模型的初始化 bert_ner_model = BertNerModel.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-11-22-tensorflow2.0%E4%BD%BF%E7%94%A8berttransformers%E4%B8%8Ekashgari.html rel=bookmark>Tensorflow2.0使用bert：transformers与kashgaria</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-11-22T11:07:00Z>2019-11-22</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">**背景：**最近打算做NER任务，打算使用kashgaria当baseline，然后使用transformers来做模型开发。
**问题：**使用kashgaria来做Baseline，使用人民日报语料可以得到99.99%的准确率，但是使用transformers的TFBertForTokenClassification来做的话只能得到75%左右的效果， 看了两个项目的源码开始怀疑是模型结构的问题，因此使用transformers模拟kashgaria的结果又做了一次，效果还是在75%左右。相关代码如下：
kashgaria代码：
from kashgari.corpus import ChineseDailyNerCorpus from kashgari.tasks.labeling import BiLSTM_Model from kashgari.embeddings import BERTEmbedding import kashgari train_x, train_y = ChineseDailyNerCorpus.load_data('train') test_x, test_y = ChineseDailyNerCorpus.load_data('test') valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid') bert_embed = BERTEmbedding('chinese_L-12_H-768_A-12', task=kashgari.LABELING, sequence_length=100) model = BiLSTM_Model(bert_embed) model.fit(train_x, train_y, valid_x, valid_y, epochs=1) transformers代码：
# 数据的处理省略，但使用的同样语料 model = TFBertForTokenClassification.from_pretrained('bert-base-chinese') model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') model.fit(all_input_ids, tf.constant(all_label_ids_ca, dtype=tf.float32), epochs=2, batch_size=32) transformers 模拟kashgaria代码：
class BERT_NER(TFBertPreTrainedModel): def __init__(self, config, *inputs, **kwargs): super(BERT_NER, self).__init__(config, *inputs, **kwargs) self.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/other/2019-11-04-%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E5%AE%89%E8%A3%85-certbot.html rel=bookmark>免费证书安装-certbot</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-11-04T17:36:15Z>2019-11-04</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/other.html rel=category>other</a></span></div></div></header><div class="content list__excerpt post__content clearfix">为了给网站添加https协议，本文记录了如何免费获取证书。
免费的证书颁发机构：letsencrypt
为了能使用证书还需要一个可以操作letsencrypt的客户端，这里使用官网推荐的工具certbot
certbot的安装根据不同的系统安装方式不同，可以根据官网来操作。
这里给出Debain10的安装方式：
sudo apt-get install certbot 安装好之后就可以获取证书了，如果想把证书安装到nginx或apache等上，可以安装相应的插件。这里给出的是standalone方式：
sudo certbot certonly --standalone -d yourdomain.me 输入命令之后，会提示输入email之类的，IMPORTANT NOTES:会提示获取的证书最终的输出位置。
关于证书的更新
需要注意的是，现在获取的证书的有效期是90天，90后证书就是过期，因此需要更新证书。
正常安装certbot之后，证书是自动更新的，可参看官方文档
可通过systemctl list-timers查看或查看 crotab
例如在debian中在/etc/cron.d/certbot可以找到
0 */12 * * * root test -x /usr/bin/certbot -a \! -d /run/systemd/system && perl -e 'sleep int(rand(43200))' && certbot -q renew 在renew中，我们可以添加 ：
certbot renew -q --pre-hook "nginx -s stop" --post-hook "nginx" -q：不进行输出
--pre-hook、--post-hook如果证书可以更新（只有快过期时才会更新），则执行前后的命令。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-09-24-pytorch-bi-lstm-crf%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB.html rel=bookmark>pytorch BI-LSTM CRF 代码解读</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-09-24T10:07:00Z>2019-09-24</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">词性标注是比较基本的NLP任务，通常我们可以使用BI-LSTM CRF模型来完成；CRF在这里起的作用是能够约束标签序列使结果的合法性更好。例如，如果不使用CRF可能的结果是B-LOC O E-LOC
BLSTM这里就没什么说的了，我们这篇文章主要通过pytorch官网给的代码，讲解下CRF的实现部分。
我们定义两种概率矩阵，发射概率（emission ）和转移概率（transition）。$\text{EMIT}(y_i \rightarrow x_i)$表示 $x_i$映射到$y_i$的非归一化概率，$\text{TRANS}(y_{i-1} \rightarrow y_i)$表示 $y_{i-1}$转移到$y_{i}$的概率。 $$ P(y|x) = \frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})} \
\text{Score}(x,y) = \sum_i \log \psi_i(x,y) \
\text{Score}(x,y) = \sum_i \log \psi_\text{EMIT}(y_i \rightarrow x_i) + \log \psi_\text{TRANS}(y_{i-1} \rightarrow y_i) $$
因而在训练中，我们只需要最大化似然概率$P(y|x)$即可，这里我们利用对数似然 $$ \log{P(y|x)} = \log{(\frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})})} \
= \text{Score}(x, y) - \log{(\sum_{y'} \exp{(\text{Score}(x, y')}))} $$ 所以我们将损失函数定义为$-\log{P(y|x)}$，就可以利用梯度下降法来进行网络的学习了。
在对损失函数进行计算的时候，$\text{Score}(x,y)$的计算很简单，而$\log{(\sum_{y'} \exp{(\text{Score}(x, y')}))}$（下面记作logsumexp）的计算稍微复杂一些，这里使用前向算法（forward algorithm）来进行计算。
如下代码中，feats 代表[W1, W2, W3]，next_tags代表[t1, t2, t3, t4]，当feat=W1，next_tag=t1时，feat里的值是[0.1, 0.2, 0.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-09-14-bert%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html rel=bookmark>bert的基本使用</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-09-14T08:07:00Z>2019-09-14</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">本文试图回答的问题：
bert的使用过程是什么样的 bert的输入输出是什么样的 在使用bert的过程中有哪些关键点 源码下载 !git clone https://github.com/google-research/bert.git 下载bert中文预训练模型文件 # 下载 !wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip # 解压 !unzip chinese_L-12_H-768_A-12.zip 定义模型 import tensorflow as tf from bert import modeling max_length = 10 input_ids = tf.placeholder(tf.int64, shape=[bash_size, max_length]) input_mask = tf.placeholder(tf.int64, shape=[bash_size, max_length]) token_type_ids = tf.placeholder(tf.int64, shape=[bash_size, max_length]) # 生成bert_config """" 不使用预训练模型时也可以自己定义： config = modeling.BertConfig(vocab_size=32000, hidden_size=512, num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024) """ bert_config_file = 'chinese_L-12_H-768_A-12/bert_config.json' bert_config = modeling.BertConfig.from_json_file(bert_config_file) model = modeling.BertModel( config=bert_config, input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids, is_training=False, use_one_hot_embeddings=True) 生成测试数据 from bert import tokenization tokenizer = tokenization.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-09-03-tensorflow-example-%E5%92%8C-tfrecord.html rel=bookmark>Tensorflow example 和 TFRecord</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-09-03T20:07:00Z>2019-09-03</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">生成tf.example import tensorflow as tf import numpy as np def generate_example(feature0, feature1, feature2, feature3): """ Creates a tf.Example message ready to be written to a file. """ # Create a dictionary mapping the feature name to the tf.Example-compatible # data type. feature = { 'feature0': _int64_feature(feature0), 'feature1': _int64_feature(feature1), 'feature2': _bytes_feature(feature2), 'feature3': _float_feature(feature3), } # Create a Features message using tf.train.Example. example_proto = tf.train.Example(features=tf.train.Features(feature=feature)) return example_proto def _bytes_feature(value): """Returns a bytes_list from a string / byte.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2019-08-29-tensorflow-dataset.html rel=bookmark>Tensorflow dataset</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-08-29T20:07:00Z>2019-08-29</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/deep-learning.html rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">tf.data API 在 TensorFlow 中引入了两个新的抽象类：
tf.data.Dataset 表示一系列元素，其中每个元素包含一个或多个 Tensor 对象。例如，在图像管道中，元素可能是单个训练样本，具有一对表示图像数据和标签的张量。可以通过两种不同的方式来创建数据集： 创建来源（例如 Dataset.from_tensor_slices()），以通过一个或多个 tf.Tensor 对象构建数据集。 应用转换（例如 Dataset.batch()），以通过一个或多个 tf.data.Dataset 对象构建数据集。 tf.data.Iterator 提供了从数据集中提取元素的主要方法。Iterator.get_next() 返回的操作会在执行时生成 Dataset 的下一个元素，并且此操作通常充当输入管道代码和模型之间的接口。最简单的迭代器是“单次迭代器”，它与特定的 Dataset 相关联，并对其进行一次迭代。要实现更复杂的用途，您可以通过 Iterator.initializer 操作使用不同的数据集重新初始化和参数化迭代器，这样一来，您就可以在同一个程序中对训练和验证数据进行多次迭代（举例而言）。 1、从dataset中获取数据 1.1 one_shot_iterator import tensorflow as tf features = [[1,1,2,2], [3,3,4,4], [5,5,6,6]] # [1,1,2,2] 是一个sample ，1 1 2 2 是四个特征值 labels = [0, 1, 1] # 每个sample的标签值 dataset = tf.data.Dataset.from_tensor_slices((features, labels)) iterator = dataset.make_one_shot_iterator() next_element = iterator.get_next() with tf.Session() as sess: while True: try: value = sess.</div></article></main><div class=pagination><a class="pagination__item pagination__item--prev btn" href=/>«</a>
<span class="pagination__item pagination__item--current">2/8</span>
<a class="pagination__item pagination__item--next btn" href=/page/3.html>»</a></div></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-21-%E5%9F%BA%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%B9%B3%E5%9D%87%E7%9A%84%E7%83%AD%E8%AF%8D%E5%8F%91%E7%8E%B0%E7%AE%97%E6%B3%95.html>基于贝叶斯平均的新词发现算法</a></li><li class=widget__item><a class=widget__link href=/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/2021-05-13-han-for-document-classification.html>HAN for Document Classification</a></li><li class=widget__item><a class=widget__link href=/post/other/2021-05-06-%E4%BD%BF%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%88%B0github-pages.html>使用GitHub Actions自动部署hugo到GitHub Pages</a></li><li class=widget__item><a class=widget__link href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-10-16-deeplearning%E4%B8%ADcrf%E7%9A%84tensorflow%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.html>DeepLearning中CRF的Tensorflow代码实现</a></li><li class=widget__item><a class=widget__link href=/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2020-10-16-deeplearning%E4%B8%ADcrf%E8%AE%A1%E7%AE%97%E5%8E%9F%E7%90%86.html>DeepLearning中CRF计算原理</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/categories/algorithm.html>algorithm</a></li><li class=widget__item><a class=widget__link href=/categories/deep-learning.html>deep learning</a></li><li class=widget__item><a class=widget__link href=/categories/machine-learning.html>machine learning</a></li><li class=widget__item><a class=widget__link href=/categories/mathematics.html>mathematics</a></li><li class=widget__item><a class=widget__link href=/categories/nlp.html>NLP</a></li><li class=widget__item><a class=widget__link href=/categories/other.html>other</a></li><li class=widget__item><a class=widget__link href=/categories/reading.html>reading</a></li><li class=widget__item><a class=widget__link href=/categories/tensorflow2.x-keras.html>Tensorflow2.x keras</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=/tags/acme.html title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=/tags/albert.html title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/batch-normalization.html title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/bert.html title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=/tags/blog.html title=blog>blog</a>
<a class="widget-taglist__link widget__link btn" href=/tags/crf.html title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=/tags/deep-learning.html title="Deep Learning">Deep Learning</a>
<a class="widget-taglist__link widget__link btn" href=/tags/git.html title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=/tags/github-actions.html title="github actions">github actions</a>
<a class="widget-taglist__link widget__link btn" href=/tags/github-pages.html title="github pages">github pages</a>
<a class="widget-taglist__link widget__link btn" href=/tags/guice.html title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hexo.html title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/hugo.html title=hugo>hugo</a>
<a class="widget-taglist__link widget__link btn" href=/tags/kaggle.html title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=/tags/keras.html title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=/tags/layer-normalization.html title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=/tags/linux.html title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=/tags/log.html title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=/tags/logback.html title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=/tags/nginx.html title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=/tags/openwrt.html title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=/tags/slf4j.html title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=/tags/stanford-corenlp.html title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow.html title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow-hub.html title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tensorflow2.0.html title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tra.html title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=/tags/transformers.html title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=/tags/ubuntu.html title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%88%86%E8%AF%8D.html title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%93%88%E5%B8%8C.html title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D.html title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%B9%B6%E5%8F%91.html title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%8E%92%E5%BA%8F.html title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%A3%E5%88%97%E8%A1%A8.html title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E.html title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0.html title=新词发现>新词发现</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%97%A0%E7%9B%91%E7%9D%A3.html title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%86%B5.html title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%AE%97%E6%B3%95.html title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0.html title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.html title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/js/custom.js></script></body></html>