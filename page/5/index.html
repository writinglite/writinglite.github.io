<!doctype html><html class=no-js lang=en><head><meta name=generator content="Hugo 0.81.0"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Writing Lite</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Writing Lite"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="writinglite.com/"><meta itemprop=name content="Writing Lite"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Writing Lite"><meta name=twitter:description content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/writinglite.com/css/style.css><link rel=stylesheet href=/writinglite.com/css/custom.css><link rel=alternate type=application/rss+xml href=writinglite.com/index.xml title="Writing Lite"><link rel="shortcut icon" href=/writinglite.com/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/writinglite.com title="Writing Lite" rel=home><div class="logo__item logo__text"><div class=logo__title>Writing Lite</div><div class=logo__tagline>Just writing a lite blog</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/deep-learning/2020-05-09-transformers%E7%9A%84%E4%B8%80%E4%BA%9B%E8%BF%B7%E6%80%9D/ rel=bookmark>Transformers的一些迷思</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">通过from_pretrained缓存的模型在哪 如果调用from_pretrained方法时指定了cache_dir 则保存到cache_dir，
cache_dir = kwargs.pop("cache_dir", None) 如果没指定则去通过系统环境变量寻找（&ldquo;PYTORCH_TRANSFORMERS_CACHE&rdquo;", &ldquo;PYTORCH_PRETRAINED_BERT_CACHE&rdquo;）
os.getenv("PYTORCH_TRANSFORMERS_CACHE", os.getenv("PYTORCH_PRETRAINED_BERT_CACHE", default_cache_path)) 如果还没找到则设置为pytorch_home下的transformers目录下
from torch.hub import _get_torch_home torch_cache_home = _get_torch_home() os.path.join(torch_cache_home, "transformers") from_pretrained方法是如何加载模型的 首先判断是否在pretrained_model_archive_map中，然后判断是否为目录或文件，如果都不是则默认为hf_bucket_url
https://s3.amazonaws.com/models.huggingface.co/bert/{pretrained_model_name_or_path}/{pytorch_model.bin/tf_model.h5} pytorch_model.bin或tf_model.h5 通过from_tf判断
不同模型实现from_pretrained的方式 from_pretrained 的根据不同 cls 来实现加载不同模型的差异， 以bert为例， cls -> BertPreTrainedModel；
class BertPreTrainedModel(PreTrainedModel): """ An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models. """ config_class = BertConfig pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP load_tf_weights = load_tf_weights_in_bert base_model_prefix = "bert" def _init_weights(self, module): """ Initialize the weights """ if isinstance(module, (nn.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/reading/2018-10-04-%E4%B8%80%E5%B0%8F%E6%97%B6%E5%A4%96/ rel=bookmark>一小时外</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/reading/ rel=category>reading</a></span></div></div></header><div class="content list__excerpt post__content clearfix">「做事就可以有结果，不管做的是什么事，都一样。」
心理治疗的目标，并不是制造一种廉价的甚至虚幻的开心，恰恰相反，它应当致力于让来访者有更高的内省力，更强的自主感，更符合现实的自尊，更清晰地认识并处理自身情绪的能力，面对困境时更强的自我力量以及自我协调性，爱的能力，工作的能力，以及成熟依赖的能力——最终的目标，是进入一种她称之为「平和」的心境。而那也许是一个人毕生的追求。
我相信生命是美好的，我深信这种美好，即使我们身处严峻的时代。但同时我也深信，这美好是一种复杂而深沉的体验，它同时包含着开心和痛苦，包含着我所体验到的一切真实。它承载我，如同大地。如果这种承载带来愉悦，那自然值得享受，而如果这种承载带来痛苦，那自然也就值得深深的哀伤——握着它，体会它的痛感，而不是幻想它变成一种别的什么。
我们感到愤怒，往往是我们认为一件事不公平，打破了某种既定的规则，
一个半吊子，图样图森破，但是自信，敢讲，也许会被当面反驳，也许会被背后窃笑。 或者玩沉默，笑而不语，光听不评，别人摸不清水有多深，也可能会显得很怯懦。 ——无所谓哪种比哪种更好，两种形象各有人喜欢，也各有人反感。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-07-06-%E4%BC%BC%E7%84%B6%E4%B8%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/ rel=bookmark>似然与极大似然估计</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">概要 本文先会介绍似然的概念，似然与概率的区别，然后介绍参数估计的方法——极大似然估计。
似然 在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。
概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性，比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；
而似然刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数），还是抛硬币的例子，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%，这个过程就是我们根据结果来判断这个事情本身的性质（参数），也就是似然。
极大似然估计 似大似然估计解决的问题是，最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。似大似然估计经常在机器学习方法中作为一种学习策略。
似然函数的最大值意味着什么？让我们回到概率和似然的定义，概率描述的是在一定条件下某个事件发生的可能性，概率越大说明这件事情越可能会发生；而似然描述的是结果已知的情况下，该事件在不同条件下发生的可能性，似然函数的值越大说明该事件在对应的条件下发生的可能性越大。
也就是说似然函数取得最大值表示相应的参数能够使得统计模型最为合理。
考虑一个抛硬币的例子。假设这个硬币正面跟反面轻重不同。我们把这个硬币抛80次（即，我们获取一个采样$x_1=H,x_2=T,&mldr;..x_{80}$并把正面的次数记下来，正面记为H，反面记为T）。并把抛出一个正面的概率记为p，抛出一个反面的概率记为1-p（因此，这里的 p即相当于上边的 $\theta$ ）。 假设我们抛出了49个正面，31个反面，即49次H，31次T。假设这个硬币是我们从一个装了三个硬币的盒子里头取出的。这三个硬币抛出正面的概率分别为p=1/3, p=1/2,p=2/3.这些硬币没有标记，所以我们无法知道哪个是哪个。 使用最大似然估计，通过这些试验数据（即采样数据），我们可以计算出哪个硬币的可能性最大。这个似然函数取以下三个值中的一个： $$ P(H=49,T=31 | p=1/3) = (1/3)^{49}(1-1/3) ^{31}= 0.000 \
P(H=49,T=31 | p=1/2) = (1/2)^{49}(1-1/2)^{31} = 0.012 \
P(H=49,T=31 | p=2/3) = (2/3)^{49}(1-2/3)^{31} = 0.054 \
$$ 我们可以看到当p=2/3时，似然函数取得最大值。这就是 p的最大似然估计。
但在机器学习中我们要估计的并不是离散的情况，因此最大似然估计的一般求解过程是：
写出似然函数； 对似然函数取对数，并整理，也就对数似然函数； 求导数，解似然方程，也就是取极值； 参考： https://zhuanlan.zhihu.com/p/22092462 https://zh.wikipedia.org/wiki/似然函数 https://zh.wikipedia.org/wiki/最大似然估计</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/nlp/2017-07-06-%E4%BF%A1%E6%81%AF%E7%9A%84%E5%BA%A6%E9%87%8F%E7%86%B5/ rel=bookmark>信息的度量熵</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/nlp/ rel=category>NLP</a></span></div></div></header><div class="content list__excerpt post__content clearfix">熵 一条信息的信息量与它的不确定性成正比关系。比如说，我们要搞清楚一件非常不确定的事，或是我们一无所知的事情，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，那么不需要太多的信息就能把它搞清楚。所以从这个角度来看，可以认为，信息量就等于不确定性的多少。
它的定义如下： $$ H(X)=-\sum_{x\in X}P(x) \log P(x) $$
条件熵 如果 H(Y|X=x) 为变数 Y 在 X 取特定值 x 条件下的熵，那么 H(Y|X)就是 H(Y|X=x) 在 X 取遍所有可能 x 后平均的结果： $$ \begin{align} H(Y|X) &= \sum_i^n P(x_i)H(Y|X=x_i) \
&= \sum_i^n P(x_i) \sum_j^m P(y_j) \log P(y_j|x_i) \
&= \sum_i^n \sum_j^m P(x_i,y_j) \log P(y_j|x_i) \
&= - \sum_{x\in X,y \in Y}P(x,y) \log P(y|x) \
\end{align} $$ 现在假定我们还知道X的一些情况，包括它和Y一起出现的概率，以及在X取不同值的前提下X的概率分布。定义在X的条件下Y的条件熵为： $$ H(Y|X)=-\sum_{x\in X,y \in Y}P(x,y) \log P(y|x) $$ H(Y)>=H(Y|X)，也就是说Y的不确定性下降了。
互信息 互信息有时也叫“信息增益”。当获取的信息和要研究的事物“有关系”时，这些信息才能帮助我们消除不确定性。当然“有关系”这种说法太模糊，太不科学，最好能度化的度量“相关性”。香农在信息论中提出了一个“互信息”的概念作为对两个随机事件“相关性”的量化度量。假定有两个随机事件X和Y，它们的互信息定义如下： $$ I(X;Y)=\sum_{x \in X,y \in Y}P(x,y) \log \frac{P(x,y)}{P(x)P(y))} $$ 其实这个互信息就是，随机事件X的不确定性或者说熵H(X)，以及在知道随机事件Y条件下的不确定性或者说H(X|Y)之间的差异，即： $$ I(X;Y)=H(X) - H(X|Y) $$ 也就是说，**所谓的两个事件相关性的度量，就是在了解了其中一个Y的前提下，对消除另一个X不确定性所提供的信息量**。当X和Y完全相关是，它的取值是1；当两者完全无关时，它的取值是0。</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/algorithm/2017-05-16-%E5%85%A8%E5%9F%9F%E5%93%88%E5%B8%8C/ rel=bookmark>全域哈希</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/algorithm/ rel=category>algorithm</a></span></div></div></header><div class="content list__excerpt post__content clearfix">全域哈希的讲解有些不同，它是先给出一个奇怪的定义，然后推论出一些较好的性质。最后介绍构造方法，并证明符合定义。
定义 设H为一组有限散列函数，它将给定的关键字全域 U映射到{1,2,&mldr;m-1}中。这样的一组函数称为全域的，如果对每对不同的关键字$x,y \in U$，满足h(x)=h(y)的散列函数的个数至多为|H|/m。
性质 推论1 当关键字x!=y时，两者发生冲突的概率小于等于1/m。
推论2 随机选择一个函数，将n个关键字放进T表的m个槽中，对于给定的关键字x，它发生冲突的期望次数小于n/m（也就是装载因子）。 $$ E[#与x碰撞]&lt;n/m $$ 证明： 设$C_{x}$表示哈希表T里与x发生碰撞的次数 $$ C_{xy}= \begin{cases} 1 \ if \ h(x) = h(y)\
0 \ otherwise\
\end{cases} $$ 根据全域函数的定义可知 $$ E[C_{xy}]=1/m $$ 因此 $$ C_x = \sum_{y\in T-{x}}C_{xy} $$ $$ \begin{align} E[C_x] &= E[\sum_{y\in T-{x}}C_{xy}] \
&=\sum_{y\in T-{x}}E\left[C_{xy} \right] \
&=\sum_{y\in T-{x}} 1/m \
&= \frac{n-1}{m} \end{align} $$
全域哈希构造方法 假设m是一个质数，将关键字k转换成r+1位的m进制数，$k=&lt;k_0,k_1,\dots,k_r>,0&lt;=k_r&lt;=m-1$ 设随机数$a=&lt;a_0,a_1,\dots,a_r>$同样为成r+1位的m进制数，defin 哈希函数为 $$ h_a(x)=(\sum_{i=0}^ra_ik_i) \mod m $$ 哈希函数集H的大小为$m^{r+1}$</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/machine-learning/2017-07-19-%E5%86%B3%E7%AD%96%E6%A0%91/ rel=bookmark>决策树</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/machine-learning/ rel=category>machine learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 决策树的优点是计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点是可能会产生过度匹配问题。适用于连续值和离散值数据。
决策树生成 训练集$D={(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)}$ 属性集$A={a_1,a_2,\dots,a_d}$
def function treeGenerate(D,A): 生成结node; if Dv为空: 将该分支结点标记为叶结点，其类别标记为D中样本最多的类;return; if D中所有实例属于同一类: 将该分支结点标记为叶结点，将该类作为其类别标记;return; a* = 从A中选择最优划分属性; for v in a*: 为node生成一个分支； 令Dv表示D中在a*上取值为v的样本子集； 以treeGenerate(Dv,A-a*)为分支结点 特征选择 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有特征是没有分类能力的。
信息增益 信息增益也叫互信息。当获取的信息和要研究的事物“有关系”时，这些信息才能帮助我们消除不确定性。当然“有关系”这种说法太模糊，太不科学，最好能度化的度量“相关性”。 熵： $H(X) = -\sum_i^nP(x_i)\log(P(x_i)$ 条件熵： $H(Y|X) = \sum_i^nP(x_i)H(Y|X=x_i)$ 信息增益就是熵与条件熵的差值： $$ Gain(D,a) = H(D) - H(D|a) = -\sum_i^{|Y|} P(Y_i) \log P(Y_i) - \sum_j^{|a|}P(a_j)H(D|a=a_j) \
= H(D) - \sum_{v=1}^V \frac{|D^v|}{|D|}H(D^v) $$
信息增益比 信息增益作为划分训练数据的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。 信息增益比： $$ Gain_ratio(D,a) = \frac{Gain(D,a)}{H(a)} \
= \frac{H(D) - H(D|a)}{H(a)} \</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/tensorflow2.x-keras/tensorflow%E5%AE%9E%E6%88%984-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E6%88%98_%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/ rel=bookmark>卷积神经网络实战_图片分类</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/tensorflow2.x-keras/ rel=category>Tensorflow2.x keras</a></span></div></div></header><div class="content list__excerpt post__content clearfix">理论部分 CNN解决的问题 在CNN出现之前，图像对于人工智能来说是一个难题，有2个原因：
图像需要处理的数据量太大，导致成本很高，效率很低。 图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高。 另一个角度（用全连接神经网络处理大尺寸图像的缺点）：
其次参数过多效率低下，训练困难，同时大量的参数也很快会导致网络过拟合 图像展开为向量会丢失空间信息； 卷积层 这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。
在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：
池化层 池化层简单说就是下采样，他可以大大降低数据的维度，也可以缓解卷积层对位置的过度敏感性。其过程如下：
上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。
之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。
总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。
实战部分 import matplotlib.pyplot as plt import numpy as np import os import PIL import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import Sequential # 卷积层 x = tf.constant(range(9), dtype=tf.float32) x = tf.reshape(x, shape=(1, 3, 3, 1)) print('x :', tf.reshape(x, shape=(3, 3))) conv_layer = tf.keras.layers.Conv2D(filters=1, kernel_size=(2, 2), strides=(1, 1), kernel_initializer=tf.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/deep-learning/2017-07-11-%E5%9C%A8kaggle%E4%B8%AD%E4%BD%BF%E7%94%A8keras%E5%81%9A%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/ rel=bookmark>在kaggle中使用keras做数字识别</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/deep-learning/ rel=category>deep learning</a></span></div></div></header><div class="content list__excerpt post__content clearfix">背景 本文将讲述通过keras写一个深度学习模型，来完成 kaggle 的 Digit Recognizer 准确率可达到98%。
下载数据 首先登入kaggle网站，点击competitions，在类型筛选中选择 Getting Started。然后点击Digit Recognizer。
下载Data中的train.csv（模型的训练数据）和test.csv（测试数据）。 使用keras训练卷积神经网络模型 下面的代码分为三部分：
读取数据 训练模型 预测测试数据 from __future__ import print_function import keras from keras.datasets import mnist from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D from keras import backend as K import pandas as pd from keras.utils import np_utils from keras.layers import Dense, Dropout, Activation, Flatten batch_size = 128 num_classes = 10 epochs = 12 # Read data train = pd.</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/nlp/2017-09-09-%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E8%AF%8D%E5%88%87%E5%88%86%E5%92%8C%E6%A0%87%E6%B3%A8%E4%B8%80%E4%BD%93%E5%8C%96%E6%A8%A1%E5%9E%8B/ rel=bookmark>基于统计的词切分和标注一体化模型</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/nlp/ rel=category>NLP</a></span></div></div></header><div class="content list__excerpt post__content clearfix">基于统计的切记和标注一体化模型 假设$C=c_1,c_2 \dots c_m$为m个的字符串，$W=w_1,_2 \dots w_n$是把C切分后得到的由n个词组成的词序列，$T=t_1,t_2 \dots t_n$是对W进行标注后的标记序列。
$$ P(W|C) = \frac{P(C|W)P(W)}{P(C)} $$ 可知P(C)是一个确定的值，P(C|W)是在给定词序列的情况下字符串的概率，可以认为是1。因此可得： $$ \max P(W|C) = \max P(W) $$ 即是说，分词的过程即是寻找概率最大词序列的过程。 我们再来考虑词序列与词序列与词性标注序列的关系。 $$ P(T|W) = \frac{P(W|T)P(T)}{P(W)} $$ 可推导出： $$ P(W) = \frac{P(W|T)P(T)}{P(T|W)} $$ 如果隐马尔可夫假设和独立输出假设 即： $$ P(T) = \prod_i^n P(t_i)P(t_{i-1}) \
P(W|T) = \prod_i^n P(|w_i,t_i) \
P(T|W) = \prod_i^n P(|t_i,w_i) $$
计算方法：
找到一条分词路径W 用记性标注的模型计算具有最大概率的记性标注序列，得到对应的标注路径T。 利用公式可得到W的概率 若干个健忘路径中，概率最大的W即为分词结果 引用： 白栓虎 - 基于统计的切记和标注一体化模型</div></article><article class="list__item post"><header class=list__header><h2 class="list__title post__title"><a href=writinglite.com/post/algorithm/2017-03-26-%E5%A0%86%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/ rel=bookmark>堆排序算法</a></h2><div class="list__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>hwyang</span></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=writinglite.com/categories/algorithm/ rel=category>algorithm</a></span></div></div></header><div class="content list__excerpt post__content clearfix">堆排序的时间复杂度是nlong(n)，并且具有空间原址性。堆排序使用一种被称为堆的数据结构来进行信息管理。堆不仅用在堆排序中，而且它也可以构造一种有效的优先队列。
堆 堆是一个数组，它可以被看成一个近似的完全二叉树。 二叉堆有两种形式，最大堆和最小堆。在堆排序算法中我们使用最大堆，最小堆常用于构造优先队列。</div></article></main><div class=pagination><a class="pagination__item pagination__item--prev btn" href=writinglite.com/page/4/>«</a>
<span class="pagination__item pagination__item--current">5/8</span>
<a class="pagination__item pagination__item--next btn" href=writinglite.com/page/6/>»</a></div></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=writinglite.com></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2020-03-02-%E5%9F%BA%E4%BA%8Enginx%E7%9A%84acme%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E6%96%B9%E6%A1%88/>基于nginx的acme免费证书方案</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-11-04-%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6%E5%AE%89%E8%A3%85-certbot/>免费证书安装-certbot</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-07-27-%E6%A0%91%E8%8E%93%E6%B4%BE3b+-%E5%AE%89%E8%A3%85openwrt/>树莓派3b+安装openwrt 18.06.4</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-05-18-git-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Git 常用命令</a></li><li class=widget__item><a class=widget__link href=writinglite.com/post/other/2019-05-18-linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/>Linux 常用命令</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=writinglite.com/categories/algorithm/>algorithm</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/deep-learning/>deep learning</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/machine-learning/>machine learning</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/mathematics/>mathematics</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/nlp/>NLP</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/other/>other</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/reading/>reading</a></li><li class=widget__item><a class=widget__link href=writinglite.com/categories/tensorflow2.x-keras/>Tensorflow2.x keras</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/acme/ title=acme>acme</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/albert/ title=albert>albert</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/batch-normalization/ title="Batch Normalization">Batch Normalization</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/bert/ title=bert>bert</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/crf/ title=crf>crf</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/git/ title=Git>Git</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/guice/ title=guice>guice</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/hexo/ title=hexo>hexo</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/kaggle/ title=kaggle>kaggle</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/keras/ title=keras>keras</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/layer-normalization/ title="Layer Normalization">Layer Normalization</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/linux/ title=Linux>Linux</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/log/ title=log>log</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/logback/ title=logback>logback</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/nginx/ title=nginx>nginx</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/openwrt/ title=openwrt>openwrt</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/slf4j/ title=slf4j>slf4j</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/stanford-corenlp/ title="Stanford CoreNLP">Stanford CoreNLP</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow/ title=Tensorflow>Tensorflow</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow-hub/ title="tensorflow hub">tensorflow hub</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tensorflow2.0/ title=tensorflow2.0>tensorflow2.0</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/tra/ title=tra>tra</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/transformers/ title=transformers>transformers</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/ubuntu/ title=ubuntu>ubuntu</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%88%86%E8%AF%8D/ title=分词>分词</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%93%88%E5%B8%8C/ title=哈希>哈希</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/ title=字符串匹配>字符串匹配</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E5%B9%B6%E5%8F%91/ title=并发>并发</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%8E%92%E5%BA%8F/ title=排序>排序</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%95%A3%E5%88%97%E8%A1%A8/ title=散列表>散列表</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/ title=数学之美>数学之美</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3/ title=无监督>无监督</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ title=机器学习>机器学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ title=深度学习>深度学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%86%B5/ title=熵>熵</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%AE%97%E6%B3%95/ title=算法>算法</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/ title=统计学习>统计学习</a>
<a class="widget-taglist__link widget__link btn" href=writinglite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/ title=自然语言处理>自然语言处理</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/writinglite target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:yhw1813@126.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>yhw1813@126.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Writing Lite.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/writinglite.com/js/menu.js></script><script src=/writinglite.com/js/custom.js></script></body></html>